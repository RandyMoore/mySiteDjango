[{"model": "contenttypes.contenttype", "pk": 1, "fields": {"app_label": "wagtailcore", "model": "page"}}, {"model": "contenttypes.contenttype", "pk": 2, "fields": {"app_label": "wagtailadmin", "model": "admin"}}, {"model": "contenttypes.contenttype", "pk": 3, "fields": {"app_label": "wagtaildocs", "model": "document"}}, {"model": "contenttypes.contenttype", "pk": 4, "fields": {"app_label": "wagtailimages", "model": "image"}}, {"model": "contenttypes.contenttype", "pk": 5, "fields": {"app_label": "admin", "model": "logentry"}}, {"model": "contenttypes.contenttype", "pk": 6, "fields": {"app_label": "auth", "model": "permission"}}, {"model": "contenttypes.contenttype", "pk": 7, "fields": {"app_label": "auth", "model": "group"}}, {"model": "contenttypes.contenttype", "pk": 8, "fields": {"app_label": "auth", "model": "user"}}, {"model": "contenttypes.contenttype", "pk": 9, "fields": {"app_label": "contenttypes", "model": "contenttype"}}, {"model": "contenttypes.contenttype", "pk": 10, "fields": {"app_label": "sessions", "model": "session"}}, {"model": "contenttypes.contenttype", "pk": 11, "fields": {"app_label": "sites", "model": "site"}}, {"model": "contenttypes.contenttype", "pk": 12, "fields": {"app_label": "django_comments", "model": "comment"}}, {"model": "contenttypes.contenttype", "pk": 13, "fields": {"app_label": "django_comments", "model": "commentflag"}}, {"model": "contenttypes.contenttype", "pk": 14, "fields": {"app_label": "tagging", "model": "tag"}}, {"model": "contenttypes.contenttype", "pk": 15, "fields": {"app_label": "tagging", "model": "taggeditem"}}, {"model": "contenttypes.contenttype", "pk": 16, "fields": {"app_label": "wagtailforms", "model": "formsubmission"}}, {"model": "contenttypes.contenttype", "pk": 17, "fields": {"app_label": "wagtailredirects", "model": "redirect"}}, {"model": "contenttypes.contenttype", "pk": 18, "fields": {"app_label": "wagtailembeds", "model": "embed"}}, {"model": "contenttypes.contenttype", "pk": 19, "fields": {"app_label": "wagtailusers", "model": "userprofile"}}, {"model": "contenttypes.contenttype", "pk": 20, "fields": {"app_label": "wagtailimages", "model": "rendition"}}, {"model": "contenttypes.contenttype", "pk": 21, "fields": {"app_label": "wagtailsearch", "model": "query"}}, {"model": "contenttypes.contenttype", "pk": 22, "fields": {"app_label": "wagtailsearch", "model": "querydailyhits"}}, {"model": "contenttypes.contenttype", "pk": 23, "fields": {"app_label": "wagtailcore", "model": "grouppagepermission"}}, {"model": "contenttypes.contenttype", "pk": 24, "fields": {"app_label": "wagtailcore", "model": "pagerevision"}}, {"model": "contenttypes.contenttype", "pk": 25, "fields": {"app_label": "wagtailcore", "model": "pageviewrestriction"}}, {"model": "contenttypes.contenttype", "pk": 26, "fields": {"app_label": "wagtailcore", "model": "site"}}, {"model": "contenttypes.contenttype", "pk": 27, "fields": {"app_label": "wagtailcore", "model": "collection"}}, {"model": "contenttypes.contenttype", "pk": 28, "fields": {"app_label": "wagtailcore", "model": "groupcollectionpermission"}}, {"model": "contenttypes.contenttype", "pk": 29, "fields": {"app_label": "wagtailcore", "model": "collectionviewrestriction"}}, {"model": "contenttypes.contenttype", "pk": 30, "fields": {"app_label": "taggit", "model": "tag"}}, {"model": "contenttypes.contenttype", "pk": 31, "fields": {"app_label": "taggit", "model": "taggeditem"}}, {"model": "contenttypes.contenttype", "pk": 32, "fields": {"app_label": "weblog", "model": "blogindexpage"}}, {"model": "contenttypes.contenttype", "pk": 33, "fields": {"app_label": "weblog", "model": "weblogpage"}}, {"model": "contenttypes.contenttype", "pk": 34, "fields": {"app_label": "government_audit", "model": "auditdocument"}}, {"model": "contenttypes.contenttype", "pk": 35, "fields": {"app_label": "government_audit", "model": "namedentity"}}, {"model": "sessions.session", "pk": "ekafmw41mlgkteeu20r0v51p4olld2we", "fields": {"session_data": "ZjQ1ZTEzYWZjZGVlZjc5NTI3ZDFkNDMyZTM5N2VjYTc5MjFkYmRhNTp7Il9hdXRoX3VzZXJfaWQiOiIxIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiJiY2ViOGY0Y2FmNWI0NGU2NjRjZGJiZmE5ZjdjZGVhYTcwYjVhOTY2In0=", "expire_date": "2018-05-19T12:24:58.407Z"}}, {"model": "sites.site", "pk": 1, "fields": {"domain": "example.com", "name": "example.com"}}, {"model": "wagtailimages.rendition", "pk": 1, "fields": {"filter_spec": "max-165x165", "file": "images/InitialDeploymentThoughts2.max-165x165.png", "width": 165, "height": 109, "focal_point_key": "", "image": 1}}, {"model": "wagtailimages.rendition", "pk": 2, "fields": {"filter_spec": "original", "file": "images/InitialDeploymentThoughts2.original.png", "width": 360, "height": 238, "focal_point_key": "", "image": 1}}, {"model": "wagtailimages.rendition", "pk": 3, "fields": {"filter_spec": "max-165x165", "file": "images/InitialDeploymentThoughts2.max-165x165.png", "width": 165, "height": 108, "focal_point_key": "", "image": 2}}, {"model": "wagtailimages.rendition", "pk": 4, "fields": {"filter_spec": "original", "file": "images/InitialDeploymentThoughts2.original.png", "width": 362, "height": 239, "focal_point_key": "", "image": 2}}, {"model": "wagtailimages.rendition", "pk": 5, "fields": {"filter_spec": "max-165x165", "file": "images/BuildingBlocks_ProcessThread.max-165x165.png", "width": 165, "height": 78, "focal_point_key": "", "image": 3}}, {"model": "wagtailimages.rendition", "pk": 6, "fields": {"filter_spec": "original", "file": "images/BuildingBlocks_ProcessThread.original.png", "width": 519, "height": 247, "focal_point_key": "", "image": 3}}, {"model": "wagtailimages.rendition", "pk": 10, "fields": {"filter_spec": "max-165x165", "file": "images/FullStackArch.max-165x165.jpg", "width": 165, "height": 53, "focal_point_key": "", "image": 5}}, {"model": "wagtailimages.rendition", "pk": 11, "fields": {"filter_spec": "original", "file": "images/FullStackArch.original.jpg", "width": 638, "height": 206, "focal_point_key": "", "image": 5}}, {"model": "wagtailimages.rendition", "pk": 36, "fields": {"filter_spec": "max-165x165", "file": "images/NE_start_s_e.max-165x165.png", "width": 165, "height": 120, "focal_point_key": "", "image": 14}}, {"model": "wagtailimages.rendition", "pk": 37, "fields": {"filter_spec": "original", "file": "images/NE_start_s_e.original.png", "width": 857, "height": 626, "focal_point_key": "", "image": 14}}, {"model": "wagtailimages.rendition", "pk": 41, "fields": {"filter_spec": "max-165x165", "file": "images/NE_veterans_only_e_s.max-165x165.png", "width": 165, "height": 122, "focal_point_key": "", "image": 16}}, {"model": "wagtailimages.rendition", "pk": 43, "fields": {"filter_spec": "max-165x165", "file": "images/NE_veterans_VISN_e_s.max-165x165.png", "width": 165, "height": 122, "focal_point_key": "", "image": 17}}, {"model": "wagtailimages.rendition", "pk": 44, "fields": {"filter_spec": "original", "file": "images/NE_veterans_only_e_s.original.png", "width": 656, "height": 489, "focal_point_key": "", "image": 16}}, {"model": "wagtailimages.rendition", "pk": 45, "fields": {"filter_spec": "original", "file": "images/NE_veterans_VISN_e_s.original.png", "width": 659, "height": 491, "focal_point_key": "", "image": 17}}, {"model": "wagtailimages.rendition", "pk": 50, "fields": {"filter_spec": "max-165x165", "file": "images/NE_landing_s_e_s.max-165x165.png", "width": 165, "height": 89, "focal_point_key": "", "image": 19}}, {"model": "wagtailimages.rendition", "pk": 51, "fields": {"filter_spec": "original", "file": "images/NE_landing_s_e_s.original.png", "width": 660, "height": 356, "focal_point_key": "", "image": 19}}, {"model": "wagtailimages.rendition", "pk": 56, "fields": {"filter_spec": "max-165x165", "file": "images/NE_veterans_end_e.max-165x165.png", "width": 163, "height": 165, "focal_point_key": "", "image": 21}}, {"model": "wagtailimages.rendition", "pk": 57, "fields": {"filter_spec": "original", "file": "images/NE_veterans_end_e.original.png", "width": 897, "height": 908, "focal_point_key": "", "image": 21}}, {"model": "wagtailcore.site", "pk": 1, "fields": {"hostname": "localhost", "port": 80, "site_name": null, "root_page": 3, "is_default_site": true}}, {"model": "wagtailcore.collection", "pk": 1, "fields": {"path": "0001", "depth": 1, "numchild": 0, "name": "Root"}}, {"model": "auth.permission", "pk": 1, "fields": {"name": "Can access Wagtail admin", "content_type": 2, "codename": "access_admin"}}, {"model": "auth.permission", "pk": 2, "fields": {"name": "Can add document", "content_type": 3, "codename": "add_document"}}, {"model": "auth.permission", "pk": 3, "fields": {"name": "Can change document", "content_type": 3, "codename": "change_document"}}, {"model": "auth.permission", "pk": 4, "fields": {"name": "Can delete document", "content_type": 3, "codename": "delete_document"}}, {"model": "auth.permission", "pk": 5, "fields": {"name": "Can add image", "content_type": 4, "codename": "add_image"}}, {"model": "auth.permission", "pk": 6, "fields": {"name": "Can change image", "content_type": 4, "codename": "change_image"}}, {"model": "auth.permission", "pk": 7, "fields": {"name": "Can delete image", "content_type": 4, "codename": "delete_image"}}, {"model": "auth.permission", "pk": 8, "fields": {"name": "Can add log entry", "content_type": 5, "codename": "add_logentry"}}, {"model": "auth.permission", "pk": 9, "fields": {"name": "Can change log entry", "content_type": 5, "codename": "change_logentry"}}, {"model": "auth.permission", "pk": 10, "fields": {"name": "Can delete log entry", "content_type": 5, "codename": "delete_logentry"}}, {"model": "auth.permission", "pk": 11, "fields": {"name": "Can add permission", "content_type": 6, "codename": "add_permission"}}, {"model": "auth.permission", "pk": 12, "fields": {"name": "Can change permission", "content_type": 6, "codename": "change_permission"}}, {"model": "auth.permission", "pk": 13, "fields": {"name": "Can delete permission", "content_type": 6, "codename": "delete_permission"}}, {"model": "auth.permission", "pk": 14, "fields": {"name": "Can add group", "content_type": 7, "codename": "add_group"}}, {"model": "auth.permission", "pk": 15, "fields": {"name": "Can change group", "content_type": 7, "codename": "change_group"}}, {"model": "auth.permission", "pk": 16, "fields": {"name": "Can delete group", "content_type": 7, "codename": "delete_group"}}, {"model": "auth.permission", "pk": 17, "fields": {"name": "Can add user", "content_type": 8, "codename": "add_user"}}, {"model": "auth.permission", "pk": 18, "fields": {"name": "Can change user", "content_type": 8, "codename": "change_user"}}, {"model": "auth.permission", "pk": 19, "fields": {"name": "Can delete user", "content_type": 8, "codename": "delete_user"}}, {"model": "auth.permission", "pk": 20, "fields": {"name": "Can add content type", "content_type": 9, "codename": "add_contenttype"}}, {"model": "auth.permission", "pk": 21, "fields": {"name": "Can change content type", "content_type": 9, "codename": "change_contenttype"}}, {"model": "auth.permission", "pk": 22, "fields": {"name": "Can delete content type", "content_type": 9, "codename": "delete_contenttype"}}, {"model": "auth.permission", "pk": 23, "fields": {"name": "Can add session", "content_type": 10, "codename": "add_session"}}, {"model": "auth.permission", "pk": 24, "fields": {"name": "Can change session", "content_type": 10, "codename": "change_session"}}, {"model": "auth.permission", "pk": 25, "fields": {"name": "Can delete session", "content_type": 10, "codename": "delete_session"}}, {"model": "auth.permission", "pk": 26, "fields": {"name": "Can add site", "content_type": 11, "codename": "add_site"}}, {"model": "auth.permission", "pk": 27, "fields": {"name": "Can change site", "content_type": 11, "codename": "change_site"}}, {"model": "auth.permission", "pk": 28, "fields": {"name": "Can delete site", "content_type": 11, "codename": "delete_site"}}, {"model": "auth.permission", "pk": 29, "fields": {"name": "Can add comment", "content_type": 12, "codename": "add_comment"}}, {"model": "auth.permission", "pk": 30, "fields": {"name": "Can change comment", "content_type": 12, "codename": "change_comment"}}, {"model": "auth.permission", "pk": 31, "fields": {"name": "Can delete comment", "content_type": 12, "codename": "delete_comment"}}, {"model": "auth.permission", "pk": 32, "fields": {"name": "Can moderate comments", "content_type": 12, "codename": "can_moderate"}}, {"model": "auth.permission", "pk": 33, "fields": {"name": "Can add comment flag", "content_type": 13, "codename": "add_commentflag"}}, {"model": "auth.permission", "pk": 34, "fields": {"name": "Can change comment flag", "content_type": 13, "codename": "change_commentflag"}}, {"model": "auth.permission", "pk": 35, "fields": {"name": "Can delete comment flag", "content_type": 13, "codename": "delete_commentflag"}}, {"model": "auth.permission", "pk": 36, "fields": {"name": "Can add tag", "content_type": 14, "codename": "add_tag"}}, {"model": "auth.permission", "pk": 37, "fields": {"name": "Can change tag", "content_type": 14, "codename": "change_tag"}}, {"model": "auth.permission", "pk": 38, "fields": {"name": "Can delete tag", "content_type": 14, "codename": "delete_tag"}}, {"model": "auth.permission", "pk": 39, "fields": {"name": "Can add tagged item", "content_type": 15, "codename": "add_taggeditem"}}, {"model": "auth.permission", "pk": 40, "fields": {"name": "Can change tagged item", "content_type": 15, "codename": "change_taggeditem"}}, {"model": "auth.permission", "pk": 41, "fields": {"name": "Can delete tagged item", "content_type": 15, "codename": "delete_taggeditem"}}, {"model": "auth.permission", "pk": 42, "fields": {"name": "Can add form submission", "content_type": 16, "codename": "add_formsubmission"}}, {"model": "auth.permission", "pk": 43, "fields": {"name": "Can change form submission", "content_type": 16, "codename": "change_formsubmission"}}, {"model": "auth.permission", "pk": 44, "fields": {"name": "Can delete form submission", "content_type": 16, "codename": "delete_formsubmission"}}, {"model": "auth.permission", "pk": 45, "fields": {"name": "Can add redirect", "content_type": 17, "codename": "add_redirect"}}, {"model": "auth.permission", "pk": 46, "fields": {"name": "Can change redirect", "content_type": 17, "codename": "change_redirect"}}, {"model": "auth.permission", "pk": 47, "fields": {"name": "Can delete redirect", "content_type": 17, "codename": "delete_redirect"}}, {"model": "auth.permission", "pk": 48, "fields": {"name": "Can add embed", "content_type": 18, "codename": "add_embed"}}, {"model": "auth.permission", "pk": 49, "fields": {"name": "Can change embed", "content_type": 18, "codename": "change_embed"}}, {"model": "auth.permission", "pk": 50, "fields": {"name": "Can delete embed", "content_type": 18, "codename": "delete_embed"}}, {"model": "auth.permission", "pk": 51, "fields": {"name": "Can add user profile", "content_type": 19, "codename": "add_userprofile"}}, {"model": "auth.permission", "pk": 52, "fields": {"name": "Can change user profile", "content_type": 19, "codename": "change_userprofile"}}, {"model": "auth.permission", "pk": 53, "fields": {"name": "Can delete user profile", "content_type": 19, "codename": "delete_userprofile"}}, {"model": "auth.permission", "pk": 54, "fields": {"name": "Can add rendition", "content_type": 20, "codename": "add_rendition"}}, {"model": "auth.permission", "pk": 55, "fields": {"name": "Can change rendition", "content_type": 20, "codename": "change_rendition"}}, {"model": "auth.permission", "pk": 56, "fields": {"name": "Can delete rendition", "content_type": 20, "codename": "delete_rendition"}}, {"model": "auth.permission", "pk": 57, "fields": {"name": "Can add query", "content_type": 21, "codename": "add_query"}}, {"model": "auth.permission", "pk": 58, "fields": {"name": "Can change query", "content_type": 21, "codename": "change_query"}}, {"model": "auth.permission", "pk": 59, "fields": {"name": "Can delete query", "content_type": 21, "codename": "delete_query"}}, {"model": "auth.permission", "pk": 60, "fields": {"name": "Can add Query Daily Hits", "content_type": 22, "codename": "add_querydailyhits"}}, {"model": "auth.permission", "pk": 61, "fields": {"name": "Can change Query Daily Hits", "content_type": 22, "codename": "change_querydailyhits"}}, {"model": "auth.permission", "pk": 62, "fields": {"name": "Can delete Query Daily Hits", "content_type": 22, "codename": "delete_querydailyhits"}}, {"model": "auth.permission", "pk": 63, "fields": {"name": "Can add page", "content_type": 1, "codename": "add_page"}}, {"model": "auth.permission", "pk": 64, "fields": {"name": "Can change page", "content_type": 1, "codename": "change_page"}}, {"model": "auth.permission", "pk": 65, "fields": {"name": "Can delete page", "content_type": 1, "codename": "delete_page"}}, {"model": "auth.permission", "pk": 66, "fields": {"name": "Can add group page permission", "content_type": 23, "codename": "add_grouppagepermission"}}, {"model": "auth.permission", "pk": 67, "fields": {"name": "Can change group page permission", "content_type": 23, "codename": "change_grouppagepermission"}}, {"model": "auth.permission", "pk": 68, "fields": {"name": "Can delete group page permission", "content_type": 23, "codename": "delete_grouppagepermission"}}, {"model": "auth.permission", "pk": 69, "fields": {"name": "Can add page revision", "content_type": 24, "codename": "add_pagerevision"}}, {"model": "auth.permission", "pk": 70, "fields": {"name": "Can change page revision", "content_type": 24, "codename": "change_pagerevision"}}, {"model": "auth.permission", "pk": 71, "fields": {"name": "Can delete page revision", "content_type": 24, "codename": "delete_pagerevision"}}, {"model": "auth.permission", "pk": 72, "fields": {"name": "Can add page view restriction", "content_type": 25, "codename": "add_pageviewrestriction"}}, {"model": "auth.permission", "pk": 73, "fields": {"name": "Can change page view restriction", "content_type": 25, "codename": "change_pageviewrestriction"}}, {"model": "auth.permission", "pk": 74, "fields": {"name": "Can delete page view restriction", "content_type": 25, "codename": "delete_pageviewrestriction"}}, {"model": "auth.permission", "pk": 75, "fields": {"name": "Can add site", "content_type": 26, "codename": "add_site"}}, {"model": "auth.permission", "pk": 76, "fields": {"name": "Can change site", "content_type": 26, "codename": "change_site"}}, {"model": "auth.permission", "pk": 77, "fields": {"name": "Can delete site", "content_type": 26, "codename": "delete_site"}}, {"model": "auth.permission", "pk": 78, "fields": {"name": "Can add collection", "content_type": 27, "codename": "add_collection"}}, {"model": "auth.permission", "pk": 79, "fields": {"name": "Can change collection", "content_type": 27, "codename": "change_collection"}}, {"model": "auth.permission", "pk": 80, "fields": {"name": "Can delete collection", "content_type": 27, "codename": "delete_collection"}}, {"model": "auth.permission", "pk": 81, "fields": {"name": "Can add group collection permission", "content_type": 28, "codename": "add_groupcollectionpermission"}}, {"model": "auth.permission", "pk": 82, "fields": {"name": "Can change group collection permission", "content_type": 28, "codename": "change_groupcollectionpermission"}}, {"model": "auth.permission", "pk": 83, "fields": {"name": "Can delete group collection permission", "content_type": 28, "codename": "delete_groupcollectionpermission"}}, {"model": "auth.permission", "pk": 84, "fields": {"name": "Can add collection view restriction", "content_type": 29, "codename": "add_collectionviewrestriction"}}, {"model": "auth.permission", "pk": 85, "fields": {"name": "Can change collection view restriction", "content_type": 29, "codename": "change_collectionviewrestriction"}}, {"model": "auth.permission", "pk": 86, "fields": {"name": "Can delete collection view restriction", "content_type": 29, "codename": "delete_collectionviewrestriction"}}, {"model": "auth.permission", "pk": 87, "fields": {"name": "Can add Tag", "content_type": 30, "codename": "add_tag"}}, {"model": "auth.permission", "pk": 88, "fields": {"name": "Can change Tag", "content_type": 30, "codename": "change_tag"}}, {"model": "auth.permission", "pk": 89, "fields": {"name": "Can delete Tag", "content_type": 30, "codename": "delete_tag"}}, {"model": "auth.permission", "pk": 90, "fields": {"name": "Can add Tagged Item", "content_type": 31, "codename": "add_taggeditem"}}, {"model": "auth.permission", "pk": 91, "fields": {"name": "Can change Tagged Item", "content_type": 31, "codename": "change_taggeditem"}}, {"model": "auth.permission", "pk": 92, "fields": {"name": "Can delete Tagged Item", "content_type": 31, "codename": "delete_taggeditem"}}, {"model": "auth.permission", "pk": 93, "fields": {"name": "Can add blog index page", "content_type": 32, "codename": "add_blogindexpage"}}, {"model": "auth.permission", "pk": 94, "fields": {"name": "Can change blog index page", "content_type": 32, "codename": "change_blogindexpage"}}, {"model": "auth.permission", "pk": 95, "fields": {"name": "Can delete blog index page", "content_type": 32, "codename": "delete_blogindexpage"}}, {"model": "auth.permission", "pk": 96, "fields": {"name": "Can add weblog page", "content_type": 33, "codename": "add_weblogpage"}}, {"model": "auth.permission", "pk": 97, "fields": {"name": "Can change weblog page", "content_type": 33, "codename": "change_weblogpage"}}, {"model": "auth.permission", "pk": 98, "fields": {"name": "Can delete weblog page", "content_type": 33, "codename": "delete_weblogpage"}}, {"model": "auth.permission", "pk": 99, "fields": {"name": "Can add audit document", "content_type": 34, "codename": "add_auditdocument"}}, {"model": "auth.permission", "pk": 100, "fields": {"name": "Can change audit document", "content_type": 34, "codename": "change_auditdocument"}}, {"model": "auth.permission", "pk": 101, "fields": {"name": "Can delete audit document", "content_type": 34, "codename": "delete_auditdocument"}}, {"model": "auth.permission", "pk": 102, "fields": {"name": "Can add named entity", "content_type": 35, "codename": "add_namedentity"}}, {"model": "auth.permission", "pk": 103, "fields": {"name": "Can change named entity", "content_type": 35, "codename": "change_namedentity"}}, {"model": "auth.permission", "pk": 104, "fields": {"name": "Can delete named entity", "content_type": 35, "codename": "delete_namedentity"}}, {"model": "auth.group", "pk": 1, "fields": {"name": "Moderators", "permissions": [1, 2, 3, 4, 5, 6, 7]}}, {"model": "auth.group", "pk": 2, "fields": {"name": "Editors", "permissions": [1, 2, 3, 4, 5, 6, 7]}}, {"model": "auth.user", "pk": 1, "fields": {"password": "pbkdf2_sha256$100000$UlpNFbOV1GtJ$l3MH6T6Y7TU1jXonhBCaBd1LiT+I0oHc6CSVxeea1Gc=", "last_login": "2018-05-05T12:24:58.403Z", "is_superuser": true, "username": "fool", "first_name": "", "last_name": "", "email": "", "is_staff": true, "is_active": true, "date_joined": "2017-07-29T20:49:22.947Z", "groups": [], "user_permissions": []}}, {"model": "wagtailimages.image", "pk": 1, "fields": {"collection": 1, "title": "Initial Thoughts", "file": "original_images/InitialDeploymentThoughts2.png", "width": 360, "height": 238, "created_at": "2017-07-29T21:08:26.470Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": null}}, {"model": "wagtailimages.image", "pk": 2, "fields": {"collection": 1, "title": "Initial Thoughts", "file": "original_images/InitialDeploymentThoughts2.png", "width": 362, "height": 239, "created_at": "2017-07-30T18:13:34.317Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": null}}, {"model": "wagtailimages.image", "pk": 3, "fields": {"collection": 1, "title": "Building Blocks", "file": "original_images/BuildingBlocks_ProcessThread.png", "width": 519, "height": 247, "created_at": "2017-07-30T18:16:16.870Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": null}}, {"model": "wagtailimages.image", "pk": 5, "fields": {"collection": 1, "title": "Two Tier Architecture", "file": "original_images/FullStackArch.jpg", "width": 638, "height": 206, "created_at": "2017-10-02T17:39:30.404Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": null}}, {"model": "wagtailimages.image", "pk": 14, "fields": {"collection": 1, "title": "NE Start", "file": "original_images/NE_start_s_e.png", "width": 857, "height": 626, "created_at": "2017-11-25T09:44:07.999Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": 87570}}, {"model": "wagtailimages.image", "pk": 16, "fields": {"collection": 1, "title": "Veterans Only", "file": "original_images/NE_veterans_only_e_s.png", "width": 656, "height": 489, "created_at": "2017-11-25T09:48:29.160Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": null}}, {"model": "wagtailimages.image", "pk": 17, "fields": {"collection": 1, "title": "NE VISN", "file": "original_images/NE_veterans_VISN_e_s.png", "width": 659, "height": 491, "created_at": "2017-11-25T09:49:52.131Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": null}}, {"model": "wagtailimages.image", "pk": 19, "fields": {"collection": 1, "title": "NE Landing", "file": "original_images/NE_landing_s_e_s.png", "width": 660, "height": 356, "created_at": "2017-11-25T09:52:24Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": 54592}}, {"model": "wagtailimages.image", "pk": 21, "fields": {"collection": 1, "title": "NE Veterans Final", "file": "original_images/NE_veterans_end_e.png", "width": 897, "height": 908, "created_at": "2017-11-25T09:56:33.042Z", "uploaded_by_user": 1, "focal_point_x": null, "focal_point_y": null, "focal_point_width": null, "focal_point_height": null, "file_size": null}}, {"model": "wagtailcore.page", "pk": 1, "fields": {"path": "0001", "depth": 1, "numchild": 1, "title": "Root", "draft_title": "", "slug": "root", "content_type": 1, "live": true, "has_unpublished_changes": false, "url_path": "/", "owner": null, "seo_title": "", "show_in_menus": false, "search_description": "", "go_live_at": null, "expire_at": null, "expired": false, "locked": false, "first_published_at": null, "last_published_at": null, "latest_revision_created_at": null, "live_revision": null}}, {"model": "wagtailcore.page", "pk": 2, "fields": {"path": "00010001", "depth": 2, "numchild": 0, "title": "Welcome to your new Wagtail site!", "draft_title": "", "slug": "home", "content_type": 1, "live": true, "has_unpublished_changes": false, "url_path": "/home/", "owner": null, "seo_title": "", "show_in_menus": false, "search_description": "", "go_live_at": null, "expire_at": null, "expired": false, "locked": false, "first_published_at": null, "last_published_at": null, "latest_revision_created_at": null, "live_revision": null}}, {"model": "wagtailcore.page", "pk": 3, "fields": {"path": "00010002", "depth": 2, "numchild": 4, "title": "Randy Moore", "draft_title": "", "slug": "randy-moore", "content_type": 32, "live": true, "has_unpublished_changes": false, "url_path": "/randy-moore/", "owner": 1, "seo_title": "", "show_in_menus": false, "search_description": "", "go_live_at": null, "expire_at": null, "expired": false, "locked": false, "first_published_at": "2017-07-29T21:05:36.834Z", "last_published_at": "2017-07-29T21:05:36.834Z", "latest_revision_created_at": null, "live_revision": null}}, {"model": "wagtailcore.page", "pk": 4, "fields": {"path": "000100020001", "depth": 3, "numchild": 0, "title": "Genesis", "draft_title": "Genesis", "slug": "genesis", "content_type": 33, "live": true, "has_unpublished_changes": false, "url_path": "/randy-moore/genesis/", "owner": 1, "seo_title": "", "show_in_menus": false, "search_description": "", "go_live_at": null, "expire_at": null, "expired": false, "locked": false, "first_published_at": "2017-07-29T21:11:03.825Z", "last_published_at": "2018-05-05T12:33:24.212Z", "latest_revision_created_at": "2018-05-05T12:33:24.181Z", "live_revision": 45}}, {"model": "wagtailcore.page", "pk": 5, "fields": {"path": "000100020002", "depth": 3, "numchild": 0, "title": "Asynchronous Programming", "draft_title": "Asynchronous Programming", "slug": "asynchronous-programming", "content_type": 33, "live": true, "has_unpublished_changes": false, "url_path": "/randy-moore/asynchronous-programming/", "owner": 1, "seo_title": "", "show_in_menus": false, "search_description": "", "go_live_at": null, "expire_at": null, "expired": false, "locked": false, "first_published_at": "2017-07-30T18:20:11.671Z", "last_published_at": "2018-05-05T12:34:15.704Z", "latest_revision_created_at": "2018-05-05T12:34:15.671Z", "live_revision": 49}}, {"model": "wagtailcore.page", "pk": 6, "fields": {"path": "000100020003", "depth": 3, "numchild": 0, "title": "Full Stack Walkthrough", "draft_title": "Full Stack Walkthrough", "slug": "full-stack-walkthrough", "content_type": 33, "live": true, "has_unpublished_changes": false, "url_path": "/randy-moore/full-stack-walkthrough/", "owner": 1, "seo_title": "", "show_in_menus": false, "search_description": "", "go_live_at": null, "expire_at": null, "expired": false, "locked": false, "first_published_at": "2017-10-02T20:33:57.085Z", "last_published_at": "2018-05-05T12:33:50.805Z", "latest_revision_created_at": "2018-05-05T12:33:50.772Z", "live_revision": 47}}, {"model": "wagtailcore.page", "pk": 7, "fields": {"path": "000100020004", "depth": 3, "numchild": 0, "title": "Exploring NLP Parsed Audit Documents", "draft_title": "Exploring NLP Parsed Audit Documents", "slug": "exploring-nlp-parsed-audit-documents", "content_type": 33, "live": true, "has_unpublished_changes": false, "url_path": "/randy-moore/exploring-nlp-parsed-audit-documents/", "owner": 1, "seo_title": "", "show_in_menus": false, "search_description": "", "go_live_at": null, "expire_at": null, "expired": false, "locked": false, "first_published_at": "2017-11-25T10:02:38.513Z", "last_published_at": "2018-05-05T12:26:08.402Z", "latest_revision_created_at": "2018-05-05T12:26:08.371Z", "live_revision": 43}}, {"model": "wagtailcore.pagerevision", "pk": 1, "fields": {"page": 7, "submitted_for_moderation": false, "created_at": "2017-11-25T10:00:38.884Z", "user": 1, "content_json": "{\"pk\": 7, \"path\": \"000100020004\", \"depth\": 3, \"numchild\": 0, \"title\": \"Exploring NLP Parsed Audit Documents\", \"slug\": \"exploring-nlp-parsed-audit-documents\", \"content_type\": 33, \"live\": false, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/exploring-nlp-parsed-audit-documents/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": null, \"last_published_at\": null, \"latest_revision_created_at\": null, \"live_revision\": null, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Inspiration\\\\r\\\\n========\\\\r\\\\nThis project came about for these reasons:\\\\r\\\\n\\\\r\\\\n 1.  Make a positive impact on the world.\\\\r\\\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\\\r\\\\n    *  A first project to gain some experience working with real world open data.\\\\r\\\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\\\r\\\\n 3.  Professional Development - gain experience with some of the latest technologies.\\\\r\\\\n\\\\r\\\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\\\r\\\\n\\\\r\\\\nFinal Product\\\\r\\\\n=============\\\\r\\\\n\\\\r\\\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\\\r\\\\n\\\\r\\\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\\\r\\\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\\\r\\\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\\\r\\\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\\\r\\\\n\\\\r\\\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\\\r\\\\n\\\\r\\\\nLanding Page\\\\r\\\\n-------------------\\\", \\\"id\\\": \\\"39563302-2d00-4dbd-bdcd-6353cbcde4b2\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 19, \\\"id\\\": \\\"c25448a6-6344-4b6e-9b6a-c1207e5febf7\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Named Entity Exploration\\\\\\\" Button...\\\\r\\\\n\\\\r\\\\nStart of Named Entity Exploration\\\\r\\\\n----------------------------------------------\\\", \\\"id\\\": \\\"18fc48fd-2620-4628-9b5a-bce895a02059\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 14, \\\"id\\\": \\\"e144978c-317a-4ef1-9cc4-9476442cdcf1\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Veterans\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nDocuments containing \\\\\\\"Veterans\\\\\\\"\\\\r\\\\n-----------------------------------------------\\\", \\\"id\\\": \\\"68459c9b-0032-41e8-8ba8-e1ac11e8638e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 16, \\\"id\\\": \\\"c94a824b-d7ba-4628-a724-f172b974adf5\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"VISN\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nNarrow scope for documents also containing \\\\\\\"VISN\\\\\\\"\\\\r\\\\n------------------------------------------------------------------------\\\", \\\"id\\\": \\\"4348a42c-cfca-490d-a63d-3fe31aac966c\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 17, \\\"id\\\": \\\"b0561886-8e2d-4430-9956-db11f2cf55c9\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Final Result\\\\r\\\\n----------------\\\\r\\\\nIncluding:\\\\r\\\\n\\\\r\\\\n *  Medical Center\\\\r\\\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\\\", \\\"id\\\": \\\"eb63228f-6f57-44ff-90dc-3a7d385cac3e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 21, \\\"id\\\": \\\"3748efbc-4d49-468c-882c-2d7f51678e63\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\\\r\\\\n\\\\r\\\\n---\\\\r\\\\n\\\\r\\\\nOffline Processing\\\\r\\\\n==================\\\\r\\\\n\\\\r\\\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\\\r\\\\n\\\\r\\\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asychronous programming within a thread (supported by the language), [multihreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the only option was to use multi processing.  \\\\r\\\\n\\\\r\\\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure given the root of the directory and processes the files associated with each audit document if that data isn't already in the database.  All cores of the host machine are loaded up using a multiprocessing Pool as follows:\\\\r\\\\n\\\\r\\\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\\\r\\\\n        p.map(func=process_documents, iterable=files)\\\\r\\\\n\\\\r\\\\nOriginally I also had the write to Postgres occur as part of each process (ie process_documents()) but discovered it would throw errors in the Django layer.  It would have been complicated and probably inefficient (ie process / core blocked waiting for DB access) to coordinate the DB as a shared resource so I opted instead for a queue.  Each process would drop it's results into a queue and on an additional process would live a task that consumes from the queue and writes each result to the database sequentially.  The final code was surprising simple:\\\\r\\\\n\\\\r\\\\n    # Top of file\\\\r\\\\n    db_queue = None # declared as global\\\\r\\\\n    \\\\r\\\\n    # In __main__\\\\r\\\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\\\r\\\\n    multiprocessing.Process(target=save_to_db).start()\\\\r\\\\n\\\\r\\\\n    # Process documents as above with multiple processes    \\\\r\\\\n    # In process_documents()\\\\r\\\\n    # ... processing code\\\\r\\\\n    # Drop results in queue, process has finished this document\\\\r\\\\n    db_queue.put((doc, named_entities))\\\\r\\\\n    \\\\r\\\\n    # In save_to_db()\\\\r\\\\n    # grab from queue forever, blocking if empty, returning when 'None' element is encountered\\\\r\\\\n    while True: \\\\r\\\\n        doc_tuple = db_queue.get()\\\\r\\\\n        # save results to DB\\\\r\\\\n    \\\\r\\\\n    # In __main__ at the very end\\\\r\\\\n    # Put None in queue as final element, DB process cleanly exits when it finds this.\\\\r\\\\n    db_queue.put(None)\\\\r\\\\n\\\\r\\\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\\\\\"corpus\\\\\\\" learned :)\\\", \\\"id\\\": \\\"3a156f36-80ae-4fad-ab0f-2dd520ba4968\\\"}]\", \"subheading\": \"Learning more Python because my machine is slow\", \"date\": \"2017-11-25\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 2, "fields": {"page": 7, "submitted_for_moderation": false, "created_at": "2017-11-25T10:02:38.468Z", "user": 1, "content_json": "{\"pk\": 7, \"path\": \"000100020004\", \"depth\": 3, \"numchild\": 0, \"title\": \"Exploring NLP Parsed Audit Documents\", \"slug\": \"exploring-nlp-parsed-audit-documents\", \"content_type\": 33, \"live\": false, \"has_unpublished_changes\": true, \"url_path\": \"/randy-moore/exploring-nlp-parsed-audit-documents/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": null, \"last_published_at\": null, \"latest_revision_created_at\": \"2017-11-25T10:00:38.884Z\", \"live_revision\": null, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Inspiration\\\\r\\\\n========\\\\r\\\\nThis project came about for these reasons:\\\\r\\\\n\\\\r\\\\n 1.  Make a positive impact on the world.\\\\r\\\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\\\r\\\\n    *  A first project to gain some experience working with real world open data.\\\\r\\\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\\\r\\\\n 3.  Professional Development - gain experience with some of the latest technologies.\\\\r\\\\n\\\\r\\\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\\\r\\\\n\\\\r\\\\nFinal Product\\\\r\\\\n=============\\\\r\\\\n\\\\r\\\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\\\r\\\\n\\\\r\\\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\\\r\\\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\\\r\\\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\\\r\\\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\\\r\\\\n\\\\r\\\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\\\r\\\\n\\\\r\\\\nLanding Page\\\\r\\\\n-------------------\\\", \\\"id\\\": \\\"39563302-2d00-4dbd-bdcd-6353cbcde4b2\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 19, \\\"id\\\": \\\"c25448a6-6344-4b6e-9b6a-c1207e5febf7\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Named Entity Exploration\\\\\\\" Button...\\\\r\\\\n\\\\r\\\\nStart of Named Entity Exploration\\\\r\\\\n----------------------------------------------\\\", \\\"id\\\": \\\"18fc48fd-2620-4628-9b5a-bce895a02059\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 14, \\\"id\\\": \\\"e144978c-317a-4ef1-9cc4-9476442cdcf1\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Veterans\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nDocuments containing \\\\\\\"Veterans\\\\\\\"\\\\r\\\\n-----------------------------------------------\\\", \\\"id\\\": \\\"68459c9b-0032-41e8-8ba8-e1ac11e8638e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 16, \\\"id\\\": \\\"c94a824b-d7ba-4628-a724-f172b974adf5\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"VISN\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nNarrow scope for documents also containing \\\\\\\"VISN\\\\\\\"\\\\r\\\\n------------------------------------------------------------------------\\\", \\\"id\\\": \\\"4348a42c-cfca-490d-a63d-3fe31aac966c\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 17, \\\"id\\\": \\\"b0561886-8e2d-4430-9956-db11f2cf55c9\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Final Result\\\\r\\\\n----------------\\\\r\\\\nIncluding:\\\\r\\\\n\\\\r\\\\n *  Medical Center\\\\r\\\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\\\", \\\"id\\\": \\\"eb63228f-6f57-44ff-90dc-3a7d385cac3e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 21, \\\"id\\\": \\\"3748efbc-4d49-468c-882c-2d7f51678e63\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\\\r\\\\n\\\\r\\\\n---\\\\r\\\\n\\\\r\\\\nOffline Processing\\\\r\\\\n==================\\\\r\\\\n\\\\r\\\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\\\r\\\\n\\\\r\\\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asychronous programming within a thread (supported by the language), [multihreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the only option was to use multi processing.  \\\\r\\\\n\\\\r\\\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure given the root of the directory and processes the files associated with each audit document if that data isn't already in the database.  All cores of the host machine are loaded up using a multiprocessing Pool as follows:\\\\r\\\\n\\\\r\\\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\\\r\\\\n        p.map(func=process_documents, iterable=files)\\\\r\\\\n\\\\r\\\\nOriginally I also had the write to Postgres occur as part of each process (ie process_documents()) but discovered it would throw errors in the Django layer.  It would have been complicated and probably inefficient (ie process / core blocked waiting for DB access) to coordinate the DB as a shared resource so I opted instead for a queue.  Each process would drop it's results into a queue and on an additional process would live a task that consumes from the queue and writes each result to the database sequentially.  The final code was surprising simple:\\\\r\\\\n\\\\r\\\\n    # Top of file\\\\r\\\\n    db_queue = None # declared as global\\\\r\\\\n    \\\\r\\\\n    # In __main__\\\\r\\\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\\\r\\\\n    multiprocessing.Process(target=save_to_db).start()\\\\r\\\\n\\\\r\\\\n    # Process documents as above with multiple processes    \\\\r\\\\n    # In process_documents()\\\\r\\\\n    # ... processing code\\\\r\\\\n    # Drop results in queue, process has finished this document\\\\r\\\\n    db_queue.put((doc, named_entities))\\\\r\\\\n    \\\\r\\\\n    # In save_to_db()\\\\r\\\\n    # grab from queue forever, blocking if empty, returning when 'None' element is encountered\\\\r\\\\n    while True: \\\\r\\\\n        doc_tuple = db_queue.get()\\\\r\\\\n        # save results to DB\\\\r\\\\n    \\\\r\\\\n    # In __main__ at the very end\\\\r\\\\n    # Put None in queue as final element, DB process cleanly exits when it finds this.\\\\r\\\\n    db_queue.put(None)\\\\r\\\\n\\\\r\\\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\\\\\"corpus\\\\\\\" learned :)\\\", \\\"id\\\": \\\"3a156f36-80ae-4fad-ab0f-2dd520ba4968\\\"}]\", \"subheading\": \"Learning more Python because my machine is slow\", \"date\": \"2017-11-25\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 35, "fields": {"page": 7, "submitted_for_moderation": false, "created_at": "2017-11-25T14:01:51.843Z", "user": 1, "content_json": "{\"pk\": 7, \"path\": \"000100020004\", \"depth\": 3, \"numchild\": 0, \"title\": \"Exploring NLP Parsed Audit Documents\", \"slug\": \"exploring-nlp-parsed-audit-documents\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/exploring-nlp-parsed-audit-documents/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-11-25T10:02:38.513Z\", \"last_published_at\": \"2017-11-25T10:02:38.513Z\", \"latest_revision_created_at\": \"2017-11-25T10:02:38.468Z\", \"live_revision\": 2, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Inspiration\\\\r\\\\n========\\\\r\\\\nThis project came about for these reasons:\\\\r\\\\n\\\\r\\\\n 1.  Make a positive impact on the world.\\\\r\\\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\\\r\\\\n    *  A first project to gain some experience working with real world open data.\\\\r\\\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\\\r\\\\n 3.  Professional Development - gain experience with some of the latest technologies.\\\\r\\\\n\\\\r\\\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\\\r\\\\n\\\\r\\\\nFinal Product\\\\r\\\\n=============\\\\r\\\\n\\\\r\\\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\\\r\\\\n\\\\r\\\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\\\r\\\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\\\r\\\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\\\r\\\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\\\r\\\\n\\\\r\\\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\\\r\\\\n\\\\r\\\\nLanding Page\\\\r\\\\n-------------------\\\", \\\"id\\\": \\\"39563302-2d00-4dbd-bdcd-6353cbcde4b2\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 19, \\\"id\\\": \\\"c25448a6-6344-4b6e-9b6a-c1207e5febf7\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Named Entity Exploration\\\\\\\" Button...\\\\r\\\\n\\\\r\\\\nStart of Named Entity Exploration\\\\r\\\\n----------------------------------------------\\\", \\\"id\\\": \\\"18fc48fd-2620-4628-9b5a-bce895a02059\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 14, \\\"id\\\": \\\"e144978c-317a-4ef1-9cc4-9476442cdcf1\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Veterans\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nDocuments containing \\\\\\\"Veterans\\\\\\\"\\\\r\\\\n-----------------------------------------------\\\", \\\"id\\\": \\\"68459c9b-0032-41e8-8ba8-e1ac11e8638e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 16, \\\"id\\\": \\\"c94a824b-d7ba-4628-a724-f172b974adf5\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"VISN\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nNarrow scope for documents also containing \\\\\\\"VISN\\\\\\\"\\\\r\\\\n------------------------------------------------------------------------\\\", \\\"id\\\": \\\"4348a42c-cfca-490d-a63d-3fe31aac966c\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 17, \\\"id\\\": \\\"b0561886-8e2d-4430-9956-db11f2cf55c9\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Final Result\\\\r\\\\n----------------\\\\r\\\\nIncluding:\\\\r\\\\n\\\\r\\\\n *  Medical Center\\\\r\\\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\\\", \\\"id\\\": \\\"eb63228f-6f57-44ff-90dc-3a7d385cac3e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 21, \\\"id\\\": \\\"3748efbc-4d49-468c-882c-2d7f51678e63\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\\\r\\\\n\\\\r\\\\n---\\\\r\\\\n\\\\r\\\\nOffline Processing\\\\r\\\\n==================\\\\r\\\\n\\\\r\\\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\\\r\\\\n\\\\r\\\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asynchronous programming within a thread (supported by the language) and [multithreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the best option was to use multi processing.  \\\\r\\\\n\\\\r\\\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure at a given root and processes the files associated with each audit document if that data isn't already in the database.  The documents are processed in parallel, one per core of the host machine.\\\\r\\\\n\\\\r\\\\nThe write of results to Postgres was originally written to occur in the same task after processing the document but this resulted in Django layer concurrency errors.  It would have been complicated and inefficient to coordinate the DB as a shared resource so I opted instead to use a queue.  Each document processing task places the result into a queue.  An additional DB dedicated process consumes from the queue and writes each result to the database sequentially.  The final code was surprisingly simple:\\\\r\\\\n\\\\r\\\\nTop of file:\\\\r\\\\n\\\\r\\\\n    db_queue = None # declared as global to be shared between processes\\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ create the DB queue and start it's process:\\\\r\\\\n\\\\r\\\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\\\r\\\\n    multiprocessing.Process(target=save_to_db).start() # DB dedicated process\\\\r\\\\n\\\\r\\\\nIn process_documents() the results are placed in the queue:\\\\r\\\\n\\\\r\\\\n    db_queue.put((doc, named_entities))\\\\r\\\\n    \\\\r\\\\nIn save_to_db() we loop forever consuming from the queue until None is encountered:\\\\r\\\\n\\\\r\\\\n    while True: \\\\r\\\\n        doc_tuple = db_queue.get()  # blocks if queue is empty\\\\r\\\\n        <exit function if None found>\\\\r\\\\n        <save to DB>        \\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ all cores of the host machine are loaded up using a multiprocessing Pool and once they are finished None is placed in the DB queue to be found by the DB task causing it to exit:\\\\r\\\\n\\\\r\\\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\\\r\\\\n        p.map(func=process_documents, iterable=files) # Processes to process documents\\\\r\\\\n\\\\r\\\\n    db_queue.put(None)\\\\r\\\\n\\\\r\\\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\\\\\"corpus\\\\\\\" learned :)\\\", \\\"id\\\": \\\"3a156f36-80ae-4fad-ab0f-2dd520ba4968\\\"}]\", \"subheading\": \"Learning more Python because my machine is slow\", \"date\": \"2017-11-25\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 36, "fields": {"page": 7, "submitted_for_moderation": false, "created_at": "2017-11-25T14:01:55.344Z", "user": 1, "content_json": "{\"pk\": 7, \"path\": \"000100020004\", \"depth\": 3, \"numchild\": 0, \"title\": \"Exploring NLP Parsed Audit Documents\", \"slug\": \"exploring-nlp-parsed-audit-documents\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": true, \"url_path\": \"/randy-moore/exploring-nlp-parsed-audit-documents/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-11-25T10:02:38.513Z\", \"last_published_at\": \"2017-11-25T10:02:38.513Z\", \"latest_revision_created_at\": \"2017-11-25T14:01:51.843Z\", \"live_revision\": 2, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Inspiration\\\\r\\\\n========\\\\r\\\\nThis project came about for these reasons:\\\\r\\\\n\\\\r\\\\n 1.  Make a positive impact on the world.\\\\r\\\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\\\r\\\\n    *  A first project to gain some experience working with real world open data.\\\\r\\\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\\\r\\\\n 3.  Professional Development - gain experience with some of the latest technologies.\\\\r\\\\n\\\\r\\\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\\\r\\\\n\\\\r\\\\nFinal Product\\\\r\\\\n=============\\\\r\\\\n\\\\r\\\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\\\r\\\\n\\\\r\\\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\\\r\\\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\\\r\\\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\\\r\\\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\\\r\\\\n\\\\r\\\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\\\r\\\\n\\\\r\\\\nLanding Page\\\\r\\\\n-------------------\\\", \\\"id\\\": \\\"39563302-2d00-4dbd-bdcd-6353cbcde4b2\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 19, \\\"id\\\": \\\"c25448a6-6344-4b6e-9b6a-c1207e5febf7\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Named Entity Exploration\\\\\\\" Button...\\\\r\\\\n\\\\r\\\\nStart of Named Entity Exploration\\\\r\\\\n----------------------------------------------\\\", \\\"id\\\": \\\"18fc48fd-2620-4628-9b5a-bce895a02059\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 14, \\\"id\\\": \\\"e144978c-317a-4ef1-9cc4-9476442cdcf1\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Veterans\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nDocuments containing \\\\\\\"Veterans\\\\\\\"\\\\r\\\\n-----------------------------------------------\\\", \\\"id\\\": \\\"68459c9b-0032-41e8-8ba8-e1ac11e8638e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 16, \\\"id\\\": \\\"c94a824b-d7ba-4628-a724-f172b974adf5\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"VISN\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nNarrow scope for documents also containing \\\\\\\"VISN\\\\\\\"\\\\r\\\\n------------------------------------------------------------------------\\\", \\\"id\\\": \\\"4348a42c-cfca-490d-a63d-3fe31aac966c\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 17, \\\"id\\\": \\\"b0561886-8e2d-4430-9956-db11f2cf55c9\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Final Result\\\\r\\\\n----------------\\\\r\\\\nIncluding:\\\\r\\\\n\\\\r\\\\n *  Medical Center\\\\r\\\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\\\", \\\"id\\\": \\\"eb63228f-6f57-44ff-90dc-3a7d385cac3e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 21, \\\"id\\\": \\\"3748efbc-4d49-468c-882c-2d7f51678e63\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\\\r\\\\n\\\\r\\\\n---\\\\r\\\\n\\\\r\\\\nOffline Processing\\\\r\\\\n==================\\\\r\\\\n\\\\r\\\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\\\r\\\\n\\\\r\\\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asynchronous programming within a thread (supported by the language) and [multithreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the best option was to use multi processing.  \\\\r\\\\n\\\\r\\\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure at a given root and processes the files associated with each audit document if that data isn't already in the database.  The documents are processed in parallel, one per core of the host machine.\\\\r\\\\n\\\\r\\\\nThe write of results to Postgres was originally written to occur in the same task after processing the document but this resulted in Django layer concurrency errors.  It would have been complicated and inefficient to coordinate the DB as a shared resource so I opted instead to use a queue.  Each document processing task places the result into a queue.  An additional DB dedicated process consumes from the queue and writes each result to the database sequentially.  The final code was surprisingly simple:\\\\r\\\\n\\\\r\\\\nTop of file:\\\\r\\\\n\\\\r\\\\n    db_queue = None # declared as global to be shared between processes\\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ create the DB queue and start it's process:\\\\r\\\\n\\\\r\\\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\\\r\\\\n    multiprocessing.Process(target=save_to_db).start() # DB dedicated process\\\\r\\\\n\\\\r\\\\nIn process_documents() the results are placed in the queue:\\\\r\\\\n\\\\r\\\\n    db_queue.put((doc, named_entities))\\\\r\\\\n    \\\\r\\\\nIn save_to_db() we loop forever consuming from the queue until None is encountered:\\\\r\\\\n\\\\r\\\\n    while True: \\\\r\\\\n        doc_tuple = db_queue.get()  # blocks if queue is empty\\\\r\\\\n        <exit function if None found>\\\\r\\\\n        <save to DB>        \\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ all cores of the host machine are loaded up using a multiprocessing Pool and once they are finished None is placed in the DB queue to be found by the DB task causing it to exit:\\\\r\\\\n\\\\r\\\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\\\r\\\\n        p.map(func=process_documents, iterable=files) # Processes to process documents\\\\r\\\\n\\\\r\\\\n    db_queue.put(None)\\\\r\\\\n\\\\r\\\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\\\\\"corpus\\\\\\\" learned :)\\\", \\\"id\\\": \\\"3a156f36-80ae-4fad-ab0f-2dd520ba4968\\\"}]\", \"subheading\": \"Learning more Python because my machine is slow\", \"date\": \"2017-11-25\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 37, "fields": {"page": 5, "submitted_for_moderation": false, "created_at": "2018-05-05T09:47:45.394Z", "user": 1, "content_json": "{\"pk\": 5, \"path\": \"000100020002\", \"depth\": 3, \"numchild\": 0, \"title\": \"Asynchronous Programming\", \"draft_title\": \"Asynchronous Programming\", \"slug\": \"asynchronous-programming\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/asynchronous-programming/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-07-30T18:20:11.671Z\", \"last_published_at\": \"2017-07-30T18:20:11.671Z\", \"latest_revision_created_at\": null, \"live_revision\": null, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"### Why Care?\\\\r\\\\nBrowsing through job postings you often notice a job requirement along the lines of: \\\\r\\\\n> Able to write highly efficient asynchronous code.  \\\\r\\\\n\\\\r\\\\nA fleeting thought:\\\\r\\\\n\\\\r\\\\n\\\\\\\"Ah this just means writing efficient code in terms of Big-O and then split and farm out work to it by using a solution (language feature, library, framework, ...) someone else has already come up with.\\\\\\\"\\\\r\\\\n\\\\r\\\\nYou skip over this requirement without much more thought.  A few days later you are in an interview and get asked the seminal \\\\\\\"What's the difference between a process and a thread?\\\\\\\".   Huh? Why is that a relevant question?  You already know processes and threads are low level OS stuff.  We don't program in assembly language any more - why be concerned with OS primitives?\\\\r\\\\n\\\\r\\\\nWriting efficient asynchronous code does have the prerequisite that your code is efficient and you are skilled at using pre-existing code.  But there is (at least) one additional skill required here: choosing and effectively implementing the asynchronous paradigm that fits the problem you are trying to solve.  When the interviewer asked you about process vs thread they were quickly checking the tip of the iceberg of what they hope you already know about asynchronous programming.  Hopefully you know this stuff, otherwise in the design interview you will be yielding blank stares instead of solutions.\\\\r\\\\n\\\\r\\\\n### Problem Space\\\\r\\\\n\\\\r\\\\nSo what is the high level problem to be solved with the skill of writing \\\\\\\"Efficient Asynchronous Code\\\\\\\"?  Just writing efficient code in terms of algorithmic complexity will optimize resource usage: CPU cycles and / or memory.  The addition of \\\\\\\"Asynchronous\\\\\\\" implies that different parts of your code may more freely execute when needed, less constrained by their location in the source code file.\\\\r\\\\n\\\\r\\\\nThe asynchronous aspect is expected to yield scalar benefit because all it considers is when code executes given the same input, using the same algorithms.  In contrast, the goal of algorithmic efficiency tuning is an asymptotic benefit given increasing input size.  So why bother with a mere scalar increase?  At some point processing takes a minimum amount of time.  When you have many tasks taking some non-trivial amount of time, the ability to order and potentially parallelize them can have a significant impact on overall (calendar) run time.\\\\r\\\\n\\\\r\\\\nThere are 2 categories of what causes calendar wait times:\\\\r\\\\n\\\\r\\\\n1.  CPU Bound - Time required to process data\\\\r\\\\n    *  Can alternatively be addressed by a more efficient algorithm\\\\r\\\\n2.  I/O Bound - Time required to move data\\\\r\\\\n    *  Can alternatively be addressed by caching\\\\r\\\\n\\\\r\\\\nSkillful application of asynchronous processing technique allows you to parallelize such time bound tasks to reduce overall calendar wait time as much as possible.  In the world of a web service platform scalar gains (2X, 3X etc) would be seen as phenomenal wins.  Even marginal gains (10%) are [valuable](https://blog.kissmetrics.com/loading-time/).\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n### Building Blocks\\\\r\\\\nThat interview question about process vs thread is checking the relationship between 2 nodes in a knowledge graph required to select the building blocks required for an overall solution.\\\\r\\\\n\\\\r\\\\nIn a nutshell:  The operating system creates and manages processes, each having it's own address space.  The operating system may also create additional threads within a process (this is generally done using the language of your choice interfacing with the OS).  Threads share the address space of the process, allocating off the same heap and also able to share references to arbitrary memory in the address space.  Some languages additionally support [coroutines](https://en.wikipedia.org/wiki/Coroutine) which have their own stack but which execute within a thread.  This leads to 3 choices each of where to allocate execution and data: Processes, Threads, or Coroutines.\\\", \\\"id\\\": \\\"eda162bf-482c-4e97-a2cd-d800a63710a8\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 3, \\\"id\\\": \\\"389ea35a-6ce0-4cb8-b25c-f59883aabe18\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Of course this diagram is a simplification only showing a small subset of possibilities for the instances of {Process, Thread, Coroutine} that could exist.  For example there could be many processes, processes can spawn child processes, and there can be many instances of coroutines.  Only stacks are shown but of course other local data such as the program counter for each executable instance would need to exist.  Additionally different computing environments have different setups; e.g. a particular programming language implementation may manage threads instead of the OS (known as \\\\\\\"green\\\\\\\" threads).  Always read the docs (and blogs, tutorials, etc) related to your particular environment.\\\\r\\\\n\\\\r\\\\nSo how do you decide where to allocate processing?  Here are some pros and cons to consider for each:\\\\r\\\\n\\\\r\\\\n1.  Multiple processes\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  True parallelism, will work for increasing overall performance of CPU bound tasks.\\\\r\\\\n    *  Separate address space for each process instance, no need to worry about resource contention within the program (external resource contention may still exist, e.g. multiple processes writing to a single file).\\\\r\\\\n    *  Easiest to reason about.  Only 1 entry point for execution.\\\\r\\\\n    *  For *nix platform easiest to reuse as a modular component with OS provided IPC mechanisms (eg combining small programs with pipe '|' on the command prompt).\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Heavy weight.  Each process has it's own copy of program data, etc.  In addition to memory processes generally use other limited OS resources more heavily than the other options.\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n2.  Multiple threads\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  Lighter weight than processes.  Threads share the data and heap portion of process memory.\\\\r\\\\n    *  May offer true parallelism for CPU bound tasks.  Check your computing environment implementation docs to be sure.\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Resource contention within the process address space may be complicated to deal with, especially if the threads can be run in parallel and / or are scheduled by the OS.\\\\r\\\\n    *  Difficult to reason about because a thread may be suspended at any time and have another thread change it's environment.\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n3.  Coroutines\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  Explicit control of when code is executing and when it is not executing, easier to reason about because you don't have to account for suspension in every possible location.\\\\r\\\\n    *  Language support reduces complexity of gathering results.  Generally the result replaces the task in your code.  i.e. a list of tasks becomes a list of results from those tasks; no need to implement code to collect results (ie callbacks, global data structures, etc).\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Does not offer local parallelism.  Only 1 coroutine may be running at a given time in the thread it shares with other coroutines.\\\\r\\\\n    *  Requires implementation discipline.  It is easy to introduce blocking code which freezes your thread and subsequently all other coroutines on the thread for that duration.  Blocking code could even be part of a 3rd party library - the libraries you use need to be compatible!\\\\r\\\\n\\\\r\\\\nOnce the nature of the tasks (IO or CPU bound?) are identified along with the usability features desired for the code some combination of these options should surface as a winner.\\\\r\\\\n\\\\r\\\\nHere are two code examples (at least Python 3.5 is required) to illustrate the execution difference between threads and coroutines.  Specifically these examples illustrate how coroutine execution is more explicit and deterministic as opposed to the willy-nilly execution that threads enjoy (but the rest of us don't). Thread and process execution is similar (both willy-nilly) so a process example is not included here.  Each example contains 3 kinds of tasks.  A CPU bound task, 4 IO bound tasks of varying lengths, and a polling task that wants to recur.\\\\r\\\\n\\\\r\\\\nFirst, multi threading:\\\", \\\"id\\\": \\\"ea192bd0-251a-4892-97c1-460731b8ade6\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"python\\\", \\\"code\\\": \\\"#! /usr/bin/env python3\\\\r\\\\n    from random import sample\\\\r\\\\n    from time import sleep, time\\\\r\\\\n    from threading import Thread\\\\r\\\\n    \\\\r\\\\n    def io_bound(s, label=\\\\\\\"IO bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        sleep(s)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    def cpu_bound(s, label=\\\\\\\"CPU bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        end = time() + s\\\\r\\\\n        while time() < end:\\\\r\\\\n            sample(range(1000), 1000)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    child_threads = [Thread(target=io_bound, args=(s, str(s) + \\\\\\\"s IO Bound\\\\\\\")) for s in range(4)]\\\\r\\\\n    child_threads.append(Thread(target=cpu_bound, args=(2, \\\\\\\"2s CPU Bound\\\\\\\")))\\\\r\\\\n    \\\\r\\\\n    print(\\\\\\\"Start child threads\\\\\\\")\\\\r\\\\n    for t in child_threads:\\\\r\\\\n        t.start()\\\\r\\\\n    \\\\r\\\\n    # We are in a main thread, so this is the equivalent of the poll task from async_example\\\\r\\\\n    while True:\\\\r\\\\n        print(\\\\\\\"Poll\\\\\\\")\\\\r\\\\n        live_threads = [t for t in child_threads if t.is_alive()]\\\\r\\\\n        if len(live_threads) > 0:\\\\r\\\\n            sleep(0.5)\\\\r\\\\n        else:\\\\r\\\\n            break\\\\r\\\\n\\\\r\\\\n    print(\\\\\\\"Main thread completed\\\\\\\")\\\"}, \\\"id\\\": \\\"898aa91e-0ae1-42da-830d-48410990f74c\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<p>Output:</p>\\\", \\\"id\\\": \\\"61b07605-ded1-4aab-930d-e93d0bf3d54f\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"bash\\\", \\\"code\\\": \\\"Start child threads\\\\r\\\\n    0s IO Bound started\\\\r\\\\n    0s IO Bound finished\\\\r\\\\n    1s IO Bound started\\\\r\\\\n    2s IO Bound started\\\\r\\\\n    3s IO Bound started\\\\r\\\\n    2s CPU Bound started\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    1s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    2s CPU Bound finished\\\\r\\\\n    2s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    3s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Main thread completed\\\"}, \\\"id\\\": \\\"b8020f4c-9935-4379-870c-4b7edae0ad73\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Here all threads are running independently spread throughout time.  The drawback is seen when considering the CPU bound task.  While it is running other threads are allowed to run.  It can be preempted mid operation and have things change under it's feet.  If this task needed to access resources also accessible to the other tasks then potentially complex synchronization implementation is required.  If for example it was placing partial results in a shared global location there would need to be additional code to synchronize access to that location across threads.  One positive aspect here is shown by the polling task: it is allowed to run without being starved by the CPU bound task.\\\\r\\\\n\\\\r\\\\nNow the asynchronous version using asyncio - Python's coroutine support.\\\", \\\"id\\\": \\\"47d3561d-0999-415a-8f9b-66f088a6e6e9\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"python\\\", \\\"code\\\": \\\"#! /usr/bin/env python3\\\\r\\\\n    import asyncio\\\\r\\\\n    from random import sample\\\\r\\\\n    from time import time\\\\r\\\\n    \\\\r\\\\n    async def io_bound(s, label=\\\\\\\"IO bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        await asyncio.sleep(s)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    # Example of a blocking task\\\\r\\\\n    async def cpu_bound(s, label=\\\\\\\"CPU bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        end = time() + s\\\\r\\\\n        while time() < end:\\\\r\\\\n            sample(range(1000), 1000)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    async def poll(s):\\\\r\\\\n        while True:\\\\r\\\\n            print(\\\\\\\"Poll\\\\\\\")\\\\r\\\\n            await asyncio.sleep(s)\\\\r\\\\n            active_tasks = [task for task in asyncio.Task.all_tasks() if not task.done()]\\\\r\\\\n            if  len(active_tasks) < 3: # this poll() and wait() for run_until_complete will always exist\\\\r\\\\n                return\\\\r\\\\n    \\\\r\\\\n    loop = asyncio.get_event_loop()\\\\r\\\\n    tasks = [asyncio.ensure_future(io_bound(s, str(s) + \\\\\\\"s IO Bound\\\\\\\")) for s in range(4)]\\\\r\\\\n    tasks.append(asyncio.ensure_future(cpu_bound(2, \\\\\\\"2s CPU Bound\\\\\\\")))\\\\r\\\\n    tasks.append(asyncio.ensure_future(poll(.5)))\\\\r\\\\n    \\\\r\\\\n    loop.run_until_complete(asyncio.wait(tasks))\\\\r\\\\n    \\\\r\\\\n    print(\\\\\\\"Event loop completed\\\\\\\")\\\"}, \\\"id\\\": \\\"b6600f3d-7ca9-4c56-bb90-8dd87d0aec8e\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<p>Output:</p>\\\", \\\"id\\\": \\\"4da01b5b-cfdf-4ebe-ae78-c678471fe147\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"bash\\\", \\\"code\\\": \\\"0s IO Bound started\\\\r\\\\n    1s IO Bound started\\\\r\\\\n    2s IO Bound started\\\\r\\\\n    3s IO Bound started\\\\r\\\\n    2s CPU Bound started\\\\r\\\\n    2s CPU Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    0s IO Bound finished\\\\r\\\\n    1s IO Bound finished\\\\r\\\\n    2s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    3s IO Bound finished\\\\r\\\\n    Event loop completed\\\"}, \\\"id\\\": \\\"d46a0c4f-babc-4785-96b6-6b42995cf6c3\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The difference here is that the CPU bound tasks runs to completion without being interrupted.  You may ask yourself - isn't this a bad thing?  Yes it is for CPU bound tasks and that's why you shouldn't use coroutines to parallelize those.  But it illustrates the point that your code will not be interrupted willy-nilly!  Since execution won't be preempted you don't have to deal with implementing a synchronization strategy for shared resources.  All the tasks that have to wait for _external_ IO bound processing to complete effectively have the work processed in parallel - slashing the overall wait time.  All of this is done very efficiently and without the headache of resource synchronization!  Waiting on IO bound external tasks is very common in web development, which is why this form of asynchronous processing has become all the rage.\\\", \\\"id\\\": \\\"354bf569-3602-486b-a45c-0772bd732962\\\"}]\", \"subheading\": \"(and why it's all the rage for web services)\", \"date\": \"2017-06-19\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 38, "fields": {"page": 4, "submitted_for_moderation": false, "created_at": "2018-05-05T09:48:43.664Z", "user": 1, "content_json": "{\"pk\": 4, \"path\": \"000100020001\", \"depth\": 3, \"numchild\": 0, \"title\": \"Genesis\", \"draft_title\": \"Genesis\", \"slug\": \"genesis\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/genesis/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-07-29T21:11:03.825Z\", \"last_published_at\": \"2017-07-30T18:13:41.062Z\", \"latest_revision_created_at\": null, \"live_revision\": null, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"### Why build a personal site?\\\\r\\\\nMany folks have an online identity through sources such as Facebook or [LinkedIn](https://www.linkedin.com/in/randy-moore-b552b014/).  Profiles such as these have the benefit of a network engine but limit creative expression.  \\\\r\\\\n\\\\r\\\\nLinkedIn serves as an excellent host to one's professional profile.  The features are tailored to a professional perspective and include an automatic and personalized connection to LinkedIn's ecosystem.  Shortly after creating my LinkedIn profile I was contacted by a LinkedIn recruiter and wound up spending 5 most excellent career years there.  Similarly Facebook automatically provides opportunity to connect to people in your social circles.\\\\r\\\\n\\\\r\\\\nPreexisting sites with a particular focus come with the downside of limiting creative expression.  LinkedIn and Facebook will hold your hand and guide you through creating a profile.  This scripted creation is easy and potentially rewarding but limits you to the purpose and environment of the platform.  Creating a personal website from lower level building blocks offers greater freedom of expression but in the same vein is a daunting endeavor.  Beyond the technical challenges you need to think about content and purpose.  Will it serve as a herald of your professional brand?  You run into tough questions: is it wise to include your personal views as a part of your professional brand?\\\\r\\\\n\\\\r\\\\nFrom experience I have faith in open source to solve the technical challenges.  A plethora of proven building blocks exist.  The greater challenge is content and purpose.  This website will serve as an experiment. From life experience I've noticed many sources recommend to be either hot or cold, not [lukewarm](http://biblehub.com/revelation/3-16.htm)<sup>1</sup>. \\\\r\\\\n\\\\r\\\\nHypothesis:\\\\r\\\\n> The world is a large place.  Exercising creative freedom and following passion will lead to blow back but also growth and purpose.\\\\r\\\\n\\\\r\\\\nThis site will serve as a herald of both my professional *and* personal brand.\\\\r\\\\n\\\\r\\\\n### Creation of the site\\\\r\\\\nThe creation of this site is a story of curiosity, conjecture and a meandering path exploring what open source has to offer.  This post covers up to the initial step of publishing the source code.\\\\r\\\\n\\\\r\\\\nTime at LinkedIn was spent engaged with a Java (EE) centric environment with tooling written mostly in Python.  In response to some time sensitive needs I had decided to use Python scripting and was impressed by how quickly a solution came together for a seemingly overwhelming task.  To boot, Python has been a rising star in the industry.  Python was chosen be the central language of the site.\\\\r\\\\n\\\\r\\\\nTechnologies that encapsulate process have recently spoken to my soul.  Having spent hundreds of hours in my career dealing with development environment issues the ability to encapsulate a the creation of a development environment is particularly appealing.  Dabbling across open source projects you will see many share virtual machine environments (eg using [Vagrant](https://www.vagrantup.com/)) but this approach is still fairly heavyweight (long download times, complexity).  [Docker](https://www.docker.com/) captured my attention as a lightweight alternative to full virtual machines.  Turns out Docker natively supports orchestrating multiple Docker containers into a complete system. Sold!\\\\r\\\\n\\\\r\\\\nThe goal at this point was to create the website within a Docker container so it could be worked on anywhere.  Some poking around led to [Flask](http://flask.pocoo.org/) which promises to be minimal (meaning to me ease of use) but extensible. [Nginx](https://nginx.org/) soon came into focus since the built in Flask webserver does not support serving [more than 1 request at a time](http://flask.pocoo.org/docs/0.12/deploying/).  Nginx is advertised as a lightwight server with a built in [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy) - basically a cache to serve up commonly requested pages that have already been created by the (heavyweight) app framework (Flask).\\\\r\\\\n\\\\r\\\\nInitial Thoughts:\\\", \\\"id\\\": \\\"000b5b3f-71df-4853-be49-127ed59479fd\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 5, \\\"id\\\": \\\"7940ebfc-de7d-432a-b3a6-2ebe999a43f3\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The efficiency promised by Nginx led to dreams of hosting the site on a [Raspberry Pi](https://www.raspberrypi.org/).  Not everything is available for ARM machines, but [Docker seems to be available on the Pi](https://www.raspberrypi.org/blog/docker-comes-to-raspberry-pi/).  The magic sauce to have Nginx serve with Flask is [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface), an API specification. [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) looked like the best choice to most efficiently provide that glue since it supports native communication over sockets (instead of via HTTP or some other relatively heavy layer).\\\\r\\\\n\\\\r\\\\nThe uWSGI implementation is not just code included in a final executable, it works as a (daemon) service.  This threw a monkey wrench in the plan; a typical Docker container generally runs a single process which is tied to the lifecycle of the container.  First attempt began with the [Nginx Docker base image](https://hub.docker.com/_/nginx/) but led to creating and modifying init scripts to bring up uWSGI in addition to Nginx.  After much effort uWSGI would still not be running upon container deployment, time for a different approach.\\\\r\\\\n\\\\r\\\\nSome searching led to a question that bore fruit: how does one correctly manage multiple services within a Docker container?  A solution is provided by [phusion.nl](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/).  After reading the [story](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/) behind the base image there was a wave of relief.  Had my init script hacks worked strange issues would probably have appeared later on.  Thankfully the Nginx Docker base image rejected my hacks by following the design principle\\\\r\\\\n> [Make it easy to use correctly and hard to use incorrectly](http://principles-wiki.net/principles:easy_to_use_and_hard_to_misuse)\\\\r\\\\n\\\\r\\\\n### Sharing the site source code\\\\r\\\\nWhat drives people often isn't the what but the why.  While searching for the meaning of life it's become clear that the path to happiness involves sharing.  Open source has been a central part of my life since installing [Free BSD](https://www.freebsd.org/) on an old computer in the late 90's.  Shortly after that and up to the present [Gentoo Linux](https://gentoo.org/) has served my personal computing needs.  Having received so much from the open source community there's an itch to give back.  My hope is that others may follow the story of this site via these blog posts (more coming soon) and gain a bit of perspective on the open source landscape and the opportunities available to them.\\\\r\\\\n\\\\r\\\\n[Current source code for this site](https://github.com/RandyMoore/website)\\\\r\\\\n\\\\r\\\\n[The commit for a minimal Nginx - uWSGI - Flask Docker container](https://github.com/RandyMoore/website/commit/9690a8908408cc87c52aaed007decd276c7e01e6) (See this [README](https://github.com/RandyMoore/website/blob/f3d5d8f4a006206d8359218ba8544cdc5f8c1224/README.md) for directions)\\\\r\\\\n\\\\r\\\\n###<em>Footnotes</em>\\\\r\\\\n1.  I am spiritual but do not subscribe to a particular religion.\\\", \\\"id\\\": \\\"2b881364-9599-4b21-8d18-0ecf4b0bb04b\\\"}]\", \"subheading\": \"How easy is it to create a personal website using open source?  Why build a personal website?\", \"date\": \"2017-06-05\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 39, "fields": {"page": 6, "submitted_for_moderation": false, "created_at": "2018-05-05T09:49:06.565Z", "user": 1, "content_json": "{\"pk\": 6, \"path\": \"000100020003\", \"depth\": 3, \"numchild\": 0, \"title\": \"Full Stack Walkthrough\", \"draft_title\": \"Full Stack Walkthrough\", \"slug\": \"full-stack-walkthrough\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/full-stack-walkthrough/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-10-02T20:33:57.085Z\", \"last_published_at\": \"2017-10-02T21:40:42.633Z\", \"latest_revision_created_at\": null, \"live_revision\": null, \"body\": \"[{\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Summary</h3><p>High level development walk through for a toy example of a modern full web application stack. Intended for those who want an idea of what basic knowledge and skills a &quot;Full Stack Engineer&quot; would have without being bogged down with the knowledge required for a real world project. Some scope of additional knowledge required for a real world project (team size &gt; 1, customer facing features, security, scale-ability, ...) is mentioned.</p><p>The complete source code of the example project is available at <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango\\\\\\\">https://github.com/RandyMoore/mySiteDjango</a></p><h3>Requirements</h3><p>Before starting out on any project it is important to clearly define what the goal is. In this case a personal website serving as a space to learn, experiment, and showcase web technologies. The site serves as an example for those wishing to learn the technology and so the project should be as simple and understandable as possible. The site also serves as a professional portfolio and so should be visually appealing to a non-technical audience.</p><h3>Architecture</h3><p>The requirements drive the architecture. The user roles consist of the content creator and the content viewer. The content is static and not sensitive which simplifies several aspects of the architecture. We don&#x27;t need to be concerned with growing storage demands or security since there is nothing sensitive to protect. Another requirement is deployment flexibility compatible with the major cloud service providers for low cost high performance hosting. Someone wishing to learn the technology should also be able to easily deploy the site locally so they can experiment with it. A two tier architecture (web server and database layers) encapsulated in a deployable container fits the bill for this use case.</p>\\\", \\\"id\\\": \\\"204d1b56-0988-44f0-bf0f-54d56d395837\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 5, \\\"id\\\": \\\"69d402a6-ff31-4afb-b921-7e34a5812afc\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Languages</h3><p>Choice of programming languages impacts the set of existing software you can choose to build your project from. Language and existing software choice (framework, libraries) are somewhat of a chicken and egg problem, start with which you feel strongest about. Web applications are broadly split into two parts, the front end and back end. The front end part of the project is run by the viewer&#x27;s web browser. The back end is run by the computing resources provided by the cloud service provider.</p><p>There&#x27;s less of a language choice for the front end which is driven by the need for compatibility to run on a variety of web browsers. For a web page HTML is required for the structured layout of content; CSS and Javascript are optional for separating out style and adding behavior respectively. There are many other front end languages you might encounter (e.g. CoffeScript, SASS) but these generally compile down to some older version of Javascript or CSS before it is served to the viewer&#x27;s browser.</p><p>For the back end, anything goes. The client&#x27;s web browser will contact the server using <a href=\\\\\\\"https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol\\\\\\\">HTTP</a>. The web browser has no knowledge of what is happening on the other side of this HTTP interface and so you are free to choose whatever language you wish. But you do need to comply with the HTTP specification, which is rather large. Implementing the behavior required by the HTTP specification using only primitive language features would be an impractical task. Enter frameworks.</p><h3>Frameworks</h3><p>HTTP is well defined in a specification and this has allowed others to write reusable software to take care of the HTTP details. Frameworks are language specific - they provide a bare-bones system in a particular language and allow you to add your own code to add custom behavior. You may already be familiar with the concept of a library. A framework is similar in that it is reusable code but is different because it drives control instead of responding to commands as a library would. In this case the framework code will be the first to handle the incoming request from the viewer&#x27;s web browser and then it will call your code to form a reply. <a href=\\\\\\\"https://www.djangoproject.com/\\\\\\\">Django</a> was chosen as the web framework for this project. Here&#x27;s a visualization of framework vs library:</p>\\\", \\\"id\\\": \\\"d9bc862c-31dc-4484-ad05-64250a252e01\\\"}, {\\\"type\\\": \\\"html\\\", \\\"value\\\": \\\"<iframe width=\\\\\\\"560\\\\\\\" height=\\\\\\\"315\\\\\\\" src=\\\\\\\"https://www.youtube.com/embed/lC0KapQU0aM\\\\\\\" frameborder=\\\\\\\"0\\\\\\\" allowfullscreen></iframe>\\\", \\\"id\\\": \\\"3e30cd2a-16c7-4ec2-9e0e-00fadbc12e4a\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Libraries</h3><p>The next step in the project is deciding what content should be added. A blog is a standard way for professional software developers to market their brand; the blog may be used to write about each project as it is added to the site. A naive approach is to write a single HTML file for each blog post. But writing HTML code can be cumbersome and having each post as a stand-alone HTML file might cause problems in the future if you want to change the style of all the posts; each file would have to changed.</p><p>Writing HTML by hand can be a good fit for simple data content that isn&#x27;t meant to be pretty. Creating something that has appealing style requires design skill. Fortunately libraries exist that encapsulate design. A quick search yields a <a href=\\\\\\\"https://startbootstrap.com/template-overviews/clean-blog/\\\\\\\">Bootstrap clean-blog design</a>. This takes care of the design aspect of the blog.</p><p>People have also written reusable software for the content aspect of things like blogs. This kind of software is known as a <a href=\\\\\\\"https://en.wikipedia.org/wiki/Content_management_system\\\\\\\">Content Management System</a> (CMS). Generally CMS is used as a means to separate content creation from the technical details of a site. Content specialists (e.g. journalists) may add content without becoming mired in technical details. CMS systems usually include WYSIWYG (What You See Is What You Get) editors; one way to avoid writing raw HTML. CMS systems are generally dependent on the web framework since there is a lot of glue code within the framework that has to interface with the various facets of the content (html generation, image resources, URL paths managed by the CMS, ...). Some searching yielded <a href=\\\\\\\"https://wagtail.io/\\\\\\\">Wagtail</a> as a CMS for use with Django. Perhaps a bit overkill for this site (having a team size of 1) but I was curious about CMS technology and wanted to experiment with a full fledged example of this technology.</p><h3>Development</h3><p>For a team size of one enough has been decided at this point to begin working with code. Generally a development environment is built based on choice of language since each language has it&#x27;s own paradigm of what a development environment is. For the Python back end we use <a href=\\\\\\\"https://pypi.python.org/pypi/pip\\\\\\\">pip</a> to fetch and install project requirements (existing reusable software components). A common concern with development environments is isolation between multiple projects on the same machine. We use <a href=\\\\\\\"https://virtualenv.pypa.io/en/stable/\\\\\\\">VirtualEnv</a> to keep all libraries and frameworks from interfering with each other and the global Python environment on our development machine.</p><p>Thought should be given regarding project structure. The chosen framework will often dictate this since it needs to know where to find your extensions but often your project will have files that exist outside of the framework. For example, in this project the declaration for which Python frameworks and libraries to install are kept in a file <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/config/requirements.txt\\\\\\\">requirements.txt</a> that can be fed to pip. This and other configuration files that affect the deployed code are kept in a folder name <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/config\\\\\\\">config</a> at the highest level of the project. The <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/my_site_django\\\\\\\">Django framework structure</a> begins as a sibling at same level since it is also a component of the deployed code.</p><p>A modern trend with languages and frameworks is to provide a REPL (Read Evaluate Print Loop). Languages (ie Python) often have a shell that evaluate commands this way, providing an interactive environment that encourages experimentation and exploration. Django and many other frameworks mimic a REPL with hot reloading. In Django, this is provided by the built-in development server (&#x27;python manage.py runserver&#x27;). Anytime you make a change to your code the changes are quickly and automatically ingested by the server; reflected the next time an HTTP request is processed. This feedback loop allows you to incrementally add and see small changes to the site, speeding development.</p><p>To edit code many people find an Integrated Development Environment (IDE) helpful. The IDE adds smarts and tools to your basic text editor. &quot;smarts&quot; meaning it is aware of the code in your project and can automatically recognize relationships between pieces of your code and also library or framework code. The IDE allows you to quickly navigate through execution paths to quickly understand how the pieces work together. I&#x27;m a fan of IDEA products, their community edition (CE) PyCharm is great for Python centric projects. The &#x27;python manage.py runserver&#x27; command may be run in a debugging session within the IDE, making it trivial to debug and explore the inner workings of the server code.</p><p>PyCharm CE doesn&#x27;t support front end languages well so I&#x27;ve been using (learning) the Atom editor for front end work. As mentioned before, front end programming languages generally get compiled down to an older version of HTML, CSS and Javascript to maximize browser compatibility. The step of converting your modern code into these older language versions is called &quot;bundling&quot;. For the front end development environment this project uses <a href=\\\\\\\"https://en.wikipedia.org/wiki/JavaScript\\\\\\\">Javascript</a> with <a href=\\\\\\\"https://nodejs.org/en/\\\\\\\">Node.js</a> as the engine to execute the Javascript (instead of a web browser). Similar to pip for Python exists <a href=\\\\\\\"https://www.npmjs.com/\\\\\\\">npm</a> (Node Package Manager) for Javascript (<a href=\\\\\\\"https://yarnpkg.com/en/\\\\\\\">yarn</a> may be used interchangeably with npm if npm fails to install something).</p><p>In this project the Javascript dependencies are recorded at the top level in a <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/package.json\\\\\\\">package.json</a> file. <a href=\\\\\\\"https://gruntjs.com/\\\\\\\">Grunt</a> is a Javascript program that serves as a way to glue together the steps of the bundling process. The tasks are declared in the project level <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Gruntfile.js\\\\\\\">Gruntfile.js</a>. The default task invoked with &#x27;grunt&#x27; may be run while editing code and serves as a REPL; when any front end related file changes it invokes the rebundling process and the changes will be reflected by the development server after a browser page refresh. For debugging the developer tools available in the browser are used (available by default in modern browsers).</p><h3>Project Management</h3><p>After writing some code and getting something basic to work you will worry that making further changes will break what you&#x27;ve already accomplished. Enter version control systems. Such systems allow you to track changes, moving between versions of your code at will. Version control becomes more of a concern as team size increases, but even with a team of size of one they are worth using. For example you can use branching to work on a major new feature while having the flexibility to quickly tweak the version of code that is currently deployed and being viewed by the public. This project uses <a href=\\\\\\\"https://git-scm.com/\\\\\\\">git</a>. GitHub (a service provided by a company) is used as a publicly accessible repository since it is well suited for open source projects.</p><p>Project management consists of far more than version control and becomes increasingly important as team size grows. At a high level a team decides on a <a href=\\\\\\\"https://en.wikipedia.org/wiki/Software_development_process\\\\\\\">development methodology</a> and refines a specific software development process to meet the project needs. The choice of methodology and process can influence what technologies to use and so is usually decided early in the project&#x27;s life cycle. The art of harnessing the work of multiple individual developers is it&#x27;s own subject and best learned from experience working with a team.</p><h3>Deployment</h3><p>Once you have something you&#x27;d like to show to the public the next step is to serve it to the world. Technically you could serve from the your development machine but keeping your development machine running 100% of the time to run the site would likely be a nuisance. You could also serve from a different machine that you own but this comes with a great deal of complexity and will probably expose your personal network to a high level of security risk. Additionally your home network is likely on the fringe of the world wide web which makes it slow to respond.</p><p>Fortunately companies offering cloud services exist which allow you to host arbitrary software and take care of the messy details of hosting (networking, security, computing resource allocation ...). The idea is that the hosting company figures out how to manage all of the hosting details for many users and then provides hosting as a service, adding value with economy of scale. A popular example is <a href=\\\\\\\"https://aws.amazon.com/\\\\\\\">Amazon Web Services</a> (AWS), but many vendors exist. Even with the hosting details taken care of there is still the issue of how to install your software on the cloud.</p><p>Accessing running software on the cloud isn&#x27;t as convenient or seamless as your local development environment. Changing anything almost always guarantees bugs. Wouldn&#x27;t it be awesome if you could draw a box around the code running on your development machine and simply drop it into the cloud? Then changing to the cloud environment would introduce minimal changes (seen from the perspective of your code) and result in fewer bugs. <a href=\\\\\\\"https://en.wikipedia.org/wiki/Abstraction_(software_engineering)\\\\\\\">Abstraction</a> comes to the rescue: why not abstract a machine, or more precisely the operating system? The technology that does this has existed for some time and is know as Virtualization. Virtual machines came about early on but are cumbersome due to their large size from replicating an entire OS. A much lighter weight alternative is <a href=\\\\\\\"https://www.docker.com/\\\\\\\">Docker</a>, which this project uses.</p><p>The Docker paradigm is to have a single app with all dependencies running in a single container and to tie multiple containers together to form a system. This is an example of the <a href=\\\\\\\"https://en.wikipedia.org/wiki/Single_responsibility_principle\\\\\\\">Single Responsibility Principle</a> which gives rise to many desirable design traits including ease of reuse. In this project there are two Docker containers; one for the web service and another that hosts the database. The database container is reused from a publicly available repository; the project only needs to declare it as a dependency, populate it with data, and it works out of the box.</p><p>At the top level of the project is the <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Dockerfile\\\\\\\">Dockerfile</a> that is used to build the image for the web server. Running an image with Docker (the program installed on a machine) creates a container. The <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose.yml\\\\\\\">docker-compose.yml</a> ties together the local web server and database images to form the complete stack; running &#x27;docker-compose up&#x27; will create both web server and database containers from the images and make them visible to each other in their own network. Once the local docker images are working they are pushed to an AWS repository. AWS requires the repository identifier reflected in the image name hence the separate <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose-AWS.yml\\\\\\\">docker-compose-AWS.yml</a> file. At a high level deploying to AWS involves creating a spot to host the container (EC2 instance), authenticating through a local AWS docker client, and running &#x27;docker-compose up&#x27; on docker-compose-AWS.yml. The Docker service running on the EC2 instance fetches the images from the AWS repository and brings them up to form the complete system.</p><h3>Testing</h3><p>A minimal amount of testing was required for this project. Testing needs increase with how mission critical a project is and how many people are working on the project, among other considerations. Even though this was a toy project with one developer it does have <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/src/js/government_audit/test/AuditSearchView.test.js\\\\\\\">some unit tests</a>, serving primarily as <a href=\\\\\\\"https://en.wikipedia.org/wiki/Regression_testing\\\\\\\">regression</a> tests. <a href=\\\\\\\"https://reactjs.org/\\\\\\\">React</a> and <a href=\\\\\\\"https://facebook.github.io/flux/\\\\\\\">Flux</a> were new to me so unit tests were created once I had things working so that I could refactor the code and quickly detect when I had made a code change that resulted in a change of code output. Of note in these tests is the use of <a href=\\\\\\\"https://facebook.github.io/jest/\\\\\\\">Jest</a> which allows creation of unit tests without the manual creation of assertion code. It has built-in support for snapshots (auto verifying everything) - perfect for regression testing to ensure that refactoring code doesn&#x27;t change the output of the code.</p><p>The development process for this project was most like <a href=\\\\\\\"https://en.wikipedia.org/wiki/Software_prototyping\\\\\\\">Software Prototyping</a>. Emphasis was placed on getting up and running as quickly as possible with quick turnaround enabling experimentation and progress along the learning curve. In a sense the REPL like nature of the environment offered the bulk of testing for this project. For systems that allow interaction, especially when users are allowed to change data on the server, more testing would be necessary.</p><h3>Not in this Example</h3><p>Being a toy project there are many parts not present here that would be encountered in a real world project. Here minimal attention is given to security as the content is static and not sensitive. HTTP encryption via SSL <a href=\\\\\\\"https://letsencrypt.org/\\\\\\\">is becoming an expected feature</a>. Once you have any user interaction with a site authentication becomes a requirement. Scalability isn&#x27;t an issue here but if a web site provides a non-trivial service to many users a distributed system architecture will likely be necessary.</p><p>For development workflow a team would have a system for deciding, recording, organizing, and tracking implementation progress for requirements. <a href=\\\\\\\"https://continuousdelivery.com/foundations/test-automation/\\\\\\\">Automated testing</a> with a deployment pipeline would exist. Error handling for both end users and developers (e.g. throw meaningful exceptions) would exist. Logging is typical as is emitting, collecting and analyzing performance metrics from the system for both engineering and business purposes. Each of these areas requires deep knowledge and experience to implement well; there is often a specialized role created for each. Participating in existing projects is the best way to learn about these subjects.</p>\\\", \\\"id\\\": \\\"da2a54e4-92c0-4563-ba59-ab25b922b24b\\\"}]\", \"subheading\": \"of this Toy Modern Web Application\", \"date\": \"2017-10-02\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 40, "fields": {"page": 4, "submitted_for_moderation": false, "created_at": "2018-05-05T09:49:29.235Z", "user": 1, "content_json": "{\"pk\": 4, \"path\": \"000100020001\", \"depth\": 3, \"numchild\": 0, \"title\": \"Genesis\", \"draft_title\": \"Genesis\", \"slug\": \"genesis\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/genesis/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-07-29T21:11:03.825Z\", \"last_published_at\": \"2018-05-05T09:48:43.696Z\", \"latest_revision_created_at\": \"2018-05-05T09:48:43.664Z\", \"live_revision\": 38, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"### Why build a personal site?\\\\r\\\\nMany folks have an online identity through sources such as Facebook or [LinkedIn](https://www.linkedin.com/in/randy-moore-b552b014/).  Profiles such as these have the benefit of a network engine but limit creative expression.  \\\\r\\\\n\\\\r\\\\nLinkedIn serves as an excellent host to one's professional profile.  The features are tailored to a professional perspective and include an automatic and personalized connection to LinkedIn's ecosystem.  Shortly after creating my LinkedIn profile I was contacted by a LinkedIn recruiter and wound up spending 5 most excellent career years there.  Similarly Facebook automatically provides opportunity to connect to people in your social circles.\\\\r\\\\n\\\\r\\\\nPreexisting sites with a particular focus come with the downside of limiting creative expression.  LinkedIn and Facebook will hold your hand and guide you through creating a profile.  This scripted creation is easy and potentially rewarding but limits you to the purpose and environment of the platform.  Creating a personal website from lower level building blocks offers greater freedom of expression but in the same vein is a daunting endeavor.  Beyond the technical challenges you need to think about content and purpose.  Will it serve as a herald of your professional brand?  You run into tough questions: is it wise to include your personal views as a part of your professional brand?\\\\r\\\\n\\\\r\\\\nFrom experience I have faith in open source to solve the technical challenges.  A plethora of proven building blocks exist.  The greater challenge is content and purpose.  This website will serve as an experiment. From life experience I've noticed many sources recommend to be either hot or cold, not [lukewarm](http://biblehub.com/revelation/3-16.htm)<sup>1</sup>. \\\\r\\\\n\\\\r\\\\nHypothesis:\\\\r\\\\n> The world is a large place.  Exercising creative freedom and following passion will lead to blow back but also growth and purpose.\\\\r\\\\n\\\\r\\\\nThis site will serve as a herald of both my professional *and* personal brand.\\\\r\\\\n\\\\r\\\\n### Creation of the site\\\\r\\\\nThe creation of this site is a story of curiosity, conjecture and a meandering path exploring what open source has to offer.  This post covers up to the initial step of publishing the source code.\\\\r\\\\n\\\\r\\\\nTime at LinkedIn was spent engaged with a Java (EE) centric environment with tooling written mostly in Python.  In response to some time sensitive needs I had decided to use Python scripting and was impressed by how quickly a solution came together for a seemingly overwhelming task.  To boot, Python has been a rising star in the industry.  Python was chosen be the central language of the site.\\\\r\\\\n\\\\r\\\\nTechnologies that encapsulate process have recently spoken to my soul.  Having spent hundreds of hours in my career dealing with development environment issues the ability to encapsulate a the creation of a development environment is particularly appealing.  Dabbling across open source projects you will see many share virtual machine environments (eg using [Vagrant](https://www.vagrantup.com/)) but this approach is still fairly heavyweight (long download times, complexity).  [Docker](https://www.docker.com/) captured my attention as a lightweight alternative to full virtual machines.  Turns out Docker natively supports orchestrating multiple Docker containers into a complete system. Sold!\\\\r\\\\n\\\\r\\\\nThe goal at this point was to create the website within a Docker container so it could be worked on anywhere.  Some poking around led to [Flask](http://flask.pocoo.org/) which promises to be minimal (meaning to me ease of use) but extensible. [Nginx](https://nginx.org/) soon came into focus since the built in Flask webserver does not support serving [more than 1 request at a time](http://flask.pocoo.org/docs/0.12/deploying/).  Nginx is advertised as a lightwight server with a built in [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy) - basically a cache to serve up commonly requested pages that have already been created by the (heavyweight) app framework (Flask).\\\\r\\\\n\\\\r\\\\nInitial Thoughts:\\\", \\\"id\\\": \\\"000b5b3f-71df-4853-be49-127ed59479fd\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 2, \\\"id\\\": \\\"7940ebfc-de7d-432a-b3a6-2ebe999a43f3\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The efficiency promised by Nginx led to dreams of hosting the site on a [Raspberry Pi](https://www.raspberrypi.org/).  Not everything is available for ARM machines, but [Docker seems to be available on the Pi](https://www.raspberrypi.org/blog/docker-comes-to-raspberry-pi/).  The magic sauce to have Nginx serve with Flask is [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface), an API specification. [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) looked like the best choice to most efficiently provide that glue since it supports native communication over sockets (instead of via HTTP or some other relatively heavy layer).\\\\r\\\\n\\\\r\\\\nThe uWSGI implementation is not just code included in a final executable, it works as a (daemon) service.  This threw a monkey wrench in the plan; a typical Docker container generally runs a single process which is tied to the lifecycle of the container.  First attempt began with the [Nginx Docker base image](https://hub.docker.com/_/nginx/) but led to creating and modifying init scripts to bring up uWSGI in addition to Nginx.  After much effort uWSGI would still not be running upon container deployment, time for a different approach.\\\\r\\\\n\\\\r\\\\nSome searching led to a question that bore fruit: how does one correctly manage multiple services within a Docker container?  A solution is provided by [phusion.nl](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/).  After reading the [story](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/) behind the base image there was a wave of relief.  Had my init script hacks worked strange issues would probably have appeared later on.  Thankfully the Nginx Docker base image rejected my hacks by following the design principle\\\\r\\\\n> [Make it easy to use correctly and hard to use incorrectly](http://principles-wiki.net/principles:easy_to_use_and_hard_to_misuse)\\\\r\\\\n\\\\r\\\\n### Sharing the site source code\\\\r\\\\nWhat drives people often isn't the what but the why.  While searching for the meaning of life it's become clear that the path to happiness involves sharing.  Open source has been a central part of my life since installing [Free BSD](https://www.freebsd.org/) on an old computer in the late 90's.  Shortly after that and up to the present [Gentoo Linux](https://gentoo.org/) has served my personal computing needs.  Having received so much from the open source community there's an itch to give back.  My hope is that others may follow the story of this site via these blog posts (more coming soon) and gain a bit of perspective on the open source landscape and the opportunities available to them.\\\\r\\\\n\\\\r\\\\n[Current source code for this site](https://github.com/RandyMoore/website)\\\\r\\\\n\\\\r\\\\n[The commit for a minimal Nginx - uWSGI - Flask Docker container](https://github.com/RandyMoore/website/commit/9690a8908408cc87c52aaed007decd276c7e01e6) (See this [README](https://github.com/RandyMoore/website/blob/f3d5d8f4a006206d8359218ba8544cdc5f8c1224/README.md) for directions)\\\\r\\\\n\\\\r\\\\n###<em>Footnotes</em>\\\\r\\\\n1.  I am spiritual but do not subscribe to a particular religion.\\\", \\\"id\\\": \\\"2b881364-9599-4b21-8d18-0ecf4b0bb04b\\\"}]\", \"subheading\": \"How easy is it to create a personal website using open source?  Why build a personal website?\", \"date\": \"2017-06-05\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 41, "fields": {"page": 7, "submitted_for_moderation": false, "created_at": "2018-05-05T09:50:14.318Z", "user": 1, "content_json": "{\"pk\": 7, \"path\": \"000100020004\", \"depth\": 3, \"numchild\": 0, \"title\": \"Exploring NLP Parsed Audit Documents\", \"draft_title\": \"Exploring NLP Parsed Audit Documents\", \"slug\": \"exploring-nlp-parsed-audit-documents\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/exploring-nlp-parsed-audit-documents/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-11-25T10:02:38.513Z\", \"last_published_at\": \"2017-11-25T14:01:55.378Z\", \"latest_revision_created_at\": \"2017-11-25T14:01:55.344Z\", \"live_revision\": 36, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Inspiration\\\\r\\\\n========\\\\r\\\\nThis project came about for these reasons:\\\\r\\\\n\\\\r\\\\n 1.  Make a positive impact on the world.\\\\r\\\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\\\r\\\\n    *  A first project to gain some experience working with real world open data.\\\\r\\\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\\\r\\\\n 3.  Professional Development - gain experience with some of the latest technologies.\\\\r\\\\n\\\\r\\\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\\\r\\\\n\\\\r\\\\nFinal Product\\\\r\\\\n=============\\\\r\\\\n\\\\r\\\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\\\r\\\\n\\\\r\\\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\\\r\\\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\\\r\\\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\\\r\\\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\\\r\\\\n\\\\r\\\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\\\r\\\\n\\\\r\\\\nLanding Page\\\\r\\\\n-------------------\\\", \\\"id\\\": \\\"39563302-2d00-4dbd-bdcd-6353cbcde4b2\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 19, \\\"id\\\": \\\"c25448a6-6344-4b6e-9b6a-c1207e5febf7\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Named Entity Exploration\\\\\\\" Button...\\\\r\\\\n\\\\r\\\\nStart of Named Entity Exploration\\\\r\\\\n----------------------------------------------\\\", \\\"id\\\": \\\"18fc48fd-2620-4628-9b5a-bce895a02059\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 14, \\\"id\\\": \\\"e144978c-317a-4ef1-9cc4-9476442cdcf1\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Veterans\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nDocuments containing \\\\\\\"Veterans\\\\\\\"\\\\r\\\\n-----------------------------------------------\\\", \\\"id\\\": \\\"68459c9b-0032-41e8-8ba8-e1ac11e8638e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 16, \\\"id\\\": \\\"c94a824b-d7ba-4628-a724-f172b974adf5\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"VISN\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nNarrow scope for documents also containing \\\\\\\"VISN\\\\\\\"\\\\r\\\\n------------------------------------------------------------------------\\\", \\\"id\\\": \\\"4348a42c-cfca-490d-a63d-3fe31aac966c\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 17, \\\"id\\\": \\\"b0561886-8e2d-4430-9956-db11f2cf55c9\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Final Result\\\\r\\\\n----------------\\\\r\\\\nIncluding:\\\\r\\\\n\\\\r\\\\n *  Medical Center\\\\r\\\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\\\", \\\"id\\\": \\\"eb63228f-6f57-44ff-90dc-3a7d385cac3e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 21, \\\"id\\\": \\\"3748efbc-4d49-468c-882c-2d7f51678e63\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\\\r\\\\n\\\\r\\\\n---\\\\r\\\\n\\\\r\\\\nOffline Processing\\\\r\\\\n==================\\\\r\\\\n\\\\r\\\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\\\r\\\\n\\\\r\\\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asynchronous programming within a thread (supported by the language) and [multithreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the best option was to use multi processing.  \\\\r\\\\n\\\\r\\\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure at a given root and processes the files associated with each audit document if that data isn't already in the database.  The documents are processed in parallel, one per core of the host machine.\\\\r\\\\n\\\\r\\\\nThe write of results to Postgres was originally written to occur in the same task after processing the document but this resulted in Django layer concurrency errors.  It would have been complicated and inefficient to coordinate the DB as a shared resource so I opted instead to use a queue.  Each document processing task places the result into a queue.  An additional DB dedicated process consumes from the queue and writes each result to the database sequentially.  The final code was surprisingly simple:\\\\r\\\\n\\\\r\\\\nTop of file:\\\\r\\\\n\\\\r\\\\n    db_queue = None # declared as global to be shared between processes\\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ create the DB queue and start it's process:\\\\r\\\\n\\\\r\\\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\\\r\\\\n    multiprocessing.Process(target=save_to_db).start() # DB dedicated process\\\\r\\\\n\\\\r\\\\nIn process_documents() the results are placed in the queue:\\\\r\\\\n\\\\r\\\\n    db_queue.put((doc, named_entities))\\\\r\\\\n    \\\\r\\\\nIn save_to_db() we loop forever consuming from the queue until None is encountered:\\\\r\\\\n\\\\r\\\\n    while True: \\\\r\\\\n        doc_tuple = db_queue.get()  # blocks if queue is empty\\\\r\\\\n        <exit function if None found>\\\\r\\\\n        <save to DB>        \\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ all cores of the host machine are loaded up using a multiprocessing Pool and once they are finished None is placed in the DB queue to be found by the DB task causing it to exit:\\\\r\\\\n\\\\r\\\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\\\r\\\\n        p.map(func=process_documents, iterable=files) # Processes to process documents\\\\r\\\\n\\\\r\\\\n    db_queue.put(None)\\\\r\\\\n\\\\r\\\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\\\\\"corpus\\\\\\\" learned :)\\\", \\\"id\\\": \\\"3a156f36-80ae-4fad-ab0f-2dd520ba4968\\\"}]\", \"subheading\": \"Learning more Python because my machine is slow\", \"date\": \"2017-11-25\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 42, "fields": {"page": 7, "submitted_for_moderation": false, "created_at": "2018-05-05T12:26:01.434Z", "user": 1, "content_json": "{\"pk\": 7, \"path\": \"000100020004\", \"depth\": 3, \"numchild\": 0, \"title\": \"Exploring NLP Parsed Audit Documents\", \"draft_title\": \"Exploring NLP Parsed Audit Documents\", \"slug\": \"exploring-nlp-parsed-audit-documents\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/exploring-nlp-parsed-audit-documents/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-11-25T10:02:38.513Z\", \"last_published_at\": \"2018-05-05T09:50:14.351Z\", \"latest_revision_created_at\": \"2018-05-05T09:50:14.318Z\", \"live_revision\": 41, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Inspiration\\\\r\\\\n========\\\\r\\\\nThis project came about for these reasons:\\\\r\\\\n\\\\r\\\\n 1.  Make a positive impact on the world.\\\\r\\\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\\\r\\\\n    *  A first project to gain some experience working with real world open data.\\\\r\\\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\\\r\\\\n 3.  Professional Development - gain experience with some of the latest technologies.\\\\r\\\\n\\\\r\\\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\\\r\\\\n\\\\r\\\\nFinal Product\\\\r\\\\n=============\\\\r\\\\n\\\\r\\\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\\\r\\\\n\\\\r\\\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\\\r\\\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\\\r\\\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\\\r\\\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\\\r\\\\n\\\\r\\\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\\\r\\\\n\\\\r\\\\nLanding Page\\\\r\\\\n-------------------\\\", \\\"id\\\": \\\"39563302-2d00-4dbd-bdcd-6353cbcde4b2\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 19, \\\"id\\\": \\\"c25448a6-6344-4b6e-9b6a-c1207e5febf7\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Named Entity Exploration\\\\\\\" Button...\\\\r\\\\n\\\\r\\\\nStart of Named Entity Exploration\\\\r\\\\n----------------------------------------------\\\", \\\"id\\\": \\\"18fc48fd-2620-4628-9b5a-bce895a02059\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 14, \\\"id\\\": \\\"e144978c-317a-4ef1-9cc4-9476442cdcf1\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Veterans\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nDocuments containing \\\\\\\"Veterans\\\\\\\"\\\\r\\\\n-----------------------------------------------\\\", \\\"id\\\": \\\"68459c9b-0032-41e8-8ba8-e1ac11e8638e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 16, \\\"id\\\": \\\"c94a824b-d7ba-4628-a724-f172b974adf5\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"VISN\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nNarrow scope for documents also containing \\\\\\\"VISN\\\\\\\"\\\\r\\\\n------------------------------------------------------------------------\\\", \\\"id\\\": \\\"4348a42c-cfca-490d-a63d-3fe31aac966c\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 17, \\\"id\\\": \\\"b0561886-8e2d-4430-9956-db11f2cf55c9\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Final Result\\\\r\\\\n----------------\\\\r\\\\nIncluding:\\\\r\\\\n\\\\r\\\\n *  Medical Center\\\\r\\\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\\\", \\\"id\\\": \\\"eb63228f-6f57-44ff-90dc-3a7d385cac3e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 21, \\\"id\\\": \\\"3748efbc-4d49-468c-882c-2d7f51678e63\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\\\r\\\\n\\\\r\\\\n---\\\\r\\\\n\\\\r\\\\nOffline Processing\\\\r\\\\n==================\\\\r\\\\n\\\\r\\\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\\\r\\\\n\\\\r\\\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asynchronous programming within a thread (supported by the language) and [multithreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the best option was to use multi processing.  \\\\r\\\\n\\\\r\\\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure at a given root and processes the files associated with each audit document if that data isn't already in the database.  The documents are processed in parallel, one per core of the host machine.\\\\r\\\\n\\\\r\\\\nThe write of results to Postgres was originally written to occur in the same task after processing the document but this resulted in Django layer concurrency errors.  It would have been complicated and inefficient to coordinate the DB as a shared resource so I opted instead to use a queue.  Each document processing task places the result into a queue.  An additional DB dedicated process consumes from the queue and writes each result to the database sequentially.  The final code was surprisingly simple:\\\\r\\\\n\\\\r\\\\nTop of file:\\\\r\\\\n\\\\r\\\\n    db_queue = None # declared as global to be shared between processes\\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ create the DB queue and start it's process:\\\\r\\\\n\\\\r\\\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\\\r\\\\n    multiprocessing.Process(target=save_to_db).start() # DB dedicated process\\\\r\\\\n\\\\r\\\\nIn process_documents() the results are placed in the queue:\\\\r\\\\n\\\\r\\\\n    db_queue.put((doc, named_entities))\\\\r\\\\n    \\\\r\\\\nIn save_to_db() we loop forever consuming from the queue until None is encountered:\\\\r\\\\n\\\\r\\\\n    while True: \\\\r\\\\n        doc_tuple = db_queue.get()  # blocks if queue is empty\\\\r\\\\n        <exit function if None found>\\\\r\\\\n        <save to DB>        \\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ all cores of the host machine are loaded up using a multiprocessing Pool and once they are finished None is placed in the DB queue to be found by the DB task causing it to exit:\\\\r\\\\n\\\\r\\\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\\\r\\\\n        p.map(func=process_documents, iterable=files) # Processes to process documents\\\\r\\\\n\\\\r\\\\n    db_queue.put(None)\\\\r\\\\n\\\\r\\\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\\\\\"corpus\\\\\\\" learned :)\\\", \\\"id\\\": \\\"3a156f36-80ae-4fad-ab0f-2dd520ba4968\\\"}]\", \"subheading\": \"Learning more Python because my machine is slow\", \"date\": \"2017-11-25\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 43, "fields": {"page": 7, "submitted_for_moderation": false, "created_at": "2018-05-05T12:26:08.371Z", "user": 1, "content_json": "{\"pk\": 7, \"path\": \"000100020004\", \"depth\": 3, \"numchild\": 0, \"title\": \"Exploring NLP Parsed Audit Documents\", \"draft_title\": \"Exploring NLP Parsed Audit Documents\", \"slug\": \"exploring-nlp-parsed-audit-documents\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": true, \"url_path\": \"/randy-moore/exploring-nlp-parsed-audit-documents/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-11-25T10:02:38.513Z\", \"last_published_at\": \"2018-05-05T09:50:14.351Z\", \"latest_revision_created_at\": \"2018-05-05T12:26:01.434Z\", \"live_revision\": 41, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Inspiration\\\\r\\\\n========\\\\r\\\\nThis project came about for these reasons:\\\\r\\\\n\\\\r\\\\n 1.  Make a positive impact on the world.\\\\r\\\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\\\r\\\\n    *  A first project to gain some experience working with real world open data.\\\\r\\\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\\\r\\\\n 3.  Professional Development - gain experience with some of the latest technologies.\\\\r\\\\n\\\\r\\\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\\\r\\\\n\\\\r\\\\nFinal Product\\\\r\\\\n=============\\\\r\\\\n\\\\r\\\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\\\r\\\\n\\\\r\\\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\\\r\\\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\\\r\\\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\\\r\\\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\\\r\\\\n\\\\r\\\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\\\r\\\\n\\\\r\\\\nLanding Page\\\\r\\\\n-------------------\\\", \\\"id\\\": \\\"39563302-2d00-4dbd-bdcd-6353cbcde4b2\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 19, \\\"id\\\": \\\"c25448a6-6344-4b6e-9b6a-c1207e5febf7\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Named Entity Exploration\\\\\\\" Button...\\\\r\\\\n\\\\r\\\\nStart of Named Entity Exploration\\\\r\\\\n----------------------------------------------\\\", \\\"id\\\": \\\"18fc48fd-2620-4628-9b5a-bce895a02059\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 14, \\\"id\\\": \\\"e144978c-317a-4ef1-9cc4-9476442cdcf1\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"Veterans\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nDocuments containing \\\\\\\"Veterans\\\\\\\"\\\\r\\\\n-----------------------------------------------\\\", \\\"id\\\": \\\"68459c9b-0032-41e8-8ba8-e1ac11e8638e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 16, \\\"id\\\": \\\"c94a824b-d7ba-4628-a724-f172b974adf5\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Click the \\\\\\\"VISN\\\\\\\" button...\\\\r\\\\n\\\\r\\\\nNarrow scope for documents also containing \\\\\\\"VISN\\\\\\\"\\\\r\\\\n------------------------------------------------------------------------\\\", \\\"id\\\": \\\"4348a42c-cfca-490d-a63d-3fe31aac966c\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 17, \\\"id\\\": \\\"b0561886-8e2d-4430-9956-db11f2cf55c9\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Final Result\\\\r\\\\n----------------\\\\r\\\\nIncluding:\\\\r\\\\n\\\\r\\\\n *  Medical Center\\\\r\\\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\\\", \\\"id\\\": \\\"eb63228f-6f57-44ff-90dc-3a7d385cac3e\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 21, \\\"id\\\": \\\"3748efbc-4d49-468c-882c-2d7f51678e63\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\\\r\\\\n\\\\r\\\\n---\\\\r\\\\n\\\\r\\\\nOffline Processing\\\\r\\\\n==================\\\\r\\\\n\\\\r\\\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\\\r\\\\n\\\\r\\\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asynchronous programming within a thread (supported by the language) and [multithreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the best option was to use multi processing.  \\\\r\\\\n\\\\r\\\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure at a given root and processes the files associated with each audit document if that data isn't already in the database.  The documents are processed in parallel, one per core of the host machine.\\\\r\\\\n\\\\r\\\\nThe write of results to Postgres was originally written to occur in the same task after processing the document but this resulted in Django layer concurrency errors.  It would have been complicated and inefficient to coordinate the DB as a shared resource so I opted instead to use a queue.  Each document processing task places the result into a queue.  An additional DB dedicated process consumes from the queue and writes each result to the database sequentially.  The final code was surprisingly simple:\\\\r\\\\n\\\\r\\\\nTop of file:\\\\r\\\\n\\\\r\\\\n    db_queue = None # declared as global to be shared between processes\\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ create the DB queue and start it's process:\\\\r\\\\n\\\\r\\\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\\\r\\\\n    multiprocessing.Process(target=save_to_db).start() # DB dedicated process\\\\r\\\\n\\\\r\\\\nIn process_documents() the results are placed in the queue:\\\\r\\\\n\\\\r\\\\n    db_queue.put((doc, named_entities))\\\\r\\\\n    \\\\r\\\\nIn save_to_db() we loop forever consuming from the queue until None is encountered:\\\\r\\\\n\\\\r\\\\n    while True: \\\\r\\\\n        doc_tuple = db_queue.get()  # blocks if queue is empty\\\\r\\\\n        <exit function if None found>\\\\r\\\\n        <save to DB>        \\\\r\\\\n    \\\\r\\\\nIn \\\\\\\\_\\\\\\\\_main\\\\\\\\_\\\\\\\\_ all cores of the host machine are loaded up using a multiprocessing Pool and once they are finished None is placed in the DB queue to be found by the DB task causing it to exit:\\\\r\\\\n\\\\r\\\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\\\r\\\\n        p.map(func=process_documents, iterable=files) # Processes to process documents\\\\r\\\\n\\\\r\\\\n    db_queue.put(None)\\\\r\\\\n\\\\r\\\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\\\\\"corpus\\\\\\\" learned :)\\\", \\\"id\\\": \\\"3a156f36-80ae-4fad-ab0f-2dd520ba4968\\\"}]\", \"subheading\": \"Learning more Python because my machine is slow\", \"date\": \"2017-11-25\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 44, "fields": {"page": 4, "submitted_for_moderation": false, "created_at": "2018-05-05T12:33:19.916Z", "user": 1, "content_json": "{\"pk\": 4, \"path\": \"000100020001\", \"depth\": 3, \"numchild\": 0, \"title\": \"Genesis\", \"draft_title\": \"Genesis\", \"slug\": \"genesis\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/genesis/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-07-29T21:11:03.825Z\", \"last_published_at\": \"2018-05-05T09:49:29.269Z\", \"latest_revision_created_at\": \"2018-05-05T09:49:29.235Z\", \"live_revision\": 40, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"### Why build a personal site?\\\\r\\\\nMany folks have an online identity through sources such as Facebook or [LinkedIn](https://www.linkedin.com/in/randy-moore-b552b014/).  Profiles such as these have the benefit of a network engine but limit creative expression.  \\\\r\\\\n\\\\r\\\\nLinkedIn serves as an excellent host to one's professional profile.  The features are tailored to a professional perspective and include an automatic and personalized connection to LinkedIn's ecosystem.  Shortly after creating my LinkedIn profile I was contacted by a LinkedIn recruiter and wound up spending 5 most excellent career years there.  Similarly Facebook automatically provides opportunity to connect to people in your social circles.\\\\r\\\\n\\\\r\\\\nPreexisting sites with a particular focus come with the downside of limiting creative expression.  LinkedIn and Facebook will hold your hand and guide you through creating a profile.  This scripted creation is easy and potentially rewarding but limits you to the purpose and environment of the platform.  Creating a personal website from lower level building blocks offers greater freedom of expression but in the same vein is a daunting endeavor.  Beyond the technical challenges you need to think about content and purpose.  Will it serve as a herald of your professional brand?  You run into tough questions: is it wise to include your personal views as a part of your professional brand?\\\\r\\\\n\\\\r\\\\nFrom experience I have faith in open source to solve the technical challenges.  A plethora of proven building blocks exist.  The greater challenge is content and purpose.  This website will serve as an experiment. From life experience I've noticed many sources recommend to be either hot or cold, not [lukewarm](http://biblehub.com/revelation/3-16.htm)<sup>1</sup>. \\\\r\\\\n\\\\r\\\\nHypothesis:\\\\r\\\\n> The world is a large place.  Exercising creative freedom and following passion will lead to blow back but also growth and purpose.\\\\r\\\\n\\\\r\\\\nThis site will serve as a herald of both my professional *and* personal brand.\\\\r\\\\n\\\\r\\\\n### Creation of the site\\\\r\\\\nThe creation of this site is a story of curiosity, conjecture and a meandering path exploring what open source has to offer.  This post covers up to the initial step of publishing the source code.\\\\r\\\\n\\\\r\\\\nTime at LinkedIn was spent engaged with a Java (EE) centric environment with tooling written mostly in Python.  In response to some time sensitive needs I had decided to use Python scripting and was impressed by how quickly a solution came together for a seemingly overwhelming task.  To boot, Python has been a rising star in the industry.  Python was chosen be the central language of the site.\\\\r\\\\n\\\\r\\\\nTechnologies that encapsulate process have recently spoken to my soul.  Having spent hundreds of hours in my career dealing with development environment issues the ability to encapsulate a the creation of a development environment is particularly appealing.  Dabbling across open source projects you will see many share virtual machine environments (eg using [Vagrant](https://www.vagrantup.com/)) but this approach is still fairly heavyweight (long download times, complexity).  [Docker](https://www.docker.com/) captured my attention as a lightweight alternative to full virtual machines.  Turns out Docker natively supports orchestrating multiple Docker containers into a complete system. Sold!\\\\r\\\\n\\\\r\\\\nThe goal at this point was to create the website within a Docker container so it could be worked on anywhere.  Some poking around led to [Flask](http://flask.pocoo.org/) which promises to be minimal (meaning to me ease of use) but extensible. [Nginx](https://nginx.org/) soon came into focus since the built in Flask webserver does not support serving [more than 1 request at a time](http://flask.pocoo.org/docs/0.12/deploying/).  Nginx is advertised as a lightwight server with a built in [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy) - basically a cache to serve up commonly requested pages that have already been created by the (heavyweight) app framework (Flask).\\\\r\\\\n\\\\r\\\\nInitial Thoughts:\\\", \\\"id\\\": \\\"000b5b3f-71df-4853-be49-127ed59479fd\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 2, \\\"id\\\": \\\"7940ebfc-de7d-432a-b3a6-2ebe999a43f3\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The efficiency promised by Nginx led to dreams of hosting the site on a [Raspberry Pi](https://www.raspberrypi.org/).  Not everything is available for ARM machines, but [Docker seems to be available on the Pi](https://www.raspberrypi.org/blog/docker-comes-to-raspberry-pi/).  The magic sauce to have Nginx serve with Flask is [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface), an API specification. [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) looked like the best choice to most efficiently provide that glue since it supports native communication over sockets (instead of via HTTP or some other relatively heavy layer).\\\\r\\\\n\\\\r\\\\nThe uWSGI implementation is not just code included in a final executable, it works as a (daemon) service.  This threw a monkey wrench in the plan; a typical Docker container generally runs a single process which is tied to the lifecycle of the container.  First attempt began with the [Nginx Docker base image](https://hub.docker.com/_/nginx/) but led to creating and modifying init scripts to bring up uWSGI in addition to Nginx.  After much effort uWSGI would still not be running upon container deployment, time for a different approach.\\\\r\\\\n\\\\r\\\\nSome searching led to a question that bore fruit: how does one correctly manage multiple services within a Docker container?  A solution is provided by [phusion.nl](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/).  After reading the [story](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/) behind the base image there was a wave of relief.  Had my init script hacks worked strange issues would probably have appeared later on.  Thankfully the Nginx Docker base image rejected my hacks by following the design principle\\\\r\\\\n> [Make it easy to use correctly and hard to use incorrectly](http://principles-wiki.net/principles:easy_to_use_and_hard_to_misuse)\\\\r\\\\n\\\\r\\\\n### Sharing the site source code\\\\r\\\\nWhat drives people often isn't the what but the why.  While searching for the meaning of life it's become clear that the path to happiness involves sharing.  Open source has been a central part of my life since installing [Free BSD](https://www.freebsd.org/) on an old computer in the late 90's.  Shortly after that and up to the present [Gentoo Linux](https://gentoo.org/) has served my personal computing needs.  Having received so much from the open source community there's an itch to give back.  My hope is that others may follow the story of this site via these blog posts (more coming soon) and gain a bit of perspective on the open source landscape and the opportunities available to them.\\\\r\\\\n\\\\r\\\\n[Current source code for this site](https://github.com/RandyMoore/website)\\\\r\\\\n\\\\r\\\\n[The commit for a minimal Nginx - uWSGI - Flask Docker container](https://github.com/RandyMoore/website/commit/9690a8908408cc87c52aaed007decd276c7e01e6) (See this [README](https://github.com/RandyMoore/website/blob/f3d5d8f4a006206d8359218ba8544cdc5f8c1224/README.md) for directions)\\\\r\\\\n\\\\r\\\\n###<em>Footnotes</em>\\\\r\\\\n1.  I am spiritual but do not subscribe to a particular religion.\\\", \\\"id\\\": \\\"2b881364-9599-4b21-8d18-0ecf4b0bb04b\\\"}]\", \"subheading\": \"How easy is it to create a personal website using open source?  Why build a personal website?\", \"date\": \"2017-06-05\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 45, "fields": {"page": 4, "submitted_for_moderation": false, "created_at": "2018-05-05T12:33:24.181Z", "user": 1, "content_json": "{\"pk\": 4, \"path\": \"000100020001\", \"depth\": 3, \"numchild\": 0, \"title\": \"Genesis\", \"draft_title\": \"Genesis\", \"slug\": \"genesis\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": true, \"url_path\": \"/randy-moore/genesis/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-07-29T21:11:03.825Z\", \"last_published_at\": \"2018-05-05T09:49:29.269Z\", \"latest_revision_created_at\": \"2018-05-05T12:33:19.916Z\", \"live_revision\": 40, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"### Why build a personal site?\\\\r\\\\nMany folks have an online identity through sources such as Facebook or [LinkedIn](https://www.linkedin.com/in/randy-moore-b552b014/).  Profiles such as these have the benefit of a network engine but limit creative expression.  \\\\r\\\\n\\\\r\\\\nLinkedIn serves as an excellent host to one's professional profile.  The features are tailored to a professional perspective and include an automatic and personalized connection to LinkedIn's ecosystem.  Shortly after creating my LinkedIn profile I was contacted by a LinkedIn recruiter and wound up spending 5 most excellent career years there.  Similarly Facebook automatically provides opportunity to connect to people in your social circles.\\\\r\\\\n\\\\r\\\\nPreexisting sites with a particular focus come with the downside of limiting creative expression.  LinkedIn and Facebook will hold your hand and guide you through creating a profile.  This scripted creation is easy and potentially rewarding but limits you to the purpose and environment of the platform.  Creating a personal website from lower level building blocks offers greater freedom of expression but in the same vein is a daunting endeavor.  Beyond the technical challenges you need to think about content and purpose.  Will it serve as a herald of your professional brand?  You run into tough questions: is it wise to include your personal views as a part of your professional brand?\\\\r\\\\n\\\\r\\\\nFrom experience I have faith in open source to solve the technical challenges.  A plethora of proven building blocks exist.  The greater challenge is content and purpose.  This website will serve as an experiment. From life experience I've noticed many sources recommend to be either hot or cold, not [lukewarm](http://biblehub.com/revelation/3-16.htm)<sup>1</sup>. \\\\r\\\\n\\\\r\\\\nHypothesis:\\\\r\\\\n> The world is a large place.  Exercising creative freedom and following passion will lead to blow back but also growth and purpose.\\\\r\\\\n\\\\r\\\\nThis site will serve as a herald of both my professional *and* personal brand.\\\\r\\\\n\\\\r\\\\n### Creation of the site\\\\r\\\\nThe creation of this site is a story of curiosity, conjecture and a meandering path exploring what open source has to offer.  This post covers up to the initial step of publishing the source code.\\\\r\\\\n\\\\r\\\\nTime at LinkedIn was spent engaged with a Java (EE) centric environment with tooling written mostly in Python.  In response to some time sensitive needs I had decided to use Python scripting and was impressed by how quickly a solution came together for a seemingly overwhelming task.  To boot, Python has been a rising star in the industry.  Python was chosen be the central language of the site.\\\\r\\\\n\\\\r\\\\nTechnologies that encapsulate process have recently spoken to my soul.  Having spent hundreds of hours in my career dealing with development environment issues the ability to encapsulate a the creation of a development environment is particularly appealing.  Dabbling across open source projects you will see many share virtual machine environments (eg using [Vagrant](https://www.vagrantup.com/)) but this approach is still fairly heavyweight (long download times, complexity).  [Docker](https://www.docker.com/) captured my attention as a lightweight alternative to full virtual machines.  Turns out Docker natively supports orchestrating multiple Docker containers into a complete system. Sold!\\\\r\\\\n\\\\r\\\\nThe goal at this point was to create the website within a Docker container so it could be worked on anywhere.  Some poking around led to [Flask](http://flask.pocoo.org/) which promises to be minimal (meaning to me ease of use) but extensible. [Nginx](https://nginx.org/) soon came into focus since the built in Flask webserver does not support serving [more than 1 request at a time](http://flask.pocoo.org/docs/0.12/deploying/).  Nginx is advertised as a lightwight server with a built in [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy) - basically a cache to serve up commonly requested pages that have already been created by the (heavyweight) app framework (Flask).\\\\r\\\\n\\\\r\\\\nInitial Thoughts:\\\", \\\"id\\\": \\\"000b5b3f-71df-4853-be49-127ed59479fd\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 2, \\\"id\\\": \\\"7940ebfc-de7d-432a-b3a6-2ebe999a43f3\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The efficiency promised by Nginx led to dreams of hosting the site on a [Raspberry Pi](https://www.raspberrypi.org/).  Not everything is available for ARM machines, but [Docker seems to be available on the Pi](https://www.raspberrypi.org/blog/docker-comes-to-raspberry-pi/).  The magic sauce to have Nginx serve with Flask is [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface), an API specification. [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) looked like the best choice to most efficiently provide that glue since it supports native communication over sockets (instead of via HTTP or some other relatively heavy layer).\\\\r\\\\n\\\\r\\\\nThe uWSGI implementation is not just code included in a final executable, it works as a (daemon) service.  This threw a monkey wrench in the plan; a typical Docker container generally runs a single process which is tied to the lifecycle of the container.  First attempt began with the [Nginx Docker base image](https://hub.docker.com/_/nginx/) but led to creating and modifying init scripts to bring up uWSGI in addition to Nginx.  After much effort uWSGI would still not be running upon container deployment, time for a different approach.\\\\r\\\\n\\\\r\\\\nSome searching led to a question that bore fruit: how does one correctly manage multiple services within a Docker container?  A solution is provided by [phusion.nl](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/).  After reading the [story](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/) behind the base image there was a wave of relief.  Had my init script hacks worked strange issues would probably have appeared later on.  Thankfully the Nginx Docker base image rejected my hacks by following the design principle\\\\r\\\\n> [Make it easy to use correctly and hard to use incorrectly](http://principles-wiki.net/principles:easy_to_use_and_hard_to_misuse)\\\\r\\\\n\\\\r\\\\n### Sharing the site source code\\\\r\\\\nWhat drives people often isn't the what but the why.  While searching for the meaning of life it's become clear that the path to happiness involves sharing.  Open source has been a central part of my life since installing [Free BSD](https://www.freebsd.org/) on an old computer in the late 90's.  Shortly after that and up to the present [Gentoo Linux](https://gentoo.org/) has served my personal computing needs.  Having received so much from the open source community there's an itch to give back.  My hope is that others may follow the story of this site via these blog posts (more coming soon) and gain a bit of perspective on the open source landscape and the opportunities available to them.\\\\r\\\\n\\\\r\\\\n[Current source code for this site](https://github.com/RandyMoore/website)\\\\r\\\\n\\\\r\\\\n[The commit for a minimal Nginx - uWSGI - Flask Docker container](https://github.com/RandyMoore/website/commit/9690a8908408cc87c52aaed007decd276c7e01e6) (See this [README](https://github.com/RandyMoore/website/blob/f3d5d8f4a006206d8359218ba8544cdc5f8c1224/README.md) for directions)\\\\r\\\\n\\\\r\\\\n###<em>Footnotes</em>\\\\r\\\\n1.  I am spiritual but do not subscribe to a particular religion.\\\", \\\"id\\\": \\\"2b881364-9599-4b21-8d18-0ecf4b0bb04b\\\"}]\", \"subheading\": \"How easy is it to create a personal website using open source?  Why build a personal website?\", \"date\": \"2017-06-05\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 46, "fields": {"page": 6, "submitted_for_moderation": false, "created_at": "2018-05-05T12:33:46.051Z", "user": 1, "content_json": "{\"pk\": 6, \"path\": \"000100020003\", \"depth\": 3, \"numchild\": 0, \"title\": \"Full Stack Walkthrough\", \"draft_title\": \"Full Stack Walkthrough\", \"slug\": \"full-stack-walkthrough\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/full-stack-walkthrough/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-10-02T20:33:57.085Z\", \"last_published_at\": \"2018-05-05T09:49:06.599Z\", \"latest_revision_created_at\": \"2018-05-05T09:49:06.565Z\", \"live_revision\": 39, \"body\": \"[{\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Summary</h3><p>High level development walk through for a toy example of a modern full web application stack. Intended for those who want an idea of what basic knowledge and skills a &quot;Full Stack Engineer&quot; would have without being bogged down with the knowledge required for a real world project. Some scope of additional knowledge required for a real world project (team size &gt; 1, customer facing features, security, scale-ability, ...) is mentioned.</p><p>The complete source code of the example project is available at <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango\\\\\\\">https://github.com/RandyMoore/mySiteDjango</a></p><h3>Requirements</h3><p>Before starting out on any project it is important to clearly define what the goal is. In this case a personal website serving as a space to learn, experiment, and showcase web technologies. The site serves as an example for those wishing to learn the technology and so the project should be as simple and understandable as possible. The site also serves as a professional portfolio and so should be visually appealing to a non-technical audience.</p><h3>Architecture</h3><p>The requirements drive the architecture. The user roles consist of the content creator and the content viewer. The content is static and not sensitive which simplifies several aspects of the architecture. We don&#x27;t need to be concerned with growing storage demands or security since there is nothing sensitive to protect. Another requirement is deployment flexibility compatible with the major cloud service providers for low cost high performance hosting. Someone wishing to learn the technology should also be able to easily deploy the site locally so they can experiment with it. A two tier architecture (web server and database layers) encapsulated in a deployable container fits the bill for this use case.</p>\\\", \\\"id\\\": \\\"204d1b56-0988-44f0-bf0f-54d56d395837\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 5, \\\"id\\\": \\\"69d402a6-ff31-4afb-b921-7e34a5812afc\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Languages</h3><p>Choice of programming languages impacts the set of existing software you can choose to build your project from. Language and existing software choice (framework, libraries) are somewhat of a chicken and egg problem, start with which you feel strongest about. Web applications are broadly split into two parts, the front end and back end. The front end part of the project is run by the viewer&#x27;s web browser. The back end is run by the computing resources provided by the cloud service provider.</p><p>There&#x27;s less of a language choice for the front end which is driven by the need for compatibility to run on a variety of web browsers. For a web page HTML is required for the structured layout of content; CSS and Javascript are optional for separating out style and adding behavior respectively. There are many other front end languages you might encounter (e.g. CoffeScript, SASS) but these generally compile down to some older version of Javascript or CSS before it is served to the viewer&#x27;s browser.</p><p>For the back end, anything goes. The client&#x27;s web browser will contact the server using <a href=\\\\\\\"https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol\\\\\\\">HTTP</a>. The web browser has no knowledge of what is happening on the other side of this HTTP interface and so you are free to choose whatever language you wish. But you do need to comply with the HTTP specification, which is rather large. Implementing the behavior required by the HTTP specification using only primitive language features would be an impractical task. Enter frameworks.</p><h3>Frameworks</h3><p>HTTP is well defined in a specification and this has allowed others to write reusable software to take care of the HTTP details. Frameworks are language specific - they provide a bare-bones system in a particular language and allow you to add your own code to add custom behavior. You may already be familiar with the concept of a library. A framework is similar in that it is reusable code but is different because it drives control instead of responding to commands as a library would. In this case the framework code will be the first to handle the incoming request from the viewer&#x27;s web browser and then it will call your code to form a reply. <a href=\\\\\\\"https://www.djangoproject.com/\\\\\\\">Django</a> was chosen as the web framework for this project. Here&#x27;s a visualization of framework vs library:</p>\\\", \\\"id\\\": \\\"d9bc862c-31dc-4484-ad05-64250a252e01\\\"}, {\\\"type\\\": \\\"html\\\", \\\"value\\\": \\\"<iframe width=\\\\\\\"560\\\\\\\" height=\\\\\\\"315\\\\\\\" src=\\\\\\\"https://www.youtube.com/embed/lC0KapQU0aM\\\\\\\" frameborder=\\\\\\\"0\\\\\\\" allowfullscreen></iframe>\\\", \\\"id\\\": \\\"3e30cd2a-16c7-4ec2-9e0e-00fadbc12e4a\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Libraries</h3><p>The next step in the project is deciding what content should be added. A blog is a standard way for professional software developers to market their brand; the blog may be used to write about each project as it is added to the site. A naive approach is to write a single HTML file for each blog post. But writing HTML code can be cumbersome and having each post as a stand-alone HTML file might cause problems in the future if you want to change the style of all the posts; each file would have to changed.</p><p>Writing HTML by hand can be a good fit for simple data content that isn&#x27;t meant to be pretty. Creating something that has appealing style requires design skill. Fortunately libraries exist that encapsulate design. A quick search yields a <a href=\\\\\\\"https://startbootstrap.com/template-overviews/clean-blog/\\\\\\\">Bootstrap clean-blog design</a>. This takes care of the design aspect of the blog.</p><p>People have also written reusable software for the content aspect of things like blogs. This kind of software is known as a <a href=\\\\\\\"https://en.wikipedia.org/wiki/Content_management_system\\\\\\\">Content Management System</a> (CMS). Generally CMS is used as a means to separate content creation from the technical details of a site. Content specialists (e.g. journalists) may add content without becoming mired in technical details. CMS systems usually include WYSIWYG (What You See Is What You Get) editors; one way to avoid writing raw HTML. CMS systems are generally dependent on the web framework since there is a lot of glue code within the framework that has to interface with the various facets of the content (html generation, image resources, URL paths managed by the CMS, ...). Some searching yielded <a href=\\\\\\\"https://wagtail.io/\\\\\\\">Wagtail</a> as a CMS for use with Django. Perhaps a bit overkill for this site (having a team size of 1) but I was curious about CMS technology and wanted to experiment with a full fledged example of this technology.</p><h3>Development</h3><p>For a team size of one enough has been decided at this point to begin working with code. Generally a development environment is built based on choice of language since each language has it&#x27;s own paradigm of what a development environment is. For the Python back end we use <a href=\\\\\\\"https://pypi.python.org/pypi/pip\\\\\\\">pip</a> to fetch and install project requirements (existing reusable software components). A common concern with development environments is isolation between multiple projects on the same machine. We use <a href=\\\\\\\"https://virtualenv.pypa.io/en/stable/\\\\\\\">VirtualEnv</a> to keep all libraries and frameworks from interfering with each other and the global Python environment on our development machine.</p><p>Thought should be given regarding project structure. The chosen framework will often dictate this since it needs to know where to find your extensions but often your project will have files that exist outside of the framework. For example, in this project the declaration for which Python frameworks and libraries to install are kept in a file <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/config/requirements.txt\\\\\\\">requirements.txt</a> that can be fed to pip. This and other configuration files that affect the deployed code are kept in a folder name <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/config\\\\\\\">config</a> at the highest level of the project. The <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/my_site_django\\\\\\\">Django framework structure</a> begins as a sibling at same level since it is also a component of the deployed code.</p><p>A modern trend with languages and frameworks is to provide a REPL (Read Evaluate Print Loop). Languages (ie Python) often have a shell that evaluate commands this way, providing an interactive environment that encourages experimentation and exploration. Django and many other frameworks mimic a REPL with hot reloading. In Django, this is provided by the built-in development server (&#x27;python manage.py runserver&#x27;). Anytime you make a change to your code the changes are quickly and automatically ingested by the server; reflected the next time an HTTP request is processed. This feedback loop allows you to incrementally add and see small changes to the site, speeding development.</p><p>To edit code many people find an Integrated Development Environment (IDE) helpful. The IDE adds smarts and tools to your basic text editor. &quot;smarts&quot; meaning it is aware of the code in your project and can automatically recognize relationships between pieces of your code and also library or framework code. The IDE allows you to quickly navigate through execution paths to quickly understand how the pieces work together. I&#x27;m a fan of IDEA products, their community edition (CE) PyCharm is great for Python centric projects. The &#x27;python manage.py runserver&#x27; command may be run in a debugging session within the IDE, making it trivial to debug and explore the inner workings of the server code.</p><p>PyCharm CE doesn&#x27;t support front end languages well so I&#x27;ve been using (learning) the Atom editor for front end work. As mentioned before, front end programming languages generally get compiled down to an older version of HTML, CSS and Javascript to maximize browser compatibility. The step of converting your modern code into these older language versions is called &quot;bundling&quot;. For the front end development environment this project uses <a href=\\\\\\\"https://en.wikipedia.org/wiki/JavaScript\\\\\\\">Javascript</a> with <a href=\\\\\\\"https://nodejs.org/en/\\\\\\\">Node.js</a> as the engine to execute the Javascript (instead of a web browser). Similar to pip for Python exists <a href=\\\\\\\"https://www.npmjs.com/\\\\\\\">npm</a> (Node Package Manager) for Javascript (<a href=\\\\\\\"https://yarnpkg.com/en/\\\\\\\">yarn</a> may be used interchangeably with npm if npm fails to install something).</p><p>In this project the Javascript dependencies are recorded at the top level in a <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/package.json\\\\\\\">package.json</a> file. <a href=\\\\\\\"https://gruntjs.com/\\\\\\\">Grunt</a> is a Javascript program that serves as a way to glue together the steps of the bundling process. The tasks are declared in the project level <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Gruntfile.js\\\\\\\">Gruntfile.js</a>. The default task invoked with &#x27;grunt&#x27; may be run while editing code and serves as a REPL; when any front end related file changes it invokes the rebundling process and the changes will be reflected by the development server after a browser page refresh. For debugging the developer tools available in the browser are used (available by default in modern browsers).</p><h3>Project Management</h3><p>After writing some code and getting something basic to work you will worry that making further changes will break what you&#x27;ve already accomplished. Enter version control systems. Such systems allow you to track changes, moving between versions of your code at will. Version control becomes more of a concern as team size increases, but even with a team of size of one they are worth using. For example you can use branching to work on a major new feature while having the flexibility to quickly tweak the version of code that is currently deployed and being viewed by the public. This project uses <a href=\\\\\\\"https://git-scm.com/\\\\\\\">git</a>. GitHub (a service provided by a company) is used as a publicly accessible repository since it is well suited for open source projects.</p><p>Project management consists of far more than version control and becomes increasingly important as team size grows. At a high level a team decides on a <a href=\\\\\\\"https://en.wikipedia.org/wiki/Software_development_process\\\\\\\">development methodology</a> and refines a specific software development process to meet the project needs. The choice of methodology and process can influence what technologies to use and so is usually decided early in the project&#x27;s life cycle. The art of harnessing the work of multiple individual developers is it&#x27;s own subject and best learned from experience working with a team.</p><h3>Deployment</h3><p>Once you have something you&#x27;d like to show to the public the next step is to serve it to the world. Technically you could serve from the your development machine but keeping your development machine running 100% of the time to run the site would likely be a nuisance. You could also serve from a different machine that you own but this comes with a great deal of complexity and will probably expose your personal network to a high level of security risk. Additionally your home network is likely on the fringe of the world wide web which makes it slow to respond.</p><p>Fortunately companies offering cloud services exist which allow you to host arbitrary software and take care of the messy details of hosting (networking, security, computing resource allocation ...). The idea is that the hosting company figures out how to manage all of the hosting details for many users and then provides hosting as a service, adding value with economy of scale. A popular example is <a href=\\\\\\\"https://aws.amazon.com/\\\\\\\">Amazon Web Services</a> (AWS), but many vendors exist. Even with the hosting details taken care of there is still the issue of how to install your software on the cloud.</p><p>Accessing running software on the cloud isn&#x27;t as convenient or seamless as your local development environment. Changing anything almost always guarantees bugs. Wouldn&#x27;t it be awesome if you could draw a box around the code running on your development machine and simply drop it into the cloud? Then changing to the cloud environment would introduce minimal changes (seen from the perspective of your code) and result in fewer bugs. <a href=\\\\\\\"https://en.wikipedia.org/wiki/Abstraction_(software_engineering)\\\\\\\">Abstraction</a> comes to the rescue: why not abstract a machine, or more precisely the operating system? The technology that does this has existed for some time and is know as Virtualization. Virtual machines came about early on but are cumbersome due to their large size from replicating an entire OS. A much lighter weight alternative is <a href=\\\\\\\"https://www.docker.com/\\\\\\\">Docker</a>, which this project uses.</p><p>The Docker paradigm is to have a single app with all dependencies running in a single container and to tie multiple containers together to form a system. This is an example of the <a href=\\\\\\\"https://en.wikipedia.org/wiki/Single_responsibility_principle\\\\\\\">Single Responsibility Principle</a> which gives rise to many desirable design traits including ease of reuse. In this project there are two Docker containers; one for the web service and another that hosts the database. The database container is reused from a publicly available repository; the project only needs to declare it as a dependency, populate it with data, and it works out of the box.</p><p>At the top level of the project is the <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Dockerfile\\\\\\\">Dockerfile</a> that is used to build the image for the web server. Running an image with Docker (the program installed on a machine) creates a container. The <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose.yml\\\\\\\">docker-compose.yml</a> ties together the local web server and database images to form the complete stack; running &#x27;docker-compose up&#x27; will create both web server and database containers from the images and make them visible to each other in their own network. Once the local docker images are working they are pushed to an AWS repository. AWS requires the repository identifier reflected in the image name hence the separate <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose-AWS.yml\\\\\\\">docker-compose-AWS.yml</a> file. At a high level deploying to AWS involves creating a spot to host the container (EC2 instance), authenticating through a local AWS docker client, and running &#x27;docker-compose up&#x27; on docker-compose-AWS.yml. The Docker service running on the EC2 instance fetches the images from the AWS repository and brings them up to form the complete system.</p><h3>Testing</h3><p>A minimal amount of testing was required for this project. Testing needs increase with how mission critical a project is and how many people are working on the project, among other considerations. Even though this was a toy project with one developer it does have <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/src/js/government_audit/test/AuditSearchView.test.js\\\\\\\">some unit tests</a>, serving primarily as <a href=\\\\\\\"https://en.wikipedia.org/wiki/Regression_testing\\\\\\\">regression</a> tests. <a href=\\\\\\\"https://reactjs.org/\\\\\\\">React</a> and <a href=\\\\\\\"https://facebook.github.io/flux/\\\\\\\">Flux</a> were new to me so unit tests were created once I had things working so that I could refactor the code and quickly detect when I had made a code change that resulted in a change of code output. Of note in these tests is the use of <a href=\\\\\\\"https://facebook.github.io/jest/\\\\\\\">Jest</a> which allows creation of unit tests without the manual creation of assertion code. It has built-in support for snapshots (auto verifying everything) - perfect for regression testing to ensure that refactoring code doesn&#x27;t change the output of the code.</p><p>The development process for this project was most like <a href=\\\\\\\"https://en.wikipedia.org/wiki/Software_prototyping\\\\\\\">Software Prototyping</a>. Emphasis was placed on getting up and running as quickly as possible with quick turnaround enabling experimentation and progress along the learning curve. In a sense the REPL like nature of the environment offered the bulk of testing for this project. For systems that allow interaction, especially when users are allowed to change data on the server, more testing would be necessary.</p><h3>Not in this Example</h3><p>Being a toy project there are many parts not present here that would be encountered in a real world project. Here minimal attention is given to security as the content is static and not sensitive. HTTP encryption via SSL <a href=\\\\\\\"https://letsencrypt.org/\\\\\\\">is becoming an expected feature</a>. Once you have any user interaction with a site authentication becomes a requirement. Scalability isn&#x27;t an issue here but if a web site provides a non-trivial service to many users a distributed system architecture will likely be necessary.</p><p>For development workflow a team would have a system for deciding, recording, organizing, and tracking implementation progress for requirements. <a href=\\\\\\\"https://continuousdelivery.com/foundations/test-automation/\\\\\\\">Automated testing</a> with a deployment pipeline would exist. Error handling for both end users and developers (e.g. throw meaningful exceptions) would exist. Logging is typical as is emitting, collecting and analyzing performance metrics from the system for both engineering and business purposes. Each of these areas requires deep knowledge and experience to implement well; there is often a specialized role created for each. Participating in existing projects is the best way to learn about these subjects.</p>\\\", \\\"id\\\": \\\"da2a54e4-92c0-4563-ba59-ab25b922b24b\\\"}]\", \"subheading\": \"of this Toy Modern Web Application\", \"date\": \"2017-10-02\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 47, "fields": {"page": 6, "submitted_for_moderation": false, "created_at": "2018-05-05T12:33:50.772Z", "user": 1, "content_json": "{\"pk\": 6, \"path\": \"000100020003\", \"depth\": 3, \"numchild\": 0, \"title\": \"Full Stack Walkthrough\", \"draft_title\": \"Full Stack Walkthrough\", \"slug\": \"full-stack-walkthrough\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": true, \"url_path\": \"/randy-moore/full-stack-walkthrough/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-10-02T20:33:57.085Z\", \"last_published_at\": \"2018-05-05T09:49:06.599Z\", \"latest_revision_created_at\": \"2018-05-05T12:33:46.051Z\", \"live_revision\": 39, \"body\": \"[{\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Summary</h3><p>High level development walk through for a toy example of a modern full web application stack. Intended for those who want an idea of what basic knowledge and skills a &quot;Full Stack Engineer&quot; would have without being bogged down with the knowledge required for a real world project. Some scope of additional knowledge required for a real world project (team size &gt; 1, customer facing features, security, scale-ability, ...) is mentioned.</p><p>The complete source code of the example project is available at <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango\\\\\\\">https://github.com/RandyMoore/mySiteDjango</a></p><h3>Requirements</h3><p>Before starting out on any project it is important to clearly define what the goal is. In this case a personal website serving as a space to learn, experiment, and showcase web technologies. The site serves as an example for those wishing to learn the technology and so the project should be as simple and understandable as possible. The site also serves as a professional portfolio and so should be visually appealing to a non-technical audience.</p><h3>Architecture</h3><p>The requirements drive the architecture. The user roles consist of the content creator and the content viewer. The content is static and not sensitive which simplifies several aspects of the architecture. We don&#x27;t need to be concerned with growing storage demands or security since there is nothing sensitive to protect. Another requirement is deployment flexibility compatible with the major cloud service providers for low cost high performance hosting. Someone wishing to learn the technology should also be able to easily deploy the site locally so they can experiment with it. A two tier architecture (web server and database layers) encapsulated in a deployable container fits the bill for this use case.</p>\\\", \\\"id\\\": \\\"204d1b56-0988-44f0-bf0f-54d56d395837\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 5, \\\"id\\\": \\\"69d402a6-ff31-4afb-b921-7e34a5812afc\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Languages</h3><p>Choice of programming languages impacts the set of existing software you can choose to build your project from. Language and existing software choice (framework, libraries) are somewhat of a chicken and egg problem, start with which you feel strongest about. Web applications are broadly split into two parts, the front end and back end. The front end part of the project is run by the viewer&#x27;s web browser. The back end is run by the computing resources provided by the cloud service provider.</p><p>There&#x27;s less of a language choice for the front end which is driven by the need for compatibility to run on a variety of web browsers. For a web page HTML is required for the structured layout of content; CSS and Javascript are optional for separating out style and adding behavior respectively. There are many other front end languages you might encounter (e.g. CoffeScript, SASS) but these generally compile down to some older version of Javascript or CSS before it is served to the viewer&#x27;s browser.</p><p>For the back end, anything goes. The client&#x27;s web browser will contact the server using <a href=\\\\\\\"https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol\\\\\\\">HTTP</a>. The web browser has no knowledge of what is happening on the other side of this HTTP interface and so you are free to choose whatever language you wish. But you do need to comply with the HTTP specification, which is rather large. Implementing the behavior required by the HTTP specification using only primitive language features would be an impractical task. Enter frameworks.</p><h3>Frameworks</h3><p>HTTP is well defined in a specification and this has allowed others to write reusable software to take care of the HTTP details. Frameworks are language specific - they provide a bare-bones system in a particular language and allow you to add your own code to add custom behavior. You may already be familiar with the concept of a library. A framework is similar in that it is reusable code but is different because it drives control instead of responding to commands as a library would. In this case the framework code will be the first to handle the incoming request from the viewer&#x27;s web browser and then it will call your code to form a reply. <a href=\\\\\\\"https://www.djangoproject.com/\\\\\\\">Django</a> was chosen as the web framework for this project. Here&#x27;s a visualization of framework vs library:</p>\\\", \\\"id\\\": \\\"d9bc862c-31dc-4484-ad05-64250a252e01\\\"}, {\\\"type\\\": \\\"html\\\", \\\"value\\\": \\\"<iframe width=\\\\\\\"560\\\\\\\" height=\\\\\\\"315\\\\\\\" src=\\\\\\\"https://www.youtube.com/embed/lC0KapQU0aM\\\\\\\" frameborder=\\\\\\\"0\\\\\\\" allowfullscreen></iframe>\\\", \\\"id\\\": \\\"3e30cd2a-16c7-4ec2-9e0e-00fadbc12e4a\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<h3>Libraries</h3><p>The next step in the project is deciding what content should be added. A blog is a standard way for professional software developers to market their brand; the blog may be used to write about each project as it is added to the site. A naive approach is to write a single HTML file for each blog post. But writing HTML code can be cumbersome and having each post as a stand-alone HTML file might cause problems in the future if you want to change the style of all the posts; each file would have to changed.</p><p>Writing HTML by hand can be a good fit for simple data content that isn&#x27;t meant to be pretty. Creating something that has appealing style requires design skill. Fortunately libraries exist that encapsulate design. A quick search yields a <a href=\\\\\\\"https://startbootstrap.com/template-overviews/clean-blog/\\\\\\\">Bootstrap clean-blog design</a>. This takes care of the design aspect of the blog.</p><p>People have also written reusable software for the content aspect of things like blogs. This kind of software is known as a <a href=\\\\\\\"https://en.wikipedia.org/wiki/Content_management_system\\\\\\\">Content Management System</a> (CMS). Generally CMS is used as a means to separate content creation from the technical details of a site. Content specialists (e.g. journalists) may add content without becoming mired in technical details. CMS systems usually include WYSIWYG (What You See Is What You Get) editors; one way to avoid writing raw HTML. CMS systems are generally dependent on the web framework since there is a lot of glue code within the framework that has to interface with the various facets of the content (html generation, image resources, URL paths managed by the CMS, ...). Some searching yielded <a href=\\\\\\\"https://wagtail.io/\\\\\\\">Wagtail</a> as a CMS for use with Django. Perhaps a bit overkill for this site (having a team size of 1) but I was curious about CMS technology and wanted to experiment with a full fledged example of this technology.</p><h3>Development</h3><p>For a team size of one enough has been decided at this point to begin working with code. Generally a development environment is built based on choice of language since each language has it&#x27;s own paradigm of what a development environment is. For the Python back end we use <a href=\\\\\\\"https://pypi.python.org/pypi/pip\\\\\\\">pip</a> to fetch and install project requirements (existing reusable software components). A common concern with development environments is isolation between multiple projects on the same machine. We use <a href=\\\\\\\"https://virtualenv.pypa.io/en/stable/\\\\\\\">VirtualEnv</a> to keep all libraries and frameworks from interfering with each other and the global Python environment on our development machine.</p><p>Thought should be given regarding project structure. The chosen framework will often dictate this since it needs to know where to find your extensions but often your project will have files that exist outside of the framework. For example, in this project the declaration for which Python frameworks and libraries to install are kept in a file <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/config/requirements.txt\\\\\\\">requirements.txt</a> that can be fed to pip. This and other configuration files that affect the deployed code are kept in a folder name <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/config\\\\\\\">config</a> at the highest level of the project. The <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/my_site_django\\\\\\\">Django framework structure</a> begins as a sibling at same level since it is also a component of the deployed code.</p><p>A modern trend with languages and frameworks is to provide a REPL (Read Evaluate Print Loop). Languages (ie Python) often have a shell that evaluate commands this way, providing an interactive environment that encourages experimentation and exploration. Django and many other frameworks mimic a REPL with hot reloading. In Django, this is provided by the built-in development server (&#x27;python manage.py runserver&#x27;). Anytime you make a change to your code the changes are quickly and automatically ingested by the server; reflected the next time an HTTP request is processed. This feedback loop allows you to incrementally add and see small changes to the site, speeding development.</p><p>To edit code many people find an Integrated Development Environment (IDE) helpful. The IDE adds smarts and tools to your basic text editor. &quot;smarts&quot; meaning it is aware of the code in your project and can automatically recognize relationships between pieces of your code and also library or framework code. The IDE allows you to quickly navigate through execution paths to quickly understand how the pieces work together. I&#x27;m a fan of IDEA products, their community edition (CE) PyCharm is great for Python centric projects. The &#x27;python manage.py runserver&#x27; command may be run in a debugging session within the IDE, making it trivial to debug and explore the inner workings of the server code.</p><p>PyCharm CE doesn&#x27;t support front end languages well so I&#x27;ve been using (learning) the Atom editor for front end work. As mentioned before, front end programming languages generally get compiled down to an older version of HTML, CSS and Javascript to maximize browser compatibility. The step of converting your modern code into these older language versions is called &quot;bundling&quot;. For the front end development environment this project uses <a href=\\\\\\\"https://en.wikipedia.org/wiki/JavaScript\\\\\\\">Javascript</a> with <a href=\\\\\\\"https://nodejs.org/en/\\\\\\\">Node.js</a> as the engine to execute the Javascript (instead of a web browser). Similar to pip for Python exists <a href=\\\\\\\"https://www.npmjs.com/\\\\\\\">npm</a> (Node Package Manager) for Javascript (<a href=\\\\\\\"https://yarnpkg.com/en/\\\\\\\">yarn</a> may be used interchangeably with npm if npm fails to install something).</p><p>In this project the Javascript dependencies are recorded at the top level in a <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/package.json\\\\\\\">package.json</a> file. <a href=\\\\\\\"https://gruntjs.com/\\\\\\\">Grunt</a> is a Javascript program that serves as a way to glue together the steps of the bundling process. The tasks are declared in the project level <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Gruntfile.js\\\\\\\">Gruntfile.js</a>. The default task invoked with &#x27;grunt&#x27; may be run while editing code and serves as a REPL; when any front end related file changes it invokes the rebundling process and the changes will be reflected by the development server after a browser page refresh. For debugging the developer tools available in the browser are used (available by default in modern browsers).</p><h3>Project Management</h3><p>After writing some code and getting something basic to work you will worry that making further changes will break what you&#x27;ve already accomplished. Enter version control systems. Such systems allow you to track changes, moving between versions of your code at will. Version control becomes more of a concern as team size increases, but even with a team of size of one they are worth using. For example you can use branching to work on a major new feature while having the flexibility to quickly tweak the version of code that is currently deployed and being viewed by the public. This project uses <a href=\\\\\\\"https://git-scm.com/\\\\\\\">git</a>. GitHub (a service provided by a company) is used as a publicly accessible repository since it is well suited for open source projects.</p><p>Project management consists of far more than version control and becomes increasingly important as team size grows. At a high level a team decides on a <a href=\\\\\\\"https://en.wikipedia.org/wiki/Software_development_process\\\\\\\">development methodology</a> and refines a specific software development process to meet the project needs. The choice of methodology and process can influence what technologies to use and so is usually decided early in the project&#x27;s life cycle. The art of harnessing the work of multiple individual developers is it&#x27;s own subject and best learned from experience working with a team.</p><h3>Deployment</h3><p>Once you have something you&#x27;d like to show to the public the next step is to serve it to the world. Technically you could serve from the your development machine but keeping your development machine running 100% of the time to run the site would likely be a nuisance. You could also serve from a different machine that you own but this comes with a great deal of complexity and will probably expose your personal network to a high level of security risk. Additionally your home network is likely on the fringe of the world wide web which makes it slow to respond.</p><p>Fortunately companies offering cloud services exist which allow you to host arbitrary software and take care of the messy details of hosting (networking, security, computing resource allocation ...). The idea is that the hosting company figures out how to manage all of the hosting details for many users and then provides hosting as a service, adding value with economy of scale. A popular example is <a href=\\\\\\\"https://aws.amazon.com/\\\\\\\">Amazon Web Services</a> (AWS), but many vendors exist. Even with the hosting details taken care of there is still the issue of how to install your software on the cloud.</p><p>Accessing running software on the cloud isn&#x27;t as convenient or seamless as your local development environment. Changing anything almost always guarantees bugs. Wouldn&#x27;t it be awesome if you could draw a box around the code running on your development machine and simply drop it into the cloud? Then changing to the cloud environment would introduce minimal changes (seen from the perspective of your code) and result in fewer bugs. <a href=\\\\\\\"https://en.wikipedia.org/wiki/Abstraction_(software_engineering)\\\\\\\">Abstraction</a> comes to the rescue: why not abstract a machine, or more precisely the operating system? The technology that does this has existed for some time and is know as Virtualization. Virtual machines came about early on but are cumbersome due to their large size from replicating an entire OS. A much lighter weight alternative is <a href=\\\\\\\"https://www.docker.com/\\\\\\\">Docker</a>, which this project uses.</p><p>The Docker paradigm is to have a single app with all dependencies running in a single container and to tie multiple containers together to form a system. This is an example of the <a href=\\\\\\\"https://en.wikipedia.org/wiki/Single_responsibility_principle\\\\\\\">Single Responsibility Principle</a> which gives rise to many desirable design traits including ease of reuse. In this project there are two Docker containers; one for the web service and another that hosts the database. The database container is reused from a publicly available repository; the project only needs to declare it as a dependency, populate it with data, and it works out of the box.</p><p>At the top level of the project is the <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Dockerfile\\\\\\\">Dockerfile</a> that is used to build the image for the web server. Running an image with Docker (the program installed on a machine) creates a container. The <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose.yml\\\\\\\">docker-compose.yml</a> ties together the local web server and database images to form the complete stack; running &#x27;docker-compose up&#x27; will create both web server and database containers from the images and make them visible to each other in their own network. Once the local docker images are working they are pushed to an AWS repository. AWS requires the repository identifier reflected in the image name hence the separate <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose-AWS.yml\\\\\\\">docker-compose-AWS.yml</a> file. At a high level deploying to AWS involves creating a spot to host the container (EC2 instance), authenticating through a local AWS docker client, and running &#x27;docker-compose up&#x27; on docker-compose-AWS.yml. The Docker service running on the EC2 instance fetches the images from the AWS repository and brings them up to form the complete system.</p><h3>Testing</h3><p>A minimal amount of testing was required for this project. Testing needs increase with how mission critical a project is and how many people are working on the project, among other considerations. Even though this was a toy project with one developer it does have <a href=\\\\\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/src/js/government_audit/test/AuditSearchView.test.js\\\\\\\">some unit tests</a>, serving primarily as <a href=\\\\\\\"https://en.wikipedia.org/wiki/Regression_testing\\\\\\\">regression</a> tests. <a href=\\\\\\\"https://reactjs.org/\\\\\\\">React</a> and <a href=\\\\\\\"https://facebook.github.io/flux/\\\\\\\">Flux</a> were new to me so unit tests were created once I had things working so that I could refactor the code and quickly detect when I had made a code change that resulted in a change of code output. Of note in these tests is the use of <a href=\\\\\\\"https://facebook.github.io/jest/\\\\\\\">Jest</a> which allows creation of unit tests without the manual creation of assertion code. It has built-in support for snapshots (auto verifying everything) - perfect for regression testing to ensure that refactoring code doesn&#x27;t change the output of the code.</p><p>The development process for this project was most like <a href=\\\\\\\"https://en.wikipedia.org/wiki/Software_prototyping\\\\\\\">Software Prototyping</a>. Emphasis was placed on getting up and running as quickly as possible with quick turnaround enabling experimentation and progress along the learning curve. In a sense the REPL like nature of the environment offered the bulk of testing for this project. For systems that allow interaction, especially when users are allowed to change data on the server, more testing would be necessary.</p><h3>Not in this Example</h3><p>Being a toy project there are many parts not present here that would be encountered in a real world project. Here minimal attention is given to security as the content is static and not sensitive. HTTP encryption via SSL <a href=\\\\\\\"https://letsencrypt.org/\\\\\\\">is becoming an expected feature</a>. Once you have any user interaction with a site authentication becomes a requirement. Scalability isn&#x27;t an issue here but if a web site provides a non-trivial service to many users a distributed system architecture will likely be necessary.</p><p>For development workflow a team would have a system for deciding, recording, organizing, and tracking implementation progress for requirements. <a href=\\\\\\\"https://continuousdelivery.com/foundations/test-automation/\\\\\\\">Automated testing</a> with a deployment pipeline would exist. Error handling for both end users and developers (e.g. throw meaningful exceptions) would exist. Logging is typical as is emitting, collecting and analyzing performance metrics from the system for both engineering and business purposes. Each of these areas requires deep knowledge and experience to implement well; there is often a specialized role created for each. Participating in existing projects is the best way to learn about these subjects.</p>\\\", \\\"id\\\": \\\"da2a54e4-92c0-4563-ba59-ab25b922b24b\\\"}]\", \"subheading\": \"of this Toy Modern Web Application\", \"date\": \"2017-10-02\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 48, "fields": {"page": 5, "submitted_for_moderation": false, "created_at": "2018-05-05T12:34:11.034Z", "user": 1, "content_json": "{\"pk\": 5, \"path\": \"000100020002\", \"depth\": 3, \"numchild\": 0, \"title\": \"Asynchronous Programming\", \"draft_title\": \"Asynchronous Programming\", \"slug\": \"asynchronous-programming\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": false, \"url_path\": \"/randy-moore/asynchronous-programming/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-07-30T18:20:11.671Z\", \"last_published_at\": \"2018-05-05T09:47:45.427Z\", \"latest_revision_created_at\": \"2018-05-05T09:47:45.394Z\", \"live_revision\": 37, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"### Why Care?\\\\r\\\\nBrowsing through job postings you often notice a job requirement along the lines of: \\\\r\\\\n> Able to write highly efficient asynchronous code.  \\\\r\\\\n\\\\r\\\\nA fleeting thought:\\\\r\\\\n\\\\r\\\\n\\\\\\\"Ah this just means writing efficient code in terms of Big-O and then split and farm out work to it by using a solution (language feature, library, framework, ...) someone else has already come up with.\\\\\\\"\\\\r\\\\n\\\\r\\\\nYou skip over this requirement without much more thought.  A few days later you are in an interview and get asked the seminal \\\\\\\"What's the difference between a process and a thread?\\\\\\\".   Huh? Why is that a relevant question?  You already know processes and threads are low level OS stuff.  We don't program in assembly language any more - why be concerned with OS primitives?\\\\r\\\\n\\\\r\\\\nWriting efficient asynchronous code does have the prerequisite that your code is efficient and you are skilled at using pre-existing code.  But there is (at least) one additional skill required here: choosing and effectively implementing the asynchronous paradigm that fits the problem you are trying to solve.  When the interviewer asked you about process vs thread they were quickly checking the tip of the iceberg of what they hope you already know about asynchronous programming.  Hopefully you know this stuff, otherwise in the design interview you will be yielding blank stares instead of solutions.\\\\r\\\\n\\\\r\\\\n### Problem Space\\\\r\\\\n\\\\r\\\\nSo what is the high level problem to be solved with the skill of writing \\\\\\\"Efficient Asynchronous Code\\\\\\\"?  Just writing efficient code in terms of algorithmic complexity will optimize resource usage: CPU cycles and / or memory.  The addition of \\\\\\\"Asynchronous\\\\\\\" implies that different parts of your code may more freely execute when needed, less constrained by their location in the source code file.\\\\r\\\\n\\\\r\\\\nThe asynchronous aspect is expected to yield scalar benefit because all it considers is when code executes given the same input, using the same algorithms.  In contrast, the goal of algorithmic efficiency tuning is an asymptotic benefit given increasing input size.  So why bother with a mere scalar increase?  At some point processing takes a minimum amount of time.  When you have many tasks taking some non-trivial amount of time, the ability to order and potentially parallelize them can have a significant impact on overall (calendar) run time.\\\\r\\\\n\\\\r\\\\nThere are 2 categories of what causes calendar wait times:\\\\r\\\\n\\\\r\\\\n1.  CPU Bound - Time required to process data\\\\r\\\\n    *  Can alternatively be addressed by a more efficient algorithm\\\\r\\\\n2.  I/O Bound - Time required to move data\\\\r\\\\n    *  Can alternatively be addressed by caching\\\\r\\\\n\\\\r\\\\nSkillful application of asynchronous processing technique allows you to parallelize such time bound tasks to reduce overall calendar wait time as much as possible.  In the world of a web service platform scalar gains (2X, 3X etc) would be seen as phenomenal wins.  Even marginal gains (10%) are [valuable](https://blog.kissmetrics.com/loading-time/).\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n### Building Blocks\\\\r\\\\nThat interview question about process vs thread is checking the relationship between 2 nodes in a knowledge graph required to select the building blocks required for an overall solution.\\\\r\\\\n\\\\r\\\\nIn a nutshell:  The operating system creates and manages processes, each having it's own address space.  The operating system may also create additional threads within a process (this is generally done using the language of your choice interfacing with the OS).  Threads share the address space of the process, allocating off the same heap and also able to share references to arbitrary memory in the address space.  Some languages additionally support [coroutines](https://en.wikipedia.org/wiki/Coroutine) which have their own stack but which execute within a thread.  This leads to 3 choices each of where to allocate execution and data: Processes, Threads, or Coroutines.\\\", \\\"id\\\": \\\"eda162bf-482c-4e97-a2cd-d800a63710a8\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 3, \\\"id\\\": \\\"389ea35a-6ce0-4cb8-b25c-f59883aabe18\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Of course this diagram is a simplification only showing a small subset of possibilities for the instances of {Process, Thread, Coroutine} that could exist.  For example there could be many processes, processes can spawn child processes, and there can be many instances of coroutines.  Only stacks are shown but of course other local data such as the program counter for each executable instance would need to exist.  Additionally different computing environments have different setups; e.g. a particular programming language implementation may manage threads instead of the OS (known as \\\\\\\"green\\\\\\\" threads).  Always read the docs (and blogs, tutorials, etc) related to your particular environment.\\\\r\\\\n\\\\r\\\\nSo how do you decide where to allocate processing?  Here are some pros and cons to consider for each:\\\\r\\\\n\\\\r\\\\n1.  Multiple processes\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  True parallelism, will work for increasing overall performance of CPU bound tasks.\\\\r\\\\n    *  Separate address space for each process instance, no need to worry about resource contention within the program (external resource contention may still exist, e.g. multiple processes writing to a single file).\\\\r\\\\n    *  Easiest to reason about.  Only 1 entry point for execution.\\\\r\\\\n    *  For *nix platform easiest to reuse as a modular component with OS provided IPC mechanisms (eg combining small programs with pipe '|' on the command prompt).\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Heavy weight.  Each process has it's own copy of program data, etc.  In addition to memory processes generally use other limited OS resources more heavily than the other options.\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n2.  Multiple threads\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  Lighter weight than processes.  Threads share the data and heap portion of process memory.\\\\r\\\\n    *  May offer true parallelism for CPU bound tasks.  Check your computing environment implementation docs to be sure.\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Resource contention within the process address space may be complicated to deal with, especially if the threads can be run in parallel and / or are scheduled by the OS.\\\\r\\\\n    *  Difficult to reason about because a thread may be suspended at any time and have another thread change it's environment.\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n3.  Coroutines\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  Explicit control of when code is executing and when it is not executing, easier to reason about because you don't have to account for suspension in every possible location.\\\\r\\\\n    *  Language support reduces complexity of gathering results.  Generally the result replaces the task in your code.  i.e. a list of tasks becomes a list of results from those tasks; no need to implement code to collect results (ie callbacks, global data structures, etc).\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Does not offer local parallelism.  Only 1 coroutine may be running at a given time in the thread it shares with other coroutines.\\\\r\\\\n    *  Requires implementation discipline.  It is easy to introduce blocking code which freezes your thread and subsequently all other coroutines on the thread for that duration.  Blocking code could even be part of a 3rd party library - the libraries you use need to be compatible!\\\\r\\\\n\\\\r\\\\nOnce the nature of the tasks (IO or CPU bound?) are identified along with the usability features desired for the code some combination of these options should surface as a winner.\\\\r\\\\n\\\\r\\\\nHere are two code examples (at least Python 3.5 is required) to illustrate the execution difference between threads and coroutines.  Specifically these examples illustrate how coroutine execution is more explicit and deterministic as opposed to the willy-nilly execution that threads enjoy (but the rest of us don't). Thread and process execution is similar (both willy-nilly) so a process example is not included here.  Each example contains 3 kinds of tasks.  A CPU bound task, 4 IO bound tasks of varying lengths, and a polling task that wants to recur.\\\\r\\\\n\\\\r\\\\nFirst, multi threading:\\\", \\\"id\\\": \\\"ea192bd0-251a-4892-97c1-460731b8ade6\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"python\\\", \\\"code\\\": \\\"#! /usr/bin/env python3\\\\r\\\\n    from random import sample\\\\r\\\\n    from time import sleep, time\\\\r\\\\n    from threading import Thread\\\\r\\\\n    \\\\r\\\\n    def io_bound(s, label=\\\\\\\"IO bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        sleep(s)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    def cpu_bound(s, label=\\\\\\\"CPU bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        end = time() + s\\\\r\\\\n        while time() < end:\\\\r\\\\n            sample(range(1000), 1000)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    child_threads = [Thread(target=io_bound, args=(s, str(s) + \\\\\\\"s IO Bound\\\\\\\")) for s in range(4)]\\\\r\\\\n    child_threads.append(Thread(target=cpu_bound, args=(2, \\\\\\\"2s CPU Bound\\\\\\\")))\\\\r\\\\n    \\\\r\\\\n    print(\\\\\\\"Start child threads\\\\\\\")\\\\r\\\\n    for t in child_threads:\\\\r\\\\n        t.start()\\\\r\\\\n    \\\\r\\\\n    # We are in a main thread, so this is the equivalent of the poll task from async_example\\\\r\\\\n    while True:\\\\r\\\\n        print(\\\\\\\"Poll\\\\\\\")\\\\r\\\\n        live_threads = [t for t in child_threads if t.is_alive()]\\\\r\\\\n        if len(live_threads) > 0:\\\\r\\\\n            sleep(0.5)\\\\r\\\\n        else:\\\\r\\\\n            break\\\\r\\\\n\\\\r\\\\n    print(\\\\\\\"Main thread completed\\\\\\\")\\\"}, \\\"id\\\": \\\"898aa91e-0ae1-42da-830d-48410990f74c\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<p>Output:</p>\\\", \\\"id\\\": \\\"61b07605-ded1-4aab-930d-e93d0bf3d54f\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"bash\\\", \\\"code\\\": \\\"Start child threads\\\\r\\\\n    0s IO Bound started\\\\r\\\\n    0s IO Bound finished\\\\r\\\\n    1s IO Bound started\\\\r\\\\n    2s IO Bound started\\\\r\\\\n    3s IO Bound started\\\\r\\\\n    2s CPU Bound started\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    1s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    2s CPU Bound finished\\\\r\\\\n    2s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    3s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Main thread completed\\\"}, \\\"id\\\": \\\"b8020f4c-9935-4379-870c-4b7edae0ad73\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Here all threads are running independently spread throughout time.  The drawback is seen when considering the CPU bound task.  While it is running other threads are allowed to run.  It can be preempted mid operation and have things change under it's feet.  If this task needed to access resources also accessible to the other tasks then potentially complex synchronization implementation is required.  If for example it was placing partial results in a shared global location there would need to be additional code to synchronize access to that location across threads.  One positive aspect here is shown by the polling task: it is allowed to run without being starved by the CPU bound task.\\\\r\\\\n\\\\r\\\\nNow the asynchronous version using asyncio - Python's coroutine support.\\\", \\\"id\\\": \\\"47d3561d-0999-415a-8f9b-66f088a6e6e9\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"python\\\", \\\"code\\\": \\\"#! /usr/bin/env python3\\\\r\\\\n    import asyncio\\\\r\\\\n    from random import sample\\\\r\\\\n    from time import time\\\\r\\\\n    \\\\r\\\\n    async def io_bound(s, label=\\\\\\\"IO bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        await asyncio.sleep(s)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    # Example of a blocking task\\\\r\\\\n    async def cpu_bound(s, label=\\\\\\\"CPU bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        end = time() + s\\\\r\\\\n        while time() < end:\\\\r\\\\n            sample(range(1000), 1000)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    async def poll(s):\\\\r\\\\n        while True:\\\\r\\\\n            print(\\\\\\\"Poll\\\\\\\")\\\\r\\\\n            await asyncio.sleep(s)\\\\r\\\\n            active_tasks = [task for task in asyncio.Task.all_tasks() if not task.done()]\\\\r\\\\n            if  len(active_tasks) < 3: # this poll() and wait() for run_until_complete will always exist\\\\r\\\\n                return\\\\r\\\\n    \\\\r\\\\n    loop = asyncio.get_event_loop()\\\\r\\\\n    tasks = [asyncio.ensure_future(io_bound(s, str(s) + \\\\\\\"s IO Bound\\\\\\\")) for s in range(4)]\\\\r\\\\n    tasks.append(asyncio.ensure_future(cpu_bound(2, \\\\\\\"2s CPU Bound\\\\\\\")))\\\\r\\\\n    tasks.append(asyncio.ensure_future(poll(.5)))\\\\r\\\\n    \\\\r\\\\n    loop.run_until_complete(asyncio.wait(tasks))\\\\r\\\\n    \\\\r\\\\n    print(\\\\\\\"Event loop completed\\\\\\\")\\\"}, \\\"id\\\": \\\"b6600f3d-7ca9-4c56-bb90-8dd87d0aec8e\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<p>Output:</p>\\\", \\\"id\\\": \\\"4da01b5b-cfdf-4ebe-ae78-c678471fe147\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"bash\\\", \\\"code\\\": \\\"0s IO Bound started\\\\r\\\\n    1s IO Bound started\\\\r\\\\n    2s IO Bound started\\\\r\\\\n    3s IO Bound started\\\\r\\\\n    2s CPU Bound started\\\\r\\\\n    2s CPU Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    0s IO Bound finished\\\\r\\\\n    1s IO Bound finished\\\\r\\\\n    2s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    3s IO Bound finished\\\\r\\\\n    Event loop completed\\\"}, \\\"id\\\": \\\"d46a0c4f-babc-4785-96b6-6b42995cf6c3\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The difference here is that the CPU bound tasks runs to completion without being interrupted.  You may ask yourself - isn't this a bad thing?  Yes it is for CPU bound tasks and that's why you shouldn't use coroutines to parallelize those.  But it illustrates the point that your code will not be interrupted willy-nilly!  Since execution won't be preempted you don't have to deal with implementing a synchronization strategy for shared resources.  All the tasks that have to wait for _external_ IO bound processing to complete effectively have the work processed in parallel - slashing the overall wait time.  All of this is done very efficiently and without the headache of resource synchronization!  Waiting on IO bound external tasks is very common in web development, which is why this form of asynchronous processing has become all the rage.\\\", \\\"id\\\": \\\"354bf569-3602-486b-a45c-0772bd732962\\\"}]\", \"subheading\": \"(and why it's all the rage for web services)\", \"date\": \"2017-06-19\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.pagerevision", "pk": 49, "fields": {"page": 5, "submitted_for_moderation": false, "created_at": "2018-05-05T12:34:15.671Z", "user": 1, "content_json": "{\"pk\": 5, \"path\": \"000100020002\", \"depth\": 3, \"numchild\": 0, \"title\": \"Asynchronous Programming\", \"draft_title\": \"Asynchronous Programming\", \"slug\": \"asynchronous-programming\", \"content_type\": 33, \"live\": true, \"has_unpublished_changes\": true, \"url_path\": \"/randy-moore/asynchronous-programming/\", \"owner\": 1, \"seo_title\": \"\", \"show_in_menus\": false, \"search_description\": \"\", \"go_live_at\": null, \"expire_at\": null, \"expired\": false, \"locked\": false, \"first_published_at\": \"2017-07-30T18:20:11.671Z\", \"last_published_at\": \"2018-05-05T09:47:45.427Z\", \"latest_revision_created_at\": \"2018-05-05T12:34:11.034Z\", \"live_revision\": 37, \"body\": \"[{\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"### Why Care?\\\\r\\\\nBrowsing through job postings you often notice a job requirement along the lines of: \\\\r\\\\n> Able to write highly efficient asynchronous code.  \\\\r\\\\n\\\\r\\\\nA fleeting thought:\\\\r\\\\n\\\\r\\\\n\\\\\\\"Ah this just means writing efficient code in terms of Big-O and then split and farm out work to it by using a solution (language feature, library, framework, ...) someone else has already come up with.\\\\\\\"\\\\r\\\\n\\\\r\\\\nYou skip over this requirement without much more thought.  A few days later you are in an interview and get asked the seminal \\\\\\\"What's the difference between a process and a thread?\\\\\\\".   Huh? Why is that a relevant question?  You already know processes and threads are low level OS stuff.  We don't program in assembly language any more - why be concerned with OS primitives?\\\\r\\\\n\\\\r\\\\nWriting efficient asynchronous code does have the prerequisite that your code is efficient and you are skilled at using pre-existing code.  But there is (at least) one additional skill required here: choosing and effectively implementing the asynchronous paradigm that fits the problem you are trying to solve.  When the interviewer asked you about process vs thread they were quickly checking the tip of the iceberg of what they hope you already know about asynchronous programming.  Hopefully you know this stuff, otherwise in the design interview you will be yielding blank stares instead of solutions.\\\\r\\\\n\\\\r\\\\n### Problem Space\\\\r\\\\n\\\\r\\\\nSo what is the high level problem to be solved with the skill of writing \\\\\\\"Efficient Asynchronous Code\\\\\\\"?  Just writing efficient code in terms of algorithmic complexity will optimize resource usage: CPU cycles and / or memory.  The addition of \\\\\\\"Asynchronous\\\\\\\" implies that different parts of your code may more freely execute when needed, less constrained by their location in the source code file.\\\\r\\\\n\\\\r\\\\nThe asynchronous aspect is expected to yield scalar benefit because all it considers is when code executes given the same input, using the same algorithms.  In contrast, the goal of algorithmic efficiency tuning is an asymptotic benefit given increasing input size.  So why bother with a mere scalar increase?  At some point processing takes a minimum amount of time.  When you have many tasks taking some non-trivial amount of time, the ability to order and potentially parallelize them can have a significant impact on overall (calendar) run time.\\\\r\\\\n\\\\r\\\\nThere are 2 categories of what causes calendar wait times:\\\\r\\\\n\\\\r\\\\n1.  CPU Bound - Time required to process data\\\\r\\\\n    *  Can alternatively be addressed by a more efficient algorithm\\\\r\\\\n2.  I/O Bound - Time required to move data\\\\r\\\\n    *  Can alternatively be addressed by caching\\\\r\\\\n\\\\r\\\\nSkillful application of asynchronous processing technique allows you to parallelize such time bound tasks to reduce overall calendar wait time as much as possible.  In the world of a web service platform scalar gains (2X, 3X etc) would be seen as phenomenal wins.  Even marginal gains (10%) are [valuable](https://blog.kissmetrics.com/loading-time/).\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n### Building Blocks\\\\r\\\\nThat interview question about process vs thread is checking the relationship between 2 nodes in a knowledge graph required to select the building blocks required for an overall solution.\\\\r\\\\n\\\\r\\\\nIn a nutshell:  The operating system creates and manages processes, each having it's own address space.  The operating system may also create additional threads within a process (this is generally done using the language of your choice interfacing with the OS).  Threads share the address space of the process, allocating off the same heap and also able to share references to arbitrary memory in the address space.  Some languages additionally support [coroutines](https://en.wikipedia.org/wiki/Coroutine) which have their own stack but which execute within a thread.  This leads to 3 choices each of where to allocate execution and data: Processes, Threads, or Coroutines.\\\", \\\"id\\\": \\\"eda162bf-482c-4e97-a2cd-d800a63710a8\\\"}, {\\\"type\\\": \\\"image\\\", \\\"value\\\": 3, \\\"id\\\": \\\"389ea35a-6ce0-4cb8-b25c-f59883aabe18\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Of course this diagram is a simplification only showing a small subset of possibilities for the instances of {Process, Thread, Coroutine} that could exist.  For example there could be many processes, processes can spawn child processes, and there can be many instances of coroutines.  Only stacks are shown but of course other local data such as the program counter for each executable instance would need to exist.  Additionally different computing environments have different setups; e.g. a particular programming language implementation may manage threads instead of the OS (known as \\\\\\\"green\\\\\\\" threads).  Always read the docs (and blogs, tutorials, etc) related to your particular environment.\\\\r\\\\n\\\\r\\\\nSo how do you decide where to allocate processing?  Here are some pros and cons to consider for each:\\\\r\\\\n\\\\r\\\\n1.  Multiple processes\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  True parallelism, will work for increasing overall performance of CPU bound tasks.\\\\r\\\\n    *  Separate address space for each process instance, no need to worry about resource contention within the program (external resource contention may still exist, e.g. multiple processes writing to a single file).\\\\r\\\\n    *  Easiest to reason about.  Only 1 entry point for execution.\\\\r\\\\n    *  For *nix platform easiest to reuse as a modular component with OS provided IPC mechanisms (eg combining small programs with pipe '|' on the command prompt).\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Heavy weight.  Each process has it's own copy of program data, etc.  In addition to memory processes generally use other limited OS resources more heavily than the other options.\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n2.  Multiple threads\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  Lighter weight than processes.  Threads share the data and heap portion of process memory.\\\\r\\\\n    *  May offer true parallelism for CPU bound tasks.  Check your computing environment implementation docs to be sure.\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Resource contention within the process address space may be complicated to deal with, especially if the threads can be run in parallel and / or are scheduled by the OS.\\\\r\\\\n    *  Difficult to reason about because a thread may be suspended at any time and have another thread change it's environment.\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n3.  Coroutines\\\\r\\\\n\\\\r\\\\n    Pros\\\\r\\\\n\\\\r\\\\n    *  Explicit control of when code is executing and when it is not executing, easier to reason about because you don't have to account for suspension in every possible location.\\\\r\\\\n    *  Language support reduces complexity of gathering results.  Generally the result replaces the task in your code.  i.e. a list of tasks becomes a list of results from those tasks; no need to implement code to collect results (ie callbacks, global data structures, etc).\\\\r\\\\n\\\\r\\\\n    Cons\\\\r\\\\n\\\\r\\\\n    *  Does not offer local parallelism.  Only 1 coroutine may be running at a given time in the thread it shares with other coroutines.\\\\r\\\\n    *  Requires implementation discipline.  It is easy to introduce blocking code which freezes your thread and subsequently all other coroutines on the thread for that duration.  Blocking code could even be part of a 3rd party library - the libraries you use need to be compatible!\\\\r\\\\n\\\\r\\\\nOnce the nature of the tasks (IO or CPU bound?) are identified along with the usability features desired for the code some combination of these options should surface as a winner.\\\\r\\\\n\\\\r\\\\nHere are two code examples (at least Python 3.5 is required) to illustrate the execution difference between threads and coroutines.  Specifically these examples illustrate how coroutine execution is more explicit and deterministic as opposed to the willy-nilly execution that threads enjoy (but the rest of us don't). Thread and process execution is similar (both willy-nilly) so a process example is not included here.  Each example contains 3 kinds of tasks.  A CPU bound task, 4 IO bound tasks of varying lengths, and a polling task that wants to recur.\\\\r\\\\n\\\\r\\\\nFirst, multi threading:\\\", \\\"id\\\": \\\"ea192bd0-251a-4892-97c1-460731b8ade6\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"python\\\", \\\"code\\\": \\\"#! /usr/bin/env python3\\\\r\\\\n    from random import sample\\\\r\\\\n    from time import sleep, time\\\\r\\\\n    from threading import Thread\\\\r\\\\n    \\\\r\\\\n    def io_bound(s, label=\\\\\\\"IO bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        sleep(s)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    def cpu_bound(s, label=\\\\\\\"CPU bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        end = time() + s\\\\r\\\\n        while time() < end:\\\\r\\\\n            sample(range(1000), 1000)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    child_threads = [Thread(target=io_bound, args=(s, str(s) + \\\\\\\"s IO Bound\\\\\\\")) for s in range(4)]\\\\r\\\\n    child_threads.append(Thread(target=cpu_bound, args=(2, \\\\\\\"2s CPU Bound\\\\\\\")))\\\\r\\\\n    \\\\r\\\\n    print(\\\\\\\"Start child threads\\\\\\\")\\\\r\\\\n    for t in child_threads:\\\\r\\\\n        t.start()\\\\r\\\\n    \\\\r\\\\n    # We are in a main thread, so this is the equivalent of the poll task from async_example\\\\r\\\\n    while True:\\\\r\\\\n        print(\\\\\\\"Poll\\\\\\\")\\\\r\\\\n        live_threads = [t for t in child_threads if t.is_alive()]\\\\r\\\\n        if len(live_threads) > 0:\\\\r\\\\n            sleep(0.5)\\\\r\\\\n        else:\\\\r\\\\n            break\\\\r\\\\n\\\\r\\\\n    print(\\\\\\\"Main thread completed\\\\\\\")\\\"}, \\\"id\\\": \\\"898aa91e-0ae1-42da-830d-48410990f74c\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<p>Output:</p>\\\", \\\"id\\\": \\\"61b07605-ded1-4aab-930d-e93d0bf3d54f\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"bash\\\", \\\"code\\\": \\\"Start child threads\\\\r\\\\n    0s IO Bound started\\\\r\\\\n    0s IO Bound finished\\\\r\\\\n    1s IO Bound started\\\\r\\\\n    2s IO Bound started\\\\r\\\\n    3s IO Bound started\\\\r\\\\n    2s CPU Bound started\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    1s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    2s CPU Bound finished\\\\r\\\\n    2s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Poll\\\\r\\\\n    3s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    Main thread completed\\\"}, \\\"id\\\": \\\"b8020f4c-9935-4379-870c-4b7edae0ad73\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"Here all threads are running independently spread throughout time.  The drawback is seen when considering the CPU bound task.  While it is running other threads are allowed to run.  It can be preempted mid operation and have things change under it's feet.  If this task needed to access resources also accessible to the other tasks then potentially complex synchronization implementation is required.  If for example it was placing partial results in a shared global location there would need to be additional code to synchronize access to that location across threads.  One positive aspect here is shown by the polling task: it is allowed to run without being starved by the CPU bound task.\\\\r\\\\n\\\\r\\\\nNow the asynchronous version using asyncio - Python's coroutine support.\\\", \\\"id\\\": \\\"47d3561d-0999-415a-8f9b-66f088a6e6e9\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"python\\\", \\\"code\\\": \\\"#! /usr/bin/env python3\\\\r\\\\n    import asyncio\\\\r\\\\n    from random import sample\\\\r\\\\n    from time import time\\\\r\\\\n    \\\\r\\\\n    async def io_bound(s, label=\\\\\\\"IO bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        await asyncio.sleep(s)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    # Example of a blocking task\\\\r\\\\n    async def cpu_bound(s, label=\\\\\\\"CPU bound\\\\\\\"):\\\\r\\\\n        print(label + \\\\\\\" started\\\\\\\")\\\\r\\\\n        end = time() + s\\\\r\\\\n        while time() < end:\\\\r\\\\n            sample(range(1000), 1000)\\\\r\\\\n        print(label + \\\\\\\" finished\\\\\\\")\\\\r\\\\n    \\\\r\\\\n    async def poll(s):\\\\r\\\\n        while True:\\\\r\\\\n            print(\\\\\\\"Poll\\\\\\\")\\\\r\\\\n            await asyncio.sleep(s)\\\\r\\\\n            active_tasks = [task for task in asyncio.Task.all_tasks() if not task.done()]\\\\r\\\\n            if  len(active_tasks) < 3: # this poll() and wait() for run_until_complete will always exist\\\\r\\\\n                return\\\\r\\\\n    \\\\r\\\\n    loop = asyncio.get_event_loop()\\\\r\\\\n    tasks = [asyncio.ensure_future(io_bound(s, str(s) + \\\\\\\"s IO Bound\\\\\\\")) for s in range(4)]\\\\r\\\\n    tasks.append(asyncio.ensure_future(cpu_bound(2, \\\\\\\"2s CPU Bound\\\\\\\")))\\\\r\\\\n    tasks.append(asyncio.ensure_future(poll(.5)))\\\\r\\\\n    \\\\r\\\\n    loop.run_until_complete(asyncio.wait(tasks))\\\\r\\\\n    \\\\r\\\\n    print(\\\\\\\"Event loop completed\\\\\\\")\\\"}, \\\"id\\\": \\\"b6600f3d-7ca9-4c56-bb90-8dd87d0aec8e\\\"}, {\\\"type\\\": \\\"paragraph\\\", \\\"value\\\": \\\"<p>Output:</p>\\\", \\\"id\\\": \\\"4da01b5b-cfdf-4ebe-ae78-c678471fe147\\\"}, {\\\"type\\\": \\\"code\\\", \\\"value\\\": {\\\"language\\\": \\\"bash\\\", \\\"code\\\": \\\"0s IO Bound started\\\\r\\\\n    1s IO Bound started\\\\r\\\\n    2s IO Bound started\\\\r\\\\n    3s IO Bound started\\\\r\\\\n    2s CPU Bound started\\\\r\\\\n    2s CPU Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    0s IO Bound finished\\\\r\\\\n    1s IO Bound finished\\\\r\\\\n    2s IO Bound finished\\\\r\\\\n    Poll\\\\r\\\\n    3s IO Bound finished\\\\r\\\\n    Event loop completed\\\"}, \\\"id\\\": \\\"d46a0c4f-babc-4785-96b6-6b42995cf6c3\\\"}, {\\\"type\\\": \\\"markdown\\\", \\\"value\\\": \\\"The difference here is that the CPU bound tasks runs to completion without being interrupted.  You may ask yourself - isn't this a bad thing?  Yes it is for CPU bound tasks and that's why you shouldn't use coroutines to parallelize those.  But it illustrates the point that your code will not be interrupted willy-nilly!  Since execution won't be preempted you don't have to deal with implementing a synchronization strategy for shared resources.  All the tasks that have to wait for _external_ IO bound processing to complete effectively have the work processed in parallel - slashing the overall wait time.  All of this is done very efficiently and without the headache of resource synchronization!  Waiting on IO bound external tasks is very common in web development, which is why this form of asynchronous processing has become all the rage.\\\", \\\"id\\\": \\\"354bf569-3602-486b-a45c-0772bd732962\\\"}]\", \"subheading\": \"(and why it's all the rage for web services)\", \"date\": \"2017-06-19\"}", "approved_go_live_at": null}}, {"model": "wagtailcore.grouppagepermission", "pk": 1, "fields": {"group": 1, "page": 1, "permission_type": "add"}}, {"model": "wagtailcore.grouppagepermission", "pk": 2, "fields": {"group": 1, "page": 1, "permission_type": "edit"}}, {"model": "wagtailcore.grouppagepermission", "pk": 3, "fields": {"group": 1, "page": 1, "permission_type": "publish"}}, {"model": "wagtailcore.grouppagepermission", "pk": 4, "fields": {"group": 2, "page": 1, "permission_type": "add"}}, {"model": "wagtailcore.grouppagepermission", "pk": 5, "fields": {"group": 2, "page": 1, "permission_type": "edit"}}, {"model": "wagtailcore.grouppagepermission", "pk": 6, "fields": {"group": 1, "page": 1, "permission_type": "lock"}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 1, "fields": {"group": 1, "collection": 1, "permission": 2}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 2, "fields": {"group": 2, "collection": 1, "permission": 2}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 3, "fields": {"group": 1, "collection": 1, "permission": 3}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 4, "fields": {"group": 2, "collection": 1, "permission": 3}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 5, "fields": {"group": 1, "collection": 1, "permission": 5}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 6, "fields": {"group": 2, "collection": 1, "permission": 5}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 7, "fields": {"group": 1, "collection": 1, "permission": 6}}, {"model": "wagtailcore.groupcollectionpermission", "pk": 8, "fields": {"group": 2, "collection": 1, "permission": 6}}, {"model": "weblog.blogindexpage", "pk": 3, "fields": {"subheading": "Software Engineer Passionate about Quality and Impact"}}, {"model": "weblog.weblogpage", "pk": 4, "fields": {"body": "[{\"type\": \"markdown\", \"value\": \"### Why build a personal site?\\r\\nMany folks have an online identity through sources such as Facebook or [LinkedIn](https://www.linkedin.com/in/randy-moore-b552b014/).  Profiles such as these have the benefit of a network engine but limit creative expression.  \\r\\n\\r\\nLinkedIn serves as an excellent host to one's professional profile.  The features are tailored to a professional perspective and include an automatic and personalized connection to LinkedIn's ecosystem.  Shortly after creating my LinkedIn profile I was contacted by a LinkedIn recruiter and wound up spending 5 most excellent career years there.  Similarly Facebook automatically provides opportunity to connect to people in your social circles.\\r\\n\\r\\nPreexisting sites with a particular focus come with the downside of limiting creative expression.  LinkedIn and Facebook will hold your hand and guide you through creating a profile.  This scripted creation is easy and potentially rewarding but limits you to the purpose and environment of the platform.  Creating a personal website from lower level building blocks offers greater freedom of expression but in the same vein is a daunting endeavor.  Beyond the technical challenges you need to think about content and purpose.  Will it serve as a herald of your professional brand?  You run into tough questions: is it wise to include your personal views as a part of your professional brand?\\r\\n\\r\\nFrom experience I have faith in open source to solve the technical challenges.  A plethora of proven building blocks exist.  The greater challenge is content and purpose.  This website will serve as an experiment. From life experience I've noticed many sources recommend to be either hot or cold, not [lukewarm](http://biblehub.com/revelation/3-16.htm)<sup>1</sup>. \\r\\n\\r\\nHypothesis:\\r\\n> The world is a large place.  Exercising creative freedom and following passion will lead to blow back but also growth and purpose.\\r\\n\\r\\nThis site will serve as a herald of both my professional *and* personal brand.\\r\\n\\r\\n### Creation of the site\\r\\nThe creation of this site is a story of curiosity, conjecture and a meandering path exploring what open source has to offer.  This post covers up to the initial step of publishing the source code.\\r\\n\\r\\nTime at LinkedIn was spent engaged with a Java (EE) centric environment with tooling written mostly in Python.  In response to some time sensitive needs I had decided to use Python scripting and was impressed by how quickly a solution came together for a seemingly overwhelming task.  To boot, Python has been a rising star in the industry.  Python was chosen be the central language of the site.\\r\\n\\r\\nTechnologies that encapsulate process have recently spoken to my soul.  Having spent hundreds of hours in my career dealing with development environment issues the ability to encapsulate a the creation of a development environment is particularly appealing.  Dabbling across open source projects you will see many share virtual machine environments (eg using [Vagrant](https://www.vagrantup.com/)) but this approach is still fairly heavyweight (long download times, complexity).  [Docker](https://www.docker.com/) captured my attention as a lightweight alternative to full virtual machines.  Turns out Docker natively supports orchestrating multiple Docker containers into a complete system. Sold!\\r\\n\\r\\nThe goal at this point was to create the website within a Docker container so it could be worked on anywhere.  Some poking around led to [Flask](http://flask.pocoo.org/) which promises to be minimal (meaning to me ease of use) but extensible. [Nginx](https://nginx.org/) soon came into focus since the built in Flask webserver does not support serving [more than 1 request at a time](http://flask.pocoo.org/docs/0.12/deploying/).  Nginx is advertised as a lightwight server with a built in [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy) - basically a cache to serve up commonly requested pages that have already been created by the (heavyweight) app framework (Flask).\\r\\n\\r\\nInitial Thoughts:\", \"id\": \"000b5b3f-71df-4853-be49-127ed59479fd\"}, {\"type\": \"image\", \"value\": 2, \"id\": \"7940ebfc-de7d-432a-b3a6-2ebe999a43f3\"}, {\"type\": \"markdown\", \"value\": \"The efficiency promised by Nginx led to dreams of hosting the site on a [Raspberry Pi](https://www.raspberrypi.org/).  Not everything is available for ARM machines, but [Docker seems to be available on the Pi](https://www.raspberrypi.org/blog/docker-comes-to-raspberry-pi/).  The magic sauce to have Nginx serve with Flask is [WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface), an API specification. [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) looked like the best choice to most efficiently provide that glue since it supports native communication over sockets (instead of via HTTP or some other relatively heavy layer).\\r\\n\\r\\nThe uWSGI implementation is not just code included in a final executable, it works as a (daemon) service.  This threw a monkey wrench in the plan; a typical Docker container generally runs a single process which is tied to the lifecycle of the container.  First attempt began with the [Nginx Docker base image](https://hub.docker.com/_/nginx/) but led to creating and modifying init scripts to bring up uWSGI in addition to Nginx.  After much effort uWSGI would still not be running upon container deployment, time for a different approach.\\r\\n\\r\\nSome searching led to a question that bore fruit: how does one correctly manage multiple services within a Docker container?  A solution is provided by [phusion.nl](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/).  After reading the [story](https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/) behind the base image there was a wave of relief.  Had my init script hacks worked strange issues would probably have appeared later on.  Thankfully the Nginx Docker base image rejected my hacks by following the design principle\\r\\n> [Make it easy to use correctly and hard to use incorrectly](http://principles-wiki.net/principles:easy_to_use_and_hard_to_misuse)\\r\\n\\r\\n### Sharing the site source code\\r\\nWhat drives people often isn't the what but the why.  While searching for the meaning of life it's become clear that the path to happiness involves sharing.  Open source has been a central part of my life since installing [Free BSD](https://www.freebsd.org/) on an old computer in the late 90's.  Shortly after that and up to the present [Gentoo Linux](https://gentoo.org/) has served my personal computing needs.  Having received so much from the open source community there's an itch to give back.  My hope is that others may follow the story of this site via these blog posts (more coming soon) and gain a bit of perspective on the open source landscape and the opportunities available to them.\\r\\n\\r\\n[Current source code for this site](https://github.com/RandyMoore/website)\\r\\n\\r\\n[The commit for a minimal Nginx - uWSGI - Flask Docker container](https://github.com/RandyMoore/website/commit/9690a8908408cc87c52aaed007decd276c7e01e6) (See this [README](https://github.com/RandyMoore/website/blob/f3d5d8f4a006206d8359218ba8544cdc5f8c1224/README.md) for directions)\\r\\n\\r\\n###<em>Footnotes</em>\\r\\n1.  I am spiritual but do not subscribe to a particular religion.\", \"id\": \"2b881364-9599-4b21-8d18-0ecf4b0bb04b\"}]", "subheading": "How easy is it to create a personal website using open source?  Why build a personal website?", "date": "2017-06-05"}}, {"model": "weblog.weblogpage", "pk": 5, "fields": {"body": "[{\"type\": \"markdown\", \"value\": \"### Why Care?\\r\\nBrowsing through job postings you often notice a job requirement along the lines of: \\r\\n> Able to write highly efficient asynchronous code.  \\r\\n\\r\\nA fleeting thought:\\r\\n\\r\\n\\\"Ah this just means writing efficient code in terms of Big-O and then split and farm out work to it by using a solution (language feature, library, framework, ...) someone else has already come up with.\\\"\\r\\n\\r\\nYou skip over this requirement without much more thought.  A few days later you are in an interview and get asked the seminal \\\"What's the difference between a process and a thread?\\\".   Huh? Why is that a relevant question?  You already know processes and threads are low level OS stuff.  We don't program in assembly language any more - why be concerned with OS primitives?\\r\\n\\r\\nWriting efficient asynchronous code does have the prerequisite that your code is efficient and you are skilled at using pre-existing code.  But there is (at least) one additional skill required here: choosing and effectively implementing the asynchronous paradigm that fits the problem you are trying to solve.  When the interviewer asked you about process vs thread they were quickly checking the tip of the iceberg of what they hope you already know about asynchronous programming.  Hopefully you know this stuff, otherwise in the design interview you will be yielding blank stares instead of solutions.\\r\\n\\r\\n### Problem Space\\r\\n\\r\\nSo what is the high level problem to be solved with the skill of writing \\\"Efficient Asynchronous Code\\\"?  Just writing efficient code in terms of algorithmic complexity will optimize resource usage: CPU cycles and / or memory.  The addition of \\\"Asynchronous\\\" implies that different parts of your code may more freely execute when needed, less constrained by their location in the source code file.\\r\\n\\r\\nThe asynchronous aspect is expected to yield scalar benefit because all it considers is when code executes given the same input, using the same algorithms.  In contrast, the goal of algorithmic efficiency tuning is an asymptotic benefit given increasing input size.  So why bother with a mere scalar increase?  At some point processing takes a minimum amount of time.  When you have many tasks taking some non-trivial amount of time, the ability to order and potentially parallelize them can have a significant impact on overall (calendar) run time.\\r\\n\\r\\nThere are 2 categories of what causes calendar wait times:\\r\\n\\r\\n1.  CPU Bound - Time required to process data\\r\\n    *  Can alternatively be addressed by a more efficient algorithm\\r\\n2.  I/O Bound - Time required to move data\\r\\n    *  Can alternatively be addressed by caching\\r\\n\\r\\nSkillful application of asynchronous processing technique allows you to parallelize such time bound tasks to reduce overall calendar wait time as much as possible.  In the world of a web service platform scalar gains (2X, 3X etc) would be seen as phenomenal wins.  Even marginal gains (10%) are [valuable](https://blog.kissmetrics.com/loading-time/).\\r\\n\\r\\n\\r\\n### Building Blocks\\r\\nThat interview question about process vs thread is checking the relationship between 2 nodes in a knowledge graph required to select the building blocks required for an overall solution.\\r\\n\\r\\nIn a nutshell:  The operating system creates and manages processes, each having it's own address space.  The operating system may also create additional threads within a process (this is generally done using the language of your choice interfacing with the OS).  Threads share the address space of the process, allocating off the same heap and also able to share references to arbitrary memory in the address space.  Some languages additionally support [coroutines](https://en.wikipedia.org/wiki/Coroutine) which have their own stack but which execute within a thread.  This leads to 3 choices each of where to allocate execution and data: Processes, Threads, or Coroutines.\", \"id\": \"eda162bf-482c-4e97-a2cd-d800a63710a8\"}, {\"type\": \"image\", \"value\": 3, \"id\": \"389ea35a-6ce0-4cb8-b25c-f59883aabe18\"}, {\"type\": \"markdown\", \"value\": \"Of course this diagram is a simplification only showing a small subset of possibilities for the instances of {Process, Thread, Coroutine} that could exist.  For example there could be many processes, processes can spawn child processes, and there can be many instances of coroutines.  Only stacks are shown but of course other local data such as the program counter for each executable instance would need to exist.  Additionally different computing environments have different setups; e.g. a particular programming language implementation may manage threads instead of the OS (known as \\\"green\\\" threads).  Always read the docs (and blogs, tutorials, etc) related to your particular environment.\\r\\n\\r\\nSo how do you decide where to allocate processing?  Here are some pros and cons to consider for each:\\r\\n\\r\\n1.  Multiple processes\\r\\n\\r\\n    Pros\\r\\n\\r\\n    *  True parallelism, will work for increasing overall performance of CPU bound tasks.\\r\\n    *  Separate address space for each process instance, no need to worry about resource contention within the program (external resource contention may still exist, e.g. multiple processes writing to a single file).\\r\\n    *  Easiest to reason about.  Only 1 entry point for execution.\\r\\n    *  For *nix platform easiest to reuse as a modular component with OS provided IPC mechanisms (eg combining small programs with pipe '|' on the command prompt).\\r\\n\\r\\n    Cons\\r\\n\\r\\n    *  Heavy weight.  Each process has it's own copy of program data, etc.  In addition to memory processes generally use other limited OS resources more heavily than the other options.\\r\\n\\r\\n\\r\\n2.  Multiple threads\\r\\n\\r\\n    Pros\\r\\n\\r\\n    *  Lighter weight than processes.  Threads share the data and heap portion of process memory.\\r\\n    *  May offer true parallelism for CPU bound tasks.  Check your computing environment implementation docs to be sure.\\r\\n\\r\\n    Cons\\r\\n\\r\\n    *  Resource contention within the process address space may be complicated to deal with, especially if the threads can be run in parallel and / or are scheduled by the OS.\\r\\n    *  Difficult to reason about because a thread may be suspended at any time and have another thread change it's environment.\\r\\n\\r\\n\\r\\n3.  Coroutines\\r\\n\\r\\n    Pros\\r\\n\\r\\n    *  Explicit control of when code is executing and when it is not executing, easier to reason about because you don't have to account for suspension in every possible location.\\r\\n    *  Language support reduces complexity of gathering results.  Generally the result replaces the task in your code.  i.e. a list of tasks becomes a list of results from those tasks; no need to implement code to collect results (ie callbacks, global data structures, etc).\\r\\n\\r\\n    Cons\\r\\n\\r\\n    *  Does not offer local parallelism.  Only 1 coroutine may be running at a given time in the thread it shares with other coroutines.\\r\\n    *  Requires implementation discipline.  It is easy to introduce blocking code which freezes your thread and subsequently all other coroutines on the thread for that duration.  Blocking code could even be part of a 3rd party library - the libraries you use need to be compatible!\\r\\n\\r\\nOnce the nature of the tasks (IO or CPU bound?) are identified along with the usability features desired for the code some combination of these options should surface as a winner.\\r\\n\\r\\nHere are two code examples (at least Python 3.5 is required) to illustrate the execution difference between threads and coroutines.  Specifically these examples illustrate how coroutine execution is more explicit and deterministic as opposed to the willy-nilly execution that threads enjoy (but the rest of us don't). Thread and process execution is similar (both willy-nilly) so a process example is not included here.  Each example contains 3 kinds of tasks.  A CPU bound task, 4 IO bound tasks of varying lengths, and a polling task that wants to recur.\\r\\n\\r\\nFirst, multi threading:\", \"id\": \"ea192bd0-251a-4892-97c1-460731b8ade6\"}, {\"type\": \"code\", \"value\": {\"language\": \"python\", \"code\": \"#! /usr/bin/env python3\\r\\n    from random import sample\\r\\n    from time import sleep, time\\r\\n    from threading import Thread\\r\\n    \\r\\n    def io_bound(s, label=\\\"IO bound\\\"):\\r\\n        print(label + \\\" started\\\")\\r\\n        sleep(s)\\r\\n        print(label + \\\" finished\\\")\\r\\n    \\r\\n    def cpu_bound(s, label=\\\"CPU bound\\\"):\\r\\n        print(label + \\\" started\\\")\\r\\n        end = time() + s\\r\\n        while time() < end:\\r\\n            sample(range(1000), 1000)\\r\\n        print(label + \\\" finished\\\")\\r\\n    \\r\\n    child_threads = [Thread(target=io_bound, args=(s, str(s) + \\\"s IO Bound\\\")) for s in range(4)]\\r\\n    child_threads.append(Thread(target=cpu_bound, args=(2, \\\"2s CPU Bound\\\")))\\r\\n    \\r\\n    print(\\\"Start child threads\\\")\\r\\n    for t in child_threads:\\r\\n        t.start()\\r\\n    \\r\\n    # We are in a main thread, so this is the equivalent of the poll task from async_example\\r\\n    while True:\\r\\n        print(\\\"Poll\\\")\\r\\n        live_threads = [t for t in child_threads if t.is_alive()]\\r\\n        if len(live_threads) > 0:\\r\\n            sleep(0.5)\\r\\n        else:\\r\\n            break\\r\\n\\r\\n    print(\\\"Main thread completed\\\")\"}, \"id\": \"898aa91e-0ae1-42da-830d-48410990f74c\"}, {\"type\": \"paragraph\", \"value\": \"<p>Output:</p>\", \"id\": \"61b07605-ded1-4aab-930d-e93d0bf3d54f\"}, {\"type\": \"code\", \"value\": {\"language\": \"bash\", \"code\": \"Start child threads\\r\\n    0s IO Bound started\\r\\n    0s IO Bound finished\\r\\n    1s IO Bound started\\r\\n    2s IO Bound started\\r\\n    3s IO Bound started\\r\\n    2s CPU Bound started\\r\\n    Poll\\r\\n    Poll\\r\\n    1s IO Bound finished\\r\\n    Poll\\r\\n    Poll\\r\\n    2s CPU Bound finished\\r\\n    2s IO Bound finished\\r\\n    Poll\\r\\n    Poll\\r\\n    3s IO Bound finished\\r\\n    Poll\\r\\n    Main thread completed\"}, \"id\": \"b8020f4c-9935-4379-870c-4b7edae0ad73\"}, {\"type\": \"markdown\", \"value\": \"Here all threads are running independently spread throughout time.  The drawback is seen when considering the CPU bound task.  While it is running other threads are allowed to run.  It can be preempted mid operation and have things change under it's feet.  If this task needed to access resources also accessible to the other tasks then potentially complex synchronization implementation is required.  If for example it was placing partial results in a shared global location there would need to be additional code to synchronize access to that location across threads.  One positive aspect here is shown by the polling task: it is allowed to run without being starved by the CPU bound task.\\r\\n\\r\\nNow the asynchronous version using asyncio - Python's coroutine support.\", \"id\": \"47d3561d-0999-415a-8f9b-66f088a6e6e9\"}, {\"type\": \"code\", \"value\": {\"language\": \"python\", \"code\": \"#! /usr/bin/env python3\\r\\n    import asyncio\\r\\n    from random import sample\\r\\n    from time import time\\r\\n    \\r\\n    async def io_bound(s, label=\\\"IO bound\\\"):\\r\\n        print(label + \\\" started\\\")\\r\\n        await asyncio.sleep(s)\\r\\n        print(label + \\\" finished\\\")\\r\\n    \\r\\n    # Example of a blocking task\\r\\n    async def cpu_bound(s, label=\\\"CPU bound\\\"):\\r\\n        print(label + \\\" started\\\")\\r\\n        end = time() + s\\r\\n        while time() < end:\\r\\n            sample(range(1000), 1000)\\r\\n        print(label + \\\" finished\\\")\\r\\n    \\r\\n    async def poll(s):\\r\\n        while True:\\r\\n            print(\\\"Poll\\\")\\r\\n            await asyncio.sleep(s)\\r\\n            active_tasks = [task for task in asyncio.Task.all_tasks() if not task.done()]\\r\\n            if  len(active_tasks) < 3: # this poll() and wait() for run_until_complete will always exist\\r\\n                return\\r\\n    \\r\\n    loop = asyncio.get_event_loop()\\r\\n    tasks = [asyncio.ensure_future(io_bound(s, str(s) + \\\"s IO Bound\\\")) for s in range(4)]\\r\\n    tasks.append(asyncio.ensure_future(cpu_bound(2, \\\"2s CPU Bound\\\")))\\r\\n    tasks.append(asyncio.ensure_future(poll(.5)))\\r\\n    \\r\\n    loop.run_until_complete(asyncio.wait(tasks))\\r\\n    \\r\\n    print(\\\"Event loop completed\\\")\"}, \"id\": \"b6600f3d-7ca9-4c56-bb90-8dd87d0aec8e\"}, {\"type\": \"paragraph\", \"value\": \"<p>Output:</p>\", \"id\": \"4da01b5b-cfdf-4ebe-ae78-c678471fe147\"}, {\"type\": \"code\", \"value\": {\"language\": \"bash\", \"code\": \"0s IO Bound started\\r\\n    1s IO Bound started\\r\\n    2s IO Bound started\\r\\n    3s IO Bound started\\r\\n    2s CPU Bound started\\r\\n    2s CPU Bound finished\\r\\n    Poll\\r\\n    0s IO Bound finished\\r\\n    1s IO Bound finished\\r\\n    2s IO Bound finished\\r\\n    Poll\\r\\n    3s IO Bound finished\\r\\n    Event loop completed\"}, \"id\": \"d46a0c4f-babc-4785-96b6-6b42995cf6c3\"}, {\"type\": \"markdown\", \"value\": \"The difference here is that the CPU bound tasks runs to completion without being interrupted.  You may ask yourself - isn't this a bad thing?  Yes it is for CPU bound tasks and that's why you shouldn't use coroutines to parallelize those.  But it illustrates the point that your code will not be interrupted willy-nilly!  Since execution won't be preempted you don't have to deal with implementing a synchronization strategy for shared resources.  All the tasks that have to wait for _external_ IO bound processing to complete effectively have the work processed in parallel - slashing the overall wait time.  All of this is done very efficiently and without the headache of resource synchronization!  Waiting on IO bound external tasks is very common in web development, which is why this form of asynchronous processing has become all the rage.\", \"id\": \"354bf569-3602-486b-a45c-0772bd732962\"}]", "subheading": "(and why it's all the rage for web services)", "date": "2017-06-19"}}, {"model": "weblog.weblogpage", "pk": 6, "fields": {"body": "[{\"type\": \"paragraph\", \"value\": \"<h3>Summary</h3><p>High level development walk through for a toy example of a modern full web application stack. Intended for those who want an idea of what basic knowledge and skills a &quot;Full Stack Engineer&quot; would have without being bogged down with the knowledge required for a real world project. Some scope of additional knowledge required for a real world project (team size &gt; 1, customer facing features, security, scale-ability, ...) is mentioned.</p><p>The complete source code of the example project is available at <a href=\\\"https://github.com/RandyMoore/mySiteDjango\\\">https://github.com/RandyMoore/mySiteDjango</a></p><h3>Requirements</h3><p>Before starting out on any project it is important to clearly define what the goal is. In this case a personal website serving as a space to learn, experiment, and showcase web technologies. The site serves as an example for those wishing to learn the technology and so the project should be as simple and understandable as possible. The site also serves as a professional portfolio and so should be visually appealing to a non-technical audience.</p><h3>Architecture</h3><p>The requirements drive the architecture. The user roles consist of the content creator and the content viewer. The content is static and not sensitive which simplifies several aspects of the architecture. We don&#x27;t need to be concerned with growing storage demands or security since there is nothing sensitive to protect. Another requirement is deployment flexibility compatible with the major cloud service providers for low cost high performance hosting. Someone wishing to learn the technology should also be able to easily deploy the site locally so they can experiment with it. A two tier architecture (web server and database layers) encapsulated in a deployable container fits the bill for this use case.</p>\", \"id\": \"204d1b56-0988-44f0-bf0f-54d56d395837\"}, {\"type\": \"image\", \"value\": 5, \"id\": \"69d402a6-ff31-4afb-b921-7e34a5812afc\"}, {\"type\": \"paragraph\", \"value\": \"<h3>Languages</h3><p>Choice of programming languages impacts the set of existing software you can choose to build your project from. Language and existing software choice (framework, libraries) are somewhat of a chicken and egg problem, start with which you feel strongest about. Web applications are broadly split into two parts, the front end and back end. The front end part of the project is run by the viewer&#x27;s web browser. The back end is run by the computing resources provided by the cloud service provider.</p><p>There&#x27;s less of a language choice for the front end which is driven by the need for compatibility to run on a variety of web browsers. For a web page HTML is required for the structured layout of content; CSS and Javascript are optional for separating out style and adding behavior respectively. There are many other front end languages you might encounter (e.g. CoffeScript, SASS) but these generally compile down to some older version of Javascript or CSS before it is served to the viewer&#x27;s browser.</p><p>For the back end, anything goes. The client&#x27;s web browser will contact the server using <a href=\\\"https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol\\\">HTTP</a>. The web browser has no knowledge of what is happening on the other side of this HTTP interface and so you are free to choose whatever language you wish. But you do need to comply with the HTTP specification, which is rather large. Implementing the behavior required by the HTTP specification using only primitive language features would be an impractical task. Enter frameworks.</p><h3>Frameworks</h3><p>HTTP is well defined in a specification and this has allowed others to write reusable software to take care of the HTTP details. Frameworks are language specific - they provide a bare-bones system in a particular language and allow you to add your own code to add custom behavior. You may already be familiar with the concept of a library. A framework is similar in that it is reusable code but is different because it drives control instead of responding to commands as a library would. In this case the framework code will be the first to handle the incoming request from the viewer&#x27;s web browser and then it will call your code to form a reply. <a href=\\\"https://www.djangoproject.com/\\\">Django</a> was chosen as the web framework for this project. Here&#x27;s a visualization of framework vs library:</p>\", \"id\": \"d9bc862c-31dc-4484-ad05-64250a252e01\"}, {\"type\": \"html\", \"value\": \"<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/lC0KapQU0aM\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\", \"id\": \"3e30cd2a-16c7-4ec2-9e0e-00fadbc12e4a\"}, {\"type\": \"paragraph\", \"value\": \"<h3>Libraries</h3><p>The next step in the project is deciding what content should be added. A blog is a standard way for professional software developers to market their brand; the blog may be used to write about each project as it is added to the site. A naive approach is to write a single HTML file for each blog post. But writing HTML code can be cumbersome and having each post as a stand-alone HTML file might cause problems in the future if you want to change the style of all the posts; each file would have to changed.</p><p>Writing HTML by hand can be a good fit for simple data content that isn&#x27;t meant to be pretty. Creating something that has appealing style requires design skill. Fortunately libraries exist that encapsulate design. A quick search yields a <a href=\\\"https://startbootstrap.com/template-overviews/clean-blog/\\\">Bootstrap clean-blog design</a>. This takes care of the design aspect of the blog.</p><p>People have also written reusable software for the content aspect of things like blogs. This kind of software is known as a <a href=\\\"https://en.wikipedia.org/wiki/Content_management_system\\\">Content Management System</a> (CMS). Generally CMS is used as a means to separate content creation from the technical details of a site. Content specialists (e.g. journalists) may add content without becoming mired in technical details. CMS systems usually include WYSIWYG (What You See Is What You Get) editors; one way to avoid writing raw HTML. CMS systems are generally dependent on the web framework since there is a lot of glue code within the framework that has to interface with the various facets of the content (html generation, image resources, URL paths managed by the CMS, ...). Some searching yielded <a href=\\\"https://wagtail.io/\\\">Wagtail</a> as a CMS for use with Django. Perhaps a bit overkill for this site (having a team size of 1) but I was curious about CMS technology and wanted to experiment with a full fledged example of this technology.</p><h3>Development</h3><p>For a team size of one enough has been decided at this point to begin working with code. Generally a development environment is built based on choice of language since each language has it&#x27;s own paradigm of what a development environment is. For the Python back end we use <a href=\\\"https://pypi.python.org/pypi/pip\\\">pip</a> to fetch and install project requirements (existing reusable software components). A common concern with development environments is isolation between multiple projects on the same machine. We use <a href=\\\"https://virtualenv.pypa.io/en/stable/\\\">VirtualEnv</a> to keep all libraries and frameworks from interfering with each other and the global Python environment on our development machine.</p><p>Thought should be given regarding project structure. The chosen framework will often dictate this since it needs to know where to find your extensions but often your project will have files that exist outside of the framework. For example, in this project the declaration for which Python frameworks and libraries to install are kept in a file <a href=\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/config/requirements.txt\\\">requirements.txt</a> that can be fed to pip. This and other configuration files that affect the deployed code are kept in a folder name <a href=\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/config\\\">config</a> at the highest level of the project. The <a href=\\\"https://github.com/RandyMoore/mySiteDjango/tree/master/my_site_django\\\">Django framework structure</a> begins as a sibling at same level since it is also a component of the deployed code.</p><p>A modern trend with languages and frameworks is to provide a REPL (Read Evaluate Print Loop). Languages (ie Python) often have a shell that evaluate commands this way, providing an interactive environment that encourages experimentation and exploration. Django and many other frameworks mimic a REPL with hot reloading. In Django, this is provided by the built-in development server (&#x27;python manage.py runserver&#x27;). Anytime you make a change to your code the changes are quickly and automatically ingested by the server; reflected the next time an HTTP request is processed. This feedback loop allows you to incrementally add and see small changes to the site, speeding development.</p><p>To edit code many people find an Integrated Development Environment (IDE) helpful. The IDE adds smarts and tools to your basic text editor. &quot;smarts&quot; meaning it is aware of the code in your project and can automatically recognize relationships between pieces of your code and also library or framework code. The IDE allows you to quickly navigate through execution paths to quickly understand how the pieces work together. I&#x27;m a fan of IDEA products, their community edition (CE) PyCharm is great for Python centric projects. The &#x27;python manage.py runserver&#x27; command may be run in a debugging session within the IDE, making it trivial to debug and explore the inner workings of the server code.</p><p>PyCharm CE doesn&#x27;t support front end languages well so I&#x27;ve been using (learning) the Atom editor for front end work. As mentioned before, front end programming languages generally get compiled down to an older version of HTML, CSS and Javascript to maximize browser compatibility. The step of converting your modern code into these older language versions is called &quot;bundling&quot;. For the front end development environment this project uses <a href=\\\"https://en.wikipedia.org/wiki/JavaScript\\\">Javascript</a> with <a href=\\\"https://nodejs.org/en/\\\">Node.js</a> as the engine to execute the Javascript (instead of a web browser). Similar to pip for Python exists <a href=\\\"https://www.npmjs.com/\\\">npm</a> (Node Package Manager) for Javascript (<a href=\\\"https://yarnpkg.com/en/\\\">yarn</a> may be used interchangeably with npm if npm fails to install something).</p><p>In this project the Javascript dependencies are recorded at the top level in a <a href=\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/package.json\\\">package.json</a> file. <a href=\\\"https://gruntjs.com/\\\">Grunt</a> is a Javascript program that serves as a way to glue together the steps of the bundling process. The tasks are declared in the project level <a href=\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Gruntfile.js\\\">Gruntfile.js</a>. The default task invoked with &#x27;grunt&#x27; may be run while editing code and serves as a REPL; when any front end related file changes it invokes the rebundling process and the changes will be reflected by the development server after a browser page refresh. For debugging the developer tools available in the browser are used (available by default in modern browsers).</p><h3>Project Management</h3><p>After writing some code and getting something basic to work you will worry that making further changes will break what you&#x27;ve already accomplished. Enter version control systems. Such systems allow you to track changes, moving between versions of your code at will. Version control becomes more of a concern as team size increases, but even with a team of size of one they are worth using. For example you can use branching to work on a major new feature while having the flexibility to quickly tweak the version of code that is currently deployed and being viewed by the public. This project uses <a href=\\\"https://git-scm.com/\\\">git</a>. GitHub (a service provided by a company) is used as a publicly accessible repository since it is well suited for open source projects.</p><p>Project management consists of far more than version control and becomes increasingly important as team size grows. At a high level a team decides on a <a href=\\\"https://en.wikipedia.org/wiki/Software_development_process\\\">development methodology</a> and refines a specific software development process to meet the project needs. The choice of methodology and process can influence what technologies to use and so is usually decided early in the project&#x27;s life cycle. The art of harnessing the work of multiple individual developers is it&#x27;s own subject and best learned from experience working with a team.</p><h3>Deployment</h3><p>Once you have something you&#x27;d like to show to the public the next step is to serve it to the world. Technically you could serve from the your development machine but keeping your development machine running 100% of the time to run the site would likely be a nuisance. You could also serve from a different machine that you own but this comes with a great deal of complexity and will probably expose your personal network to a high level of security risk. Additionally your home network is likely on the fringe of the world wide web which makes it slow to respond.</p><p>Fortunately companies offering cloud services exist which allow you to host arbitrary software and take care of the messy details of hosting (networking, security, computing resource allocation ...). The idea is that the hosting company figures out how to manage all of the hosting details for many users and then provides hosting as a service, adding value with economy of scale. A popular example is <a href=\\\"https://aws.amazon.com/\\\">Amazon Web Services</a> (AWS), but many vendors exist. Even with the hosting details taken care of there is still the issue of how to install your software on the cloud.</p><p>Accessing running software on the cloud isn&#x27;t as convenient or seamless as your local development environment. Changing anything almost always guarantees bugs. Wouldn&#x27;t it be awesome if you could draw a box around the code running on your development machine and simply drop it into the cloud? Then changing to the cloud environment would introduce minimal changes (seen from the perspective of your code) and result in fewer bugs. <a href=\\\"https://en.wikipedia.org/wiki/Abstraction_(software_engineering)\\\">Abstraction</a> comes to the rescue: why not abstract a machine, or more precisely the operating system? The technology that does this has existed for some time and is know as Virtualization. Virtual machines came about early on but are cumbersome due to their large size from replicating an entire OS. A much lighter weight alternative is <a href=\\\"https://www.docker.com/\\\">Docker</a>, which this project uses.</p><p>The Docker paradigm is to have a single app with all dependencies running in a single container and to tie multiple containers together to form a system. This is an example of the <a href=\\\"https://en.wikipedia.org/wiki/Single_responsibility_principle\\\">Single Responsibility Principle</a> which gives rise to many desirable design traits including ease of reuse. In this project there are two Docker containers; one for the web service and another that hosts the database. The database container is reused from a publicly available repository; the project only needs to declare it as a dependency, populate it with data, and it works out of the box.</p><p>At the top level of the project is the <a href=\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/Dockerfile\\\">Dockerfile</a> that is used to build the image for the web server. Running an image with Docker (the program installed on a machine) creates a container. The <a href=\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose.yml\\\">docker-compose.yml</a> ties together the local web server and database images to form the complete stack; running &#x27;docker-compose up&#x27; will create both web server and database containers from the images and make them visible to each other in their own network. Once the local docker images are working they are pushed to an AWS repository. AWS requires the repository identifier reflected in the image name hence the separate <a href=\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/docker-compose-AWS.yml\\\">docker-compose-AWS.yml</a> file. At a high level deploying to AWS involves creating a spot to host the container (EC2 instance), authenticating through a local AWS docker client, and running &#x27;docker-compose up&#x27; on docker-compose-AWS.yml. The Docker service running on the EC2 instance fetches the images from the AWS repository and brings them up to form the complete system.</p><h3>Testing</h3><p>A minimal amount of testing was required for this project. Testing needs increase with how mission critical a project is and how many people are working on the project, among other considerations. Even though this was a toy project with one developer it does have <a href=\\\"https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/src/js/government_audit/test/AuditSearchView.test.js\\\">some unit tests</a>, serving primarily as <a href=\\\"https://en.wikipedia.org/wiki/Regression_testing\\\">regression</a> tests. <a href=\\\"https://reactjs.org/\\\">React</a> and <a href=\\\"https://facebook.github.io/flux/\\\">Flux</a> were new to me so unit tests were created once I had things working so that I could refactor the code and quickly detect when I had made a code change that resulted in a change of code output. Of note in these tests is the use of <a href=\\\"https://facebook.github.io/jest/\\\">Jest</a> which allows creation of unit tests without the manual creation of assertion code. It has built-in support for snapshots (auto verifying everything) - perfect for regression testing to ensure that refactoring code doesn&#x27;t change the output of the code.</p><p>The development process for this project was most like <a href=\\\"https://en.wikipedia.org/wiki/Software_prototyping\\\">Software Prototyping</a>. Emphasis was placed on getting up and running as quickly as possible with quick turnaround enabling experimentation and progress along the learning curve. In a sense the REPL like nature of the environment offered the bulk of testing for this project. For systems that allow interaction, especially when users are allowed to change data on the server, more testing would be necessary.</p><h3>Not in this Example</h3><p>Being a toy project there are many parts not present here that would be encountered in a real world project. Here minimal attention is given to security as the content is static and not sensitive. HTTP encryption via SSL <a href=\\\"https://letsencrypt.org/\\\">is becoming an expected feature</a>. Once you have any user interaction with a site authentication becomes a requirement. Scalability isn&#x27;t an issue here but if a web site provides a non-trivial service to many users a distributed system architecture will likely be necessary.</p><p>For development workflow a team would have a system for deciding, recording, organizing, and tracking implementation progress for requirements. <a href=\\\"https://continuousdelivery.com/foundations/test-automation/\\\">Automated testing</a> with a deployment pipeline would exist. Error handling for both end users and developers (e.g. throw meaningful exceptions) would exist. Logging is typical as is emitting, collecting and analyzing performance metrics from the system for both engineering and business purposes. Each of these areas requires deep knowledge and experience to implement well; there is often a specialized role created for each. Participating in existing projects is the best way to learn about these subjects.</p>\", \"id\": \"da2a54e4-92c0-4563-ba59-ab25b922b24b\"}]", "subheading": "of this Toy Modern Web Application", "date": "2017-10-02"}}, {"model": "weblog.weblogpage", "pk": 7, "fields": {"body": "[{\"type\": \"markdown\", \"value\": \"Inspiration\\r\\n========\\r\\nThis project came about for these reasons:\\r\\n\\r\\n 1.  Make a positive impact on the world.\\r\\n    *  Enable understanding through exploration of hard data as opposed to adhering to a particular belief system.\\r\\n    *  A first project to gain some experience working with real world open data.\\r\\n 2.  Learn some of the technologies used by TravelPerk so I could hit the ground running. \\r\\n 3.  Professional Development - gain experience with some of the latest technologies.\\r\\n\\r\\nThe previous version of the site included out-of-the-box Postgres text search functionality, already very powerful.  Only documents through mid 2014 were included in the [.zip file available for download](https://archive.org/details/usinspectorsgeneral) from archive.org.  Searching through the ~35,000 audit documents for the names of my favorite politicians was fun but not not particularly insightful.  The goal become how to glean information from such a vast corpus without a huge investment in building and tuning and AI system.\\r\\n\\r\\nFinal Product\\r\\n=============\\r\\n\\r\\nAfter some probing I came across [NLTK](http://www.nltk.org/) - a Python library that may be used to parse raw text and reveal it's structure.  Of particular interest was [Named Entity Recognition](http://www.nltk.org/book/ch07.html) (see chapter 5).  Since the context is set in this case as Audit Documents any named entity within a document would be of interest.  Combined with the inspiration that [simple frequency distribution](http://www.nltk.org/book/ch01.html) often yields insight a plan was hatched to explore the audit document corpus based on frequency of Named Entities.  After some more thought the plan was refined as follows:\\r\\n\\r\\n 1.  Extract all named entities from each audit document along with their frequency within that document.\\r\\n 2.  Across all audit documents determine what the most frequently occuring named entities are.  Provide an ordered list to the user of the most N frequently occuring named entities across audit documents (each document contributes a count of 1 towards a named entity if that named entity occurs more than K times in the document).\\r\\n 3.  Allow the scope to be limited to a user selected set of years so the user can see how subject matter changes across years.\\r\\n 4.  Enable exploration through refinement.  After a user selects a named entity the scope is then limited only to Audit Documents that contain that named entity.  \\r\\n\\r\\nThe above exploration process allows the user to find a small number of audit documents who all contain the set of named entities the user is interested in.  It turns out this will result in a set of documents related to a specific concern.  Here is an example search across Audit Documents in 2017:\\r\\n\\r\\nLanding Page\\r\\n-------------------\", \"id\": \"39563302-2d00-4dbd-bdcd-6353cbcde4b2\"}, {\"type\": \"image\", \"value\": 19, \"id\": \"c25448a6-6344-4b6e-9b6a-c1207e5febf7\"}, {\"type\": \"markdown\", \"value\": \"Click the \\\"Named Entity Exploration\\\" Button...\\r\\n\\r\\nStart of Named Entity Exploration\\r\\n----------------------------------------------\", \"id\": \"18fc48fd-2620-4628-9b5a-bce895a02059\"}, {\"type\": \"image\", \"value\": 14, \"id\": \"e144978c-317a-4ef1-9cc4-9476442cdcf1\"}, {\"type\": \"markdown\", \"value\": \"Click the \\\"Veterans\\\" button...\\r\\n\\r\\nDocuments containing \\\"Veterans\\\"\\r\\n-----------------------------------------------\", \"id\": \"68459c9b-0032-41e8-8ba8-e1ac11e8638e\"}, {\"type\": \"image\", \"value\": 16, \"id\": \"c94a824b-d7ba-4628-a724-f172b974adf5\"}, {\"type\": \"markdown\", \"value\": \"Click the \\\"VISN\\\" button...\\r\\n\\r\\nNarrow scope for documents also containing \\\"VISN\\\"\\r\\n------------------------------------------------------------------------\", \"id\": \"4348a42c-cfca-490d-a63d-3fe31aac966c\"}, {\"type\": \"image\", \"value\": 17, \"id\": \"b0561886-8e2d-4430-9956-db11f2cf55c9\"}, {\"type\": \"markdown\", \"value\": \"Final Result\\r\\n----------------\\r\\nIncluding:\\r\\n\\r\\n *  Medical Center\\r\\n *  OSC: Acronym for the [U.S. Office of Special Council](https://osc.gov/Pages/about.aspx), who investigate whistle blower complaints.\", \"id\": \"eb63228f-6f57-44ff-90dc-3a7d385cac3e\"}, {\"type\": \"image\", \"value\": 21, \"id\": \"3748efbc-4d49-468c-882c-2d7f51678e63\"}, {\"type\": \"markdown\", \"value\": \"The result is a consistent set of Audit Documents investigating the performance of Medical Centers providing care for veterans across various cities.\\r\\n\\r\\n---\\r\\n\\r\\nOffline Processing\\r\\n==================\\r\\n\\r\\nAs mentioned before, the archive at archive.org only had up through 2014.  Obtaining more recent reports required using the [software that created the archive in the first place](https://github.com/unitedstates/inspectors-general#inspectors-general).  This software has knowledge of what Inspector General websites exist and how to search each one for audit documents.  Each audit document is downloaded, converted to raw text (often poorly, a topic for another post), and parsed for basic meta information (title, publication date, etc).  A directory with these files is created for each report with the overall directory structure reflecting the publication year and which website the report came from.  Running the tool resulted in a total of 51,295 audit documents to process.  The metadata would be used to obtain title, publication date, and source url while the raw text would be parsed for named entities and indexed by Postgres for text search.  Only the distilled data from this processing would be included on the site, to view the document the origin url would be provided to the end user.\\r\\n\\r\\nParsing each audit document using NLTK turned out to be the bottleneck, up to 10 seconds or so for a particularly large text.  To finish in a reasonable amount of time it became clear that all 4 of my raging CPU cores (Phenom II X4 965) would need to be fully utilized.  Python offers many options for concurrent execution, including asynchronous programming within a thread (supported by the language) and [multithreading and multiprocessing](https://docs.python.org/3.6/library/concurrency.html).  In a previous [article](http://randalmoore.me/asynchronous-programming/) I explained the differences between these.  In this case since the tasks are CPU bound and taking into consideration the [Python Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) the best option was to use multi processing.  \\r\\n\\r\\n[This file](https://github.com/RandyMoore/mySiteDjango/blob/master/my_site_django/upload_audit_docs.py) populates the Postgres database with metadata, text search, and named entity data for each audit document.  It searches through a directory structure at a given root and processes the files associated with each audit document if that data isn't already in the database.  The documents are processed in parallel, one per core of the host machine.\\r\\n\\r\\nThe write of results to Postgres was originally written to occur in the same task after processing the document but this resulted in Django layer concurrency errors.  It would have been complicated and inefficient to coordinate the DB as a shared resource so I opted instead to use a queue.  Each document processing task places the result into a queue.  An additional DB dedicated process consumes from the queue and writes each result to the database sequentially.  The final code was surprisingly simple:\\r\\n\\r\\nTop of file:\\r\\n\\r\\n    db_queue = None # declared as global to be shared between processes\\r\\n    \\r\\nIn \\\\_\\\\_main\\\\_\\\\_ create the DB queue and start it's process:\\r\\n\\r\\n    db_queue = multiprocessing.Manager().Queue() # Special queue provided for IPC use\\r\\n    multiprocessing.Process(target=save_to_db).start() # DB dedicated process\\r\\n\\r\\nIn process_documents() the results are placed in the queue:\\r\\n\\r\\n    db_queue.put((doc, named_entities))\\r\\n    \\r\\nIn save_to_db() we loop forever consuming from the queue until None is encountered:\\r\\n\\r\\n    while True: \\r\\n        doc_tuple = db_queue.get()  # blocks if queue is empty\\r\\n        <exit function if None found>\\r\\n        <save to DB>        \\r\\n    \\r\\nIn \\\\_\\\\_main\\\\_\\\\_ all cores of the host machine are loaded up using a multiprocessing Pool and once they are finished None is placed in the DB queue to be found by the DB task causing it to exit:\\r\\n\\r\\n    with multiprocessing.Pool() as p: # by default creates as many processes as available cores.\\r\\n        p.map(func=process_documents, iterable=files) # Processes to process documents\\r\\n\\r\\n    db_queue.put(None)\\r\\n\\r\\nWith this my machine was steadily maxed at 100% processor utilization, and took close to a full day to process all of the documents.  A fair amount of electricity was used but at least some neat Python tricks were explored and cool words such as \\\"corpus\\\" learned :)\", \"id\": \"3a156f36-80ae-4fad-ab0f-2dd520ba4968\"}]", "subheading": "Learning more Python because my machine is slow", "date": "2017-11-25"}}]